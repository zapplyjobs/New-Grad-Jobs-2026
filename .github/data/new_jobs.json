[
  {
    "job_title": "Product Operations Manager, Launch Readiness ",
    "employer_name": "anthropic",
    "job_city": "San Francisco, CA",
    "job_apply_link": "https://job-boards.greenhouse.io/anthropic/jobs/4978674008",
    "job_posted_at_datetime_utc": "2025-11-22T09:28:05-05:00",
    "job_description": "About Anthropic Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.About the role: We're hiring a Product Operations Manager - Launch Readiness to own and continuously improve how Anthropic launches products. This is a product-wide, horizontal role that will establish the operating system for all customer-facing launches across our portfolio. Launching at Anthropic is uniquely dynamic. We're building on frontier models that evolve constantly, serving customers from individual developers to the largest enterprises, across multiple surfaces (API, claude.ai, Claude Code). This role will coordinate launches across Product teams and critical partners including Security, Legal, GTM, Finance, DevRel, and Safety. You'll establish common language, document repeatable launch motions, and build an AI-native toolkit that makes excellent launches feel effortless. You treat launch readiness as a product. You're obsessed with making the \"happy path\" so helpful that teams obviously want to follow it. You build systems that accelerate product velocity, never gate it. You treat launch readiness as a product. You’re obsessed with making the “happy path” so helpful that teams obviously want to follow it. You build systems that accelerate product velocity, not gate it. You think like a product manager, not a compliance officer. Your work will directly impact every customer-facing launch at Anthropic, making them faster, safer, and better coordinated.ct manager, not a compliance officer. Your work will directly impact every customer-facing launch at Anthropic, making them faster, safer, and more coordinated. Responsibilities: You’ll own the operating system for customer-facing product launches at Anthropic. Working horizontally across Product teams and critical partners—including Security, Legal, GTM, Finance, DevRel, and Safety—you’ll establish frameworks, toolkits, and processes that enable teams to ship with confidence. You’ll drive adoption through influence, making launch readiness effortless and creating visibility across all launches. Launch calendar & system of record Own our launch calendar as the source of truth for all customer-facing launches. Create visibility and predictability across Product and XFN teams. Build workflows that reinforce our central system of record without becoming a documentation tax. Integrate with team operating rhythms and obsess over the user experience of our internal tools. Repeatable Launch Processes Document and iterate on “happy path” launch processes across the launch lifecycle. Establish common language and conventions that work across diverse launch types. Cross-Functional Coordination Partner with Security, Legal, GTM, Finance, DevRel, and other XFN teams to define clear engagement models. These “APIs between teams” make coordination seamless and reliable. Enable the right teams to engage with the right projects at the right time. Create a federated operating model that gives teams autonomy while maintaining common standards. AI-Native Launch Toolkit Build Claude-powered assistants and automation to streamline launch workflows. Experiment with ambient intelligence to pull information from across the org. Automate intake workflows, documentation generation, and status tracking. Partner with Engineering on tooling strategy and integrations. Continuous Improvement Define and track success metrics for Define and track success metrics for launch excellence to identify bottlenecks through data and feedback.launch excellence in order to identify bottlenecks through data and feedback. Run post-launch retros and feed learnings back into process improvements. Scale best practices across teams through documentation and education. You may be a good fit if you: 7+ years in product operations, program management, launch management, or related operational roles in fast-paced tech companies. Have experience in hyperscale companies Have experience in hypergrowth companies navigating rapid scaling and creating process that accelerates teams without slowing them down.navigating rapid growth creating common process that accelerates teams and never slows them down.Are quick to experiment with AI to streamline business processes and see optimization opportunities everywhere. Excel at building strong relationships with technical and non-technical stakeholders. Are quick to experiment with AI to streamline business processes and see opportunities everywhere for further optimization. Have experience managing launch motions across diverse technical surfaces (API, web apps, desktop/mobile) for high visibility launches. Are comfortable with ambiguity and can create structure where none exists. You're service-oriented and obsessed with making it easy for others to do great work. Strong candidates may also have experience with: Building AI-native workflows and pushing the boundaries of automation. Product Management and/or Product Marketing You treat process as a product with users, metrics, and continuous iteration. Track record of building and scaling operations teams The expected base compensation for this position is below. Our total compensation package for full-time employees includes equity, benefits, and may include incentive compensation.Annual Salary:$260,000 - $325,000 USDLogistics Education requirements: We require at least a Bachelor's degree in a related field or equivalent experience.Location-based hybrid policy: Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices. Visa sponsorship: We do sponsor visas! However, we aren't able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this. We encourage you to apply even if you do not believe you meet every single qualification. Not all strong candidates will meet every single qualification as listed. Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you're interested in this work. We think AI systems like the ones we're building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team. How we're different We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We're an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills. The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI & Compute, Concrete Problems in AI Safety, and Learning from Human Preferences. Come work with us! Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. Guidance on Candidates' AI Usage: Learn about our policy for using AI in our application process",
    "id": "job-boards-greenhouse-io-anthropic-jobs-4978674008",
    "source": "greenhouse",
    "source_url": "boards-api.greenhouse.io",
    "source_id": "4978674008",
    "title": "Product Operations Manager, Launch Readiness ",
    "company_name": "anthropic",
    "company_slug": "anthropic",
    "location": "San Francisco, CA",
    "locations": [
      "San Francisco, CA"
    ],
    "url": "https://job-boards.greenhouse.io/anthropic/jobs/4978674008",
    "departments": [
      "Product Management, Support, & Operations"
    ],
    "employment_type": null,
    "posted_at": "2025-11-22T09:28:05-05:00",
    "fetched_at": "2026-01-03T04:44:24.618Z",
    "description": "&lt;div class=&quot;content-intro&quot;&gt;&lt;h2&gt;&lt;strong&gt;About Anthropic&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.&lt;/p&gt;&lt;/div&gt;&lt;h2&gt;About the role:&lt;/h2&gt;\n&lt;p&gt;We&#39;re hiring a Product Operations Manager - Launch Readiness to own and continuously improve how Anthropic launches products. This is a product-wide, horizontal role that will establish the operating system for all customer-facing launches across our portfolio.&lt;/p&gt;\n&lt;p&gt;Launching at Anthropic is uniquely dynamic. We&#39;re building on frontier models that evolve constantly, serving customers from individual developers to the largest enterprises, across multiple surfaces (API, claude.ai, Claude Code). This role will coordinate launches across Product teams and critical partners including Security, Legal, GTM, Finance, DevRel, and Safety. You&#39;ll establish common language, document repeatable launch motions, and build an AI-native toolkit that makes excellent launches feel effortless.&lt;/p&gt;\n&lt;p&gt;You treat launch readiness as a product. You&#39;re obsessed with making the &quot;happy path&quot; so helpful that teams obviously want to follow it. You build systems that accelerate product velocity, never gate it. You treat launch readiness as a product. You’re obsessed with making the “happy path” so helpful that teams obviously want to follow it. You build systems that accelerate product velocity, not gate it. You think like a product manager, not a compliance officer. Your work will directly impact every customer-facing launch at Anthropic, making them faster, safer, and better coordinated.ct manager, not a compliance officer. Your work will directly impact every customer-facing launch at Anthropic, making them faster, safer, and more coordinated.&lt;/p&gt;\n&lt;h2&gt;Responsibilities:&lt;/h2&gt;\n&lt;p&gt;You’ll own the operating system for customer-facing product launches at Anthropic. Working horizontally across Product teams and critical partners—including Security, Legal, GTM, Finance, DevRel, and Safety—you’ll establish frameworks, toolkits, and processes that enable teams to ship with confidence. You’ll drive adoption through influence, making launch readiness effortless and creating visibility across all launches.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;Launch calendar &amp;amp; system of record&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;Own our launch calendar as the source of truth for all customer-facing launches. Create visibility and predictability across Product and XFN teams.&amp;nbsp;&lt;/li&gt;\n&lt;li&gt;Build workflows that reinforce our central system of record without becoming a documentation tax. Integrate with team operating rhythms and obsess over the user experience of our internal tools.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;p&gt;&lt;strong&gt;Repeatable Launch Processes&lt;/strong&gt;&amp;nbsp;&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;Document and iterate on “happy path” launch processes across the launch lifecycle.&amp;nbsp;&lt;/li&gt;\n&lt;li&gt;Establish common language and conventions that work across diverse launch types.&amp;nbsp;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;p&gt;&lt;strong&gt;Cross-Functional Coordination&lt;/strong&gt;&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;Partner with Security, Legal, GTM, Finance, DevRel, and other XFN teams to define clear engagement models. These “APIs between teams” make coordination seamless and reliable.&lt;/li&gt;\n&lt;li&gt;Enable the right teams to engage with the right projects at the right time.&amp;nbsp;&lt;/li&gt;\n&lt;li&gt;Create a federated operating model that gives teams autonomy while maintaining common standards.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;p&gt;&lt;strong&gt;AI-Native Launch Toolkit&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;Build Claude-powered assistants and automation to streamline launch workflows.&amp;nbsp;&lt;/li&gt;\n&lt;li&gt;Experiment with ambient intelligence to pull information from across the org.&amp;nbsp;&lt;/li&gt;\n&lt;li&gt;Automate intake workflows, documentation generation, and status tracking. Partner with Engineering on tooling strategy and integrations.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;p&gt;&lt;strong&gt;Continuous Improvement&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;Define and track success metrics for Define and track success metrics for launch excellence to identify bottlenecks through data and feedback.launch excellence in order to identify bottlenecks through data and feedback.&amp;nbsp;&lt;/li&gt;\n&lt;li&gt;Run post-launch retros and feed learnings back into process improvements. Scale best practices across teams through documentation and education.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;You may be a good fit if you:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;7+ years&lt;/strong&gt; in product operations, program management, launch management, or related operational roles in fast-paced tech companies.&amp;nbsp;&lt;/li&gt;\n&lt;li&gt;Have experience in hyperscale companies Have experience in hypergrowth companies navigating rapid scaling and creating process that accelerates teams without slowing them down.navigating rapid growth creating common process that accelerates teams and never slows them down.Are quick to experiment with AI to streamline business processes and see optimization opportunities everywhere.&lt;/li&gt;\n&lt;li&gt;Excel at building strong relationships with technical and non-technical stakeholders.&lt;/li&gt;\n&lt;li&gt;Are quick to experiment with AI to streamline business processes and see opportunities everywhere for further optimization.&lt;/li&gt;\n&lt;li&gt;Have experience managing launch motions across diverse technical surfaces (API, web apps, desktop/mobile) for high visibility launches.&lt;/li&gt;\n&lt;li&gt;Are comfortable with ambiguity and can create structure where none exists.&lt;/li&gt;\n&lt;li&gt;You&#39;re service-oriented and obsessed with making it easy for others to do great work.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;Strong candidates may also have experience with:&amp;nbsp;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Building AI-native workflows and pushing the boundaries of automation.&lt;/li&gt;\n&lt;li&gt;Product Management and/or Product Marketing&lt;/li&gt;\n&lt;li&gt;You treat process as a product with users, metrics, and continuous iteration.&lt;/li&gt;\n&lt;li&gt;Track record of building and scaling operations teams&lt;/li&gt;\n&lt;/ul&gt;&lt;div class=&quot;content-pay-transparency&quot;&gt;&lt;div class=&quot;pay-input&quot;&gt;&lt;div class=&quot;description&quot;&gt;&lt;p&gt;The expected&amp;nbsp;base compensation for this position is below. Our total compensation package for full-time employees includes equity, benefits, and may include incentive compensation.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;title&quot;&gt;Annual Salary:&lt;/div&gt;&lt;div class=&quot;pay-range&quot;&gt;&lt;span&gt;$260,000&lt;/span&gt;&lt;span class=&quot;divider&quot;&gt;&amp;mdash;&lt;/span&gt;&lt;span&gt;$325,000 USD&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;content-conclusion&quot;&gt;&lt;h2&gt;&lt;strong&gt;Logistics&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;Education requirements: &lt;/strong&gt;We require at least a Bachelor&#39;s degree in a related field or equivalent experience.&lt;strong&gt;&lt;br&gt;&lt;br&gt;Location-based hybrid policy:&lt;/strong&gt; Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices.&lt;/p&gt;\n&lt;p&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Visa sponsorship:&lt;/strong&gt;&amp;nbsp;We do sponsor visas! However, we aren&#39;t able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;We encourage you to apply even if you do not believe you meet every single qualification.&lt;/strong&gt; Not all strong candidates will meet every single qualification as listed.&amp;nbsp; Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you&#39;re interested in this work. We think AI systems like the ones we&#39;re building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;How we&#39;re different&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We&#39;re an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills.&lt;/p&gt;\n&lt;p&gt;The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI &amp;amp; Compute, Concrete Problems in AI Safety, and Learning from Human Preferences.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Come work with us!&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. &lt;strong data-stringify-type=&quot;bold&quot;&gt;Guidance on Candidates&#39; AI Usage:&lt;/strong&gt;&amp;nbsp;Learn about&amp;nbsp;&lt;a class=&quot;c-link&quot; href=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; target=&quot;_blank&quot; data-stringify-link=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; data-sk=&quot;tooltip_parent&quot;&gt;our policy&lt;/a&gt;&amp;nbsp;for using AI in our application process&lt;/p&gt;&lt;/div&gt;",
    "_raw": {
      "source": "greenhouse",
      "original_id": 4978674008
    },
    "job_posted_at": "1h",
    "description_platform": "greenhouse",
    "description_success": true
  },
  {
    "job_title": "Research Engineer, Model Evaluations",
    "employer_name": "anthropic",
    "job_city": "San Francisco, CA | New York City, NY",
    "job_apply_link": "https://job-boards.greenhouse.io/anthropic/jobs/4990535008",
    "job_posted_at_datetime_utc": "2025-11-22T09:28:05-05:00",
    "job_description": "About Anthropic Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.About the role As a Research Engineer on the Model Evaluations team, you'll lead the design and implementation of Anthropic's evaluation platform—a critical system that shapes how we understand, measure, and improve our models' capabilities and safety. You'll work at the intersection of research and engineering to develop and implement model evaluations that give us insight into emerging capabilities and build robust evaluation infrastructure that directly influences our training decisions and model development roadmap. Your work will be essential to Anthropic's mission of building safe, beneficial AI systems. You'll collaborate closely with training teams, alignment researchers, and safety teams to ensure our models meet the highest standards before deployment. This is a technical leadership role where you'll drive both the strategic vision and hands-on implementation of our evaluation systems. Responsibilities Design novel evaluation methodologies to assess model capabilities across diverse domains including reasoning, safety, helpfulness, and harmlessness Lead the design and architecture of Anthropic's evaluation platform, ensuring it scales with our rapidly evolving model capabilities and research needs Implement and maintain high-throughput evaluation pipelines that run during production training, providing real-time insights to guide training decisions Analyze evaluation results to identify patterns, failure modes, and opportunities for model improvement, translating complex findings into actionable insights Partner with research teams to develop domain-specific evaluations that probe for emerging capabilities and potential risks Build infrastructure to enable rapid iteration on evaluation design, supporting both automated and human-in-the-loop assessment approaches Establish best practices and standards for evaluation development across the organization Mentor team members and contribute to the growth of evaluation expertise at Anthropic Coordinate evaluation efforts during critical training runs, ensuring comprehensive coverage and timely results Contribute to research publications and external communications about evaluation methodologies and findings You may be a good fit if you Have experience designing and implementing evaluation systems for machine learning models, particularly large language models Have demonstrated technical leadership experience, either formally or through leading complex technical projects Are skilled at both systems engineering and experimental design, comfortable building infrastructure while maintaining scientific rigor Have strong programming skills in Python and experience with distributed computing frameworks Can translate between research needs and engineering constraints, finding pragmatic solutions to complex problems Are results-oriented and thrive in fast-paced environments where priorities can shift based on research findings Enjoy collaborative work and can effectively communicate technical concepts to diverse stakeholders Care deeply about AI safety and the societal impacts of the systems we build Have experience with statistical analysis and can draw meaningful conclusions from large-scale experimental data Strong candidates may also have Experience with evaluation during model training, particularly in production environments Familiarity with safety evaluation frameworks and red teaming methodologies Background in psychometrics, experimental psychology, or other fields focused on measurement and assessment Experience with reinforcement learning evaluation or multi-agent systems Contributions to open-source evaluation benchmarks or frameworks Knowledge of prompt engineering and its role in evaluation design Experience managing evaluation infrastructure at scale (thousands of experiments) Published research in machine learning evaluation, benchmarking, or related areas Representative projects Designing comprehensive evaluation suites that assess models across hundreds of capability dimensions Building real-time evaluation dashboards that surface critical insights during multi-week training runs Developing novel evaluation approaches for emerging capabilities like multi-step reasoning or tool use Creating automated systems to detect regression in model performance or safety properties Implementing efficient evaluation sampling strategies that balance coverage with computational constraints Collaborating with external partners to develop industry-standard evaluation benchmarks Building infrastructure to support human evaluation at scale, including quality control and aggregation systems The expected base compensation for this position is below. Our total compensation package for full-time employees includes equity, benefits, and may include incentive compensation.Annual Salary:$300,000 - $405,000 USDLogistics Education requirements: We require at least a Bachelor's degree in a related field or equivalent experience.Location-based hybrid policy: Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices. Visa sponsorship: We do sponsor visas! However, we aren't able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this. We encourage you to apply even if you do not believe you meet every single qualification. Not all strong candidates will meet every single qualification as listed. Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you're interested in this work. We think AI systems like the ones we're building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team. How we're different We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We're an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills. The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI & Compute, Concrete Problems in AI Safety, and Learning from Human Preferences. Come work with us! Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. Guidance on Candidates' AI Usage: Learn about our policy for using AI in our application process",
    "id": "job-boards-greenhouse-io-anthropic-jobs-4990535008",
    "source": "greenhouse",
    "source_url": "boards-api.greenhouse.io",
    "source_id": "4990535008",
    "title": "Research Engineer, Model Evaluations",
    "company_name": "anthropic",
    "company_slug": "anthropic",
    "location": "San Francisco, CA | New York City, NY",
    "locations": [
      "San Francisco, CA | New York City, NY"
    ],
    "url": "https://job-boards.greenhouse.io/anthropic/jobs/4990535008",
    "departments": [
      "AI Research & Engineering"
    ],
    "employment_type": null,
    "posted_at": "2025-11-22T09:28:05-05:00",
    "fetched_at": "2026-01-03T04:44:24.618Z",
    "description": "&lt;div class=&quot;content-intro&quot;&gt;&lt;h2&gt;&lt;strong&gt;About Anthropic&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.&lt;/p&gt;&lt;/div&gt;&lt;h2&gt;&lt;strong&gt;About the role&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;As a Research Engineer on the Model Evaluations team, you&#39;ll lead the design and implementation of Anthropic&#39;s evaluation platform—a critical system that shapes how we understand, measure, and improve our models&#39; capabilities and safety. You&#39;ll work at the intersection of research and engineering to develop and implement model evaluations that give us insight into emerging capabilities and build robust evaluation infrastructure that directly influences our training decisions and model development roadmap.&lt;/p&gt;\n&lt;p&gt;Your work will be essential to Anthropic&#39;s mission of building safe, beneficial AI systems. You&#39;ll collaborate closely with training teams, alignment researchers, and safety teams to ensure our models meet the highest standards before deployment. This is a technical leadership role where you&#39;ll drive both the strategic vision and hands-on implementation of our evaluation systems.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Responsibilities&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Design novel evaluation methodologies to assess model capabilities across diverse domains including reasoning, safety, helpfulness, and harmlessness&lt;/li&gt;\n&lt;li&gt;Lead the design and architecture of Anthropic&#39;s evaluation platform, ensuring it scales with our rapidly evolving model capabilities and research needs&lt;/li&gt;\n&lt;li&gt;Implement and maintain high-throughput evaluation pipelines that run during production training, providing real-time insights to guide training decisions&lt;/li&gt;\n&lt;li&gt;Analyze evaluation results to identify patterns, failure modes, and opportunities for model improvement, translating complex findings into actionable insights&lt;/li&gt;\n&lt;li&gt;Partner with research teams to develop domain-specific evaluations that probe for emerging capabilities and potential risks&lt;/li&gt;\n&lt;li&gt;Build infrastructure to enable rapid iteration on evaluation design, supporting both automated and human-in-the-loop assessment approaches&lt;/li&gt;\n&lt;li&gt;Establish best practices and standards for evaluation development across the organization&lt;/li&gt;\n&lt;li&gt;Mentor team members and contribute to the growth of evaluation expertise at Anthropic&lt;/li&gt;\n&lt;li&gt;Coordinate evaluation efforts during critical training runs, ensuring comprehensive coverage and timely results&lt;/li&gt;\n&lt;li&gt;Contribute to research publications and external communications about evaluation methodologies and findings&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;&lt;strong&gt;You may be a good fit if you&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Have experience designing and implementing evaluation systems for machine learning models, particularly large language models&lt;/li&gt;\n&lt;li&gt;Have demonstrated technical leadership experience, either formally or through leading complex technical projects&lt;/li&gt;\n&lt;li&gt;Are skilled at both systems engineering and experimental design, comfortable building infrastructure while maintaining scientific rigor&lt;/li&gt;\n&lt;li&gt;Have strong programming skills in Python and experience with distributed computing frameworks&lt;/li&gt;\n&lt;li&gt;Can translate between research needs and engineering constraints, finding pragmatic solutions to complex problems&lt;/li&gt;\n&lt;li&gt;Are results-oriented and thrive in fast-paced environments where priorities can shift based on research findings&lt;/li&gt;\n&lt;li&gt;Enjoy collaborative work and can effectively communicate technical concepts to diverse stakeholders&lt;/li&gt;\n&lt;li&gt;Care deeply about AI safety and the societal impacts of the systems we build&lt;/li&gt;\n&lt;li&gt;Have experience with statistical analysis and can draw meaningful conclusions from large-scale experimental data&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;&lt;strong&gt;Strong candidates may also have&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Experience with evaluation during model training, particularly in production environments&lt;/li&gt;\n&lt;li&gt;Familiarity with safety evaluation frameworks and red teaming methodologies&lt;/li&gt;\n&lt;li&gt;Background in psychometrics, experimental psychology, or other fields focused on measurement and assessment&lt;/li&gt;\n&lt;li&gt;Experience with reinforcement learning evaluation or multi-agent systems&lt;/li&gt;\n&lt;li&gt;Contributions to open-source evaluation benchmarks or frameworks&lt;/li&gt;\n&lt;li&gt;Knowledge of prompt engineering and its role in evaluation design&lt;/li&gt;\n&lt;li&gt;Experience managing evaluation infrastructure at scale (thousands of experiments)&lt;/li&gt;\n&lt;li&gt;Published research in machine learning evaluation, benchmarking, or related areas&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;&lt;strong&gt;Representative projects&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Designing comprehensive evaluation suites that assess models across hundreds of capability dimensions&lt;/li&gt;\n&lt;li&gt;Building real-time evaluation dashboards that surface critical insights during multi-week training runs&lt;/li&gt;\n&lt;li&gt;Developing novel evaluation approaches for emerging capabilities like multi-step reasoning or tool use&lt;/li&gt;\n&lt;li&gt;Creating automated systems to detect regression in model performance or safety properties&lt;/li&gt;\n&lt;li&gt;Implementing efficient evaluation sampling strategies that balance coverage with computational constraints&lt;/li&gt;\n&lt;li&gt;Collaborating with external partners to develop industry-standard evaluation benchmarks&lt;/li&gt;\n&lt;li&gt;Building infrastructure to support human evaluation at scale, including quality control and aggregation systems&amp;nbsp;&lt;/li&gt;\n&lt;/ul&gt;&lt;div class=&quot;content-pay-transparency&quot;&gt;&lt;div class=&quot;pay-input&quot;&gt;&lt;div class=&quot;description&quot;&gt;&lt;p&gt;The expected&amp;nbsp;base compensation for this position is below. Our total compensation package for full-time employees includes equity, benefits, and may include incentive compensation.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;title&quot;&gt;Annual Salary:&lt;/div&gt;&lt;div class=&quot;pay-range&quot;&gt;&lt;span&gt;$300,000&lt;/span&gt;&lt;span class=&quot;divider&quot;&gt;&amp;mdash;&lt;/span&gt;&lt;span&gt;$405,000 USD&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;content-conclusion&quot;&gt;&lt;h2&gt;&lt;strong&gt;Logistics&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;Education requirements: &lt;/strong&gt;We require at least a Bachelor&#39;s degree in a related field or equivalent experience.&lt;strong&gt;&lt;br&gt;&lt;br&gt;Location-based hybrid policy:&lt;/strong&gt; Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices.&lt;/p&gt;\n&lt;p&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Visa sponsorship:&lt;/strong&gt;&amp;nbsp;We do sponsor visas! However, we aren&#39;t able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;We encourage you to apply even if you do not believe you meet every single qualification.&lt;/strong&gt; Not all strong candidates will meet every single qualification as listed.&amp;nbsp; Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you&#39;re interested in this work. We think AI systems like the ones we&#39;re building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;How we&#39;re different&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We&#39;re an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills.&lt;/p&gt;\n&lt;p&gt;The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI &amp;amp; Compute, Concrete Problems in AI Safety, and Learning from Human Preferences.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Come work with us!&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. &lt;strong data-stringify-type=&quot;bold&quot;&gt;Guidance on Candidates&#39; AI Usage:&lt;/strong&gt;&amp;nbsp;Learn about&amp;nbsp;&lt;a class=&quot;c-link&quot; href=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; target=&quot;_blank&quot; data-stringify-link=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; data-sk=&quot;tooltip_parent&quot;&gt;our policy&lt;/a&gt;&amp;nbsp;for using AI in our application process&lt;/p&gt;&lt;/div&gt;",
    "_raw": {
      "source": "greenhouse",
      "original_id": 4990535008
    },
    "job_posted_at": "1h",
    "description_platform": "greenhouse",
    "description_success": true
  },
  {
    "job_title": "Research Engineer, Pre-training",
    "employer_name": "anthropic",
    "job_city": "Remote-Friendly (Travel-Required) | San Francisco, CA | Seattle, WA | New York City, NY",
    "job_apply_link": "https://job-boards.greenhouse.io/anthropic/jobs/4616971008",
    "job_posted_at_datetime_utc": "2025-11-22T09:28:05-05:00",
    "job_description": "About Anthropic Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.Anthropic is at the forefront of AI research, dedicated to developing safe, ethical, and powerful artificial intelligence. Our mission is to ensure that transformative AI systems are aligned with human interests. We are seeking a Research Engineer to join our Pre-training team, responsible for developing the next generation of large language models. In this role, you will work at the intersection of cutting-edge research and practical engineering, contributing to the development of safe, steerable, and trustworthy AI systems. Key Responsibilities: Conduct research and implement solutions in areas such as model architecture, algorithms, data processing, and optimizer development Independently lead small research projects while collaborating with team members on larger initiatives Design, run, and analyze scientific experiments to advance our understanding of large language models Optimize and scale our training infrastructure to improve efficiency and reliability Develop and improve dev tooling to enhance team productivity Contribute to the entire stack, from low-level optimizations to high-level model design Qualifications: Advanced degree (MS or PhD) in Computer Science, Machine Learning, or a related field Strong software engineering skills with a proven track record of building complex systems Expertise in Python and experience with deep learning frameworks (PyTorch preferred) Familiarity with large-scale machine learning, particularly in the context of language models Ability to balance research goals with practical engineering constraints Strong problem-solving skills and a results-oriented mindset Excellent communication skills and ability to work in a collaborative environment Care about the societal impacts of your work Preferred Experience: Work on high-performance, large-scale ML systems Familiarity with GPUs, Kubernetes, and OS internals Experience with language modeling using transformer architectures Knowledge of reinforcement learning techniques Background in large-scale ETL processes You'll thrive in this role if you: Have significant software engineering experience Are results-oriented with a bias towards flexibility and impact Willingly take on tasks outside your job description to support the team Enjoy pair programming and collaborative work Are eager to learn more about machine learning research Are enthusiastic to work at an organization that functions as a single, cohesive team pursuing large-scale AI research projects Are working to align state of the art models with human values and preferences, understand and interpret deep neural networks, or develop new models to support these areas of research View research and engineering as two sides of the same coin, and seek to understand all aspects of our research program as well as possible, to maximize the impact of your insights Have ambitious goals for AI safety and general progress in the next few years, and you’re working to create the best outcomes over the long-term. Sample Projects: Optimizing the throughput of novel attention mechanisms Comparing compute efficiency of different Transformer variants Preparing large-scale datasets for efficient model consumption Scaling distributed training jobs to thousands of GPUs Designing fault tolerance strategies for our training infrastructure Creating interactive visualizations of model internals, such as attention patterns At Anthropic, we are committed to fostering a diverse and inclusive workplace. We strongly encourage applications from candidates of all backgrounds, including those from underrepresented groups in tech. If you're excited about pushing the boundaries of AI while prioritizing safety and ethics, we want to hear from you!The expected base compensation for this position is below. Our total compensation package for full-time employees includes equity, benefits, and may include incentive compensation.Annual Salary:$340,000 - $425,000 USDLogistics Education requirements: We require at least a Bachelor's degree in a related field or equivalent experience.Location-based hybrid policy: Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices. Visa sponsorship: We do sponsor visas! However, we aren't able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this. We encourage you to apply even if you do not believe you meet every single qualification. Not all strong candidates will meet every single qualification as listed. Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you're interested in this work. We think AI systems like the ones we're building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team. How we're different We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We're an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills. The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI & Compute, Concrete Problems in AI Safety, and Learning from Human Preferences. Come work with us! Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. Guidance on Candidates' AI Usage: Learn about our policy for using AI in our application process",
    "id": "job-boards-greenhouse-io-anthropic-jobs-4616971008",
    "source": "greenhouse",
    "source_url": "boards-api.greenhouse.io",
    "source_id": "4616971008",
    "title": "Research Engineer, Pre-training",
    "company_name": "anthropic",
    "company_slug": "anthropic",
    "location": "Remote-Friendly (Travel-Required) | San Francisco, CA | Seattle, WA | New York City, NY",
    "locations": [
      "Remote-Friendly (Travel-Required) | San Francisco, CA | Seattle, WA | New York City, NY"
    ],
    "url": "https://job-boards.greenhouse.io/anthropic/jobs/4616971008",
    "departments": [
      "AI Research & Engineering"
    ],
    "employment_type": null,
    "posted_at": "2025-11-22T09:28:05-05:00",
    "fetched_at": "2026-01-03T04:44:24.618Z",
    "description": "&lt;div class=&quot;content-intro&quot;&gt;&lt;h2&gt;&lt;strong&gt;About Anthropic&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.&lt;/p&gt;&lt;/div&gt;&lt;p&gt;Anthropic is at the forefront of AI research, dedicated to developing safe, ethical, and powerful artificial intelligence. Our mission is to ensure that transformative AI systems are aligned with human interests. We are seeking a Research Engineer to join our Pre-training team, responsible for developing the next generation of large language models. In this role, you will work at the intersection of cutting-edge research and practical engineering, contributing to the development of safe, steerable, and trustworthy AI systems.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;Key Responsibilities:&lt;/strong&gt;&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;Conduct research and implement solutions in areas such as model architecture, algorithms, data processing, and optimizer development&lt;/li&gt;\n&lt;li&gt;Independently lead small research projects while collaborating with team members on larger initiatives&lt;/li&gt;\n&lt;li&gt;Design, run, and analyze scientific experiments to advance our understanding of large language models&lt;/li&gt;\n&lt;li&gt;Optimize and scale our training infrastructure to improve efficiency and reliability&lt;/li&gt;\n&lt;li&gt;Develop and improve dev tooling to enhance team productivity&lt;/li&gt;\n&lt;li&gt;Contribute to the entire stack, from low-level optimizations to high-level model design&lt;/li&gt;\n&lt;/ul&gt;\n&lt;p&gt;&lt;strong&gt;Qualifications:&lt;/strong&gt;&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;Advanced degree (MS or PhD) in Computer Science, Machine Learning, or a related field&lt;/li&gt;\n&lt;li&gt;Strong software engineering skills with a proven track record of building complex systems&lt;/li&gt;\n&lt;li&gt;Expertise in Python and experience with deep learning frameworks (PyTorch preferred)&lt;/li&gt;\n&lt;li&gt;Familiarity with large-scale machine learning, particularly in the context of language models&lt;/li&gt;\n&lt;li&gt;Ability to balance research goals with practical engineering constraints&lt;/li&gt;\n&lt;li&gt;Strong problem-solving skills and a results-oriented mindset&lt;/li&gt;\n&lt;li&gt;Excellent communication skills and ability to work in a collaborative environment&lt;/li&gt;\n&lt;li&gt;Care about the societal impacts of your work&lt;/li&gt;\n&lt;/ul&gt;\n&lt;p&gt;&lt;strong&gt;Preferred Experience:&lt;/strong&gt;&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;Work on high-performance, large-scale ML systems&lt;/li&gt;\n&lt;li&gt;Familiarity with GPUs, Kubernetes, and OS internals&lt;/li&gt;\n&lt;li&gt;Experience with language modeling using transformer architectures&lt;/li&gt;\n&lt;li&gt;Knowledge of reinforcement learning techniques&lt;/li&gt;\n&lt;li&gt;Background in large-scale ETL processes&lt;/li&gt;\n&lt;/ul&gt;\n&lt;p&gt;&lt;strong&gt;You&#39;ll thrive in this role if you:&lt;/strong&gt;&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;Have significant software engineering experience&lt;/li&gt;\n&lt;li&gt;Are results-oriented with a bias towards flexibility and impact&lt;/li&gt;\n&lt;li&gt;Willingly take on tasks outside your job description to support the team&lt;/li&gt;\n&lt;li&gt;Enjoy pair programming and collaborative work&lt;/li&gt;\n&lt;li&gt;Are eager to learn more about machine learning research&lt;/li&gt;\n&lt;li&gt;Are enthusiastic to work at an organization that functions as a single, cohesive team pursuing large-scale AI research projects&lt;/li&gt;\n&lt;li&gt;Are working to align state of the art models with human values and preferences, understand and interpret deep neural networks, or develop new models to support these areas of research&lt;/li&gt;\n&lt;li&gt;View research and engineering as two sides of the same coin, and seek to understand all aspects of our research program as well as possible, to maximize the impact of your insights&lt;/li&gt;\n&lt;li&gt;Have ambitious goals for AI safety and general progress in the next few years, and you’re working to create the best outcomes over the long-term.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;p&gt;&lt;strong&gt;Sample Projects:&lt;/strong&gt;&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;Optimizing the throughput of novel attention mechanisms&lt;/li&gt;\n&lt;li&gt;Comparing compute efficiency of different Transformer variants&lt;/li&gt;\n&lt;li&gt;Preparing large-scale datasets for efficient model consumption&lt;/li&gt;\n&lt;li&gt;Scaling distributed training jobs to thousands of GPUs&lt;/li&gt;\n&lt;li&gt;Designing fault tolerance strategies for our training infrastructure&lt;/li&gt;\n&lt;li&gt;Creating interactive visualizations of model internals, such as attention patterns&lt;/li&gt;\n&lt;/ul&gt;\n&lt;p&gt;At Anthropic, we are committed to fostering a diverse and inclusive workplace. We strongly encourage applications from candidates of all backgrounds, including those from underrepresented groups in tech.&lt;/p&gt;\n&lt;p&gt;If you&#39;re excited about pushing the boundaries of AI while prioritizing safety and ethics, we want to hear from you!&lt;/p&gt;&lt;div class=&quot;content-pay-transparency&quot;&gt;&lt;div class=&quot;pay-input&quot;&gt;&lt;div class=&quot;description&quot;&gt;&lt;p&gt;The expected&amp;nbsp;base compensation for this position is below. Our total compensation package for full-time employees includes equity, benefits, and may include incentive compensation.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;title&quot;&gt;Annual Salary:&lt;/div&gt;&lt;div class=&quot;pay-range&quot;&gt;&lt;span&gt;$340,000&lt;/span&gt;&lt;span class=&quot;divider&quot;&gt;&amp;mdash;&lt;/span&gt;&lt;span&gt;$425,000 USD&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;content-conclusion&quot;&gt;&lt;h2&gt;&lt;strong&gt;Logistics&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;Education requirements: &lt;/strong&gt;We require at least a Bachelor&#39;s degree in a related field or equivalent experience.&lt;strong&gt;&lt;br&gt;&lt;br&gt;Location-based hybrid policy:&lt;/strong&gt; Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices.&lt;/p&gt;\n&lt;p&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Visa sponsorship:&lt;/strong&gt;&amp;nbsp;We do sponsor visas! However, we aren&#39;t able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;We encourage you to apply even if you do not believe you meet every single qualification.&lt;/strong&gt; Not all strong candidates will meet every single qualification as listed.&amp;nbsp; Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you&#39;re interested in this work. We think AI systems like the ones we&#39;re building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;How we&#39;re different&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We&#39;re an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills.&lt;/p&gt;\n&lt;p&gt;The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI &amp;amp; Compute, Concrete Problems in AI Safety, and Learning from Human Preferences.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Come work with us!&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. &lt;strong data-stringify-type=&quot;bold&quot;&gt;Guidance on Candidates&#39; AI Usage:&lt;/strong&gt;&amp;nbsp;Learn about&amp;nbsp;&lt;a class=&quot;c-link&quot; href=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; target=&quot;_blank&quot; data-stringify-link=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; data-sk=&quot;tooltip_parent&quot;&gt;our policy&lt;/a&gt;&amp;nbsp;for using AI in our application process&lt;/p&gt;&lt;/div&gt;",
    "_raw": {
      "source": "greenhouse",
      "original_id": 4616971008
    },
    "job_posted_at": "1h",
    "description_platform": "greenhouse",
    "description_success": true
  },
  {
    "job_title": "Research Engineer, Pretraining Scaling",
    "employer_name": "anthropic",
    "job_city": "San Francisco, CA",
    "job_apply_link": "https://job-boards.greenhouse.io/anthropic/jobs/4938432008",
    "job_posted_at_datetime_utc": "2025-11-22T09:28:05-05:00",
    "job_description": "About Anthropic Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.About the Role: Anthropic's ML Performance and Scaling team trains our production pretrained models, work that directly shapes the company's future and our mission to build safe, beneficial AI systems. As a Research Engineer on this team, you'll ensure our frontier models train reliably, efficiently, and at scale. This is demanding, high-impact work that requires both deep technical expertise and a genuine passion for the craft of large-scale ML systems. This role lives at the boundary between research and engineering. You'll work across our entire production training stack: performance optimization, hardware debugging, experimental design, and launch coordination. During launches, the team works in tight lockstep, responding to production issues that can't wait for tomorrow. Responsibilities: Own critical aspects of our production pretraining pipeline, including model operations, performance optimization, observability, and reliability Debug and resolve complex issues across the full stack—from hardware errors and networking to training dynamics and evaluation infrastructure Design and run experiments to improve training efficiency, reduce step time, increase uptime, and enhance model performance Respond to on-call incidents during model launches, diagnosing problems quickly and coordinating solutions across teams Build and maintain production logging, monitoring dashboards, and evaluation infrastructure Add new capabilities to the training codebase, such as long context support or novel architectures Collaborate closely with teammates across SF and London, as well as with Tokens, Architectures, and Systems teams Contribute to the team's institutional knowledge by documenting systems, debugging approaches, and lessons learned You May Be a Good Fit If You: Have hands-on experience training large language models, or deep expertise with JAX, TPU, PyTorch, or large-scale distributed systems Genuinely enjoy both research and engineering work—you'd describe your ideal split as roughly 50/50 rather than heavily weighted toward one or the other Are excited about being on-call for production systems, working long days during launches, and solving hard problems under pressure Thrive when working on whatever is most impactful, even if that changes day-to-day based on what the production model needs Excel at debugging complex, ambiguous problems across multiple layers of the stack Communicate clearly and collaborate effectively, especially when coordinating across time zones or during high-stress incidents Are passionate about the work itself and want to refine your craft as a research engineer Care about the societal impacts of AI and responsible scaling Strong Candidates May Also Have: Previous experience training LLM’s or working extensively with JAX/TPU, PyTorch, or other ML frameworks at scale Contributed to open-source LLM frameworks (e.g., open_lm, llm-foundry, mesh-transformer-jax) Published research on model training, scaling laws, or ML systems Experience with production ML systems, observability tools, or evaluation infrastructure Background as a systems engineer, quant, or in other roles requiring both technical depth and operational excellence What Makes This Role Unique: This is not a typical research engineering role. The work is highly operational—you'll be deeply involved in keeping our production models training smoothly, which means being responsive to incidents, flexible about priorities, and comfortable with uncertainty. During launches, the team often works extended hours and may need to respond to issues on evenings and weekends. However, this operational intensity comes with extraordinary learning opportunities. You'll gain hands-on experience with some of the largest, most sophisticated training runs in the industry. You'll work alongside world-class researchers and engineers, and the institutional knowledge you build will compound in ways that can't be easily transferred. For people who thrive on this type of work, it's uniquely rewarding. We're building a close-knit team of people who genuinely care about doing excellent work together. If you're someone who wants to be part of training the models that will define the future of AI—and you're excited about the full reality of what that entails—we'd love to hear from you. Location:This role requires working in-office 5 days per week in San Francisco. Deadline to apply: None. Applications will be reviewed on a rolling basis.The expected base compensation for this position is below. Our total compensation package for full-time employees includes equity, benefits, and may include incentive compensation.Annual Salary:$315,000 - $560,000 USDLogistics Education requirements: We require at least a Bachelor's degree in a related field or equivalent experience.Location-based hybrid policy: Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices. Visa sponsorship: We do sponsor visas! However, we aren't able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this. We encourage you to apply even if you do not believe you meet every single qualification. Not all strong candidates will meet every single qualification as listed. Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you're interested in this work. We think AI systems like the ones we're building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team. How we're different We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We're an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills. The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI & Compute, Concrete Problems in AI Safety, and Learning from Human Preferences. Come work with us! Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. Guidance on Candidates' AI Usage: Learn about our policy for using AI in our application process",
    "id": "job-boards-greenhouse-io-anthropic-jobs-4938432008",
    "source": "greenhouse",
    "source_url": "boards-api.greenhouse.io",
    "source_id": "4938432008",
    "title": "Research Engineer, Pretraining Scaling",
    "company_name": "anthropic",
    "company_slug": "anthropic",
    "location": "San Francisco, CA",
    "locations": [
      "San Francisco, CA"
    ],
    "url": "https://job-boards.greenhouse.io/anthropic/jobs/4938432008",
    "departments": [
      "AI Research & Engineering"
    ],
    "employment_type": null,
    "posted_at": "2025-11-22T09:28:05-05:00",
    "fetched_at": "2026-01-03T04:44:24.618Z",
    "description": "&lt;div class=&quot;content-intro&quot;&gt;&lt;h2&gt;&lt;strong&gt;About Anthropic&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.&lt;/p&gt;&lt;/div&gt;&lt;h2&gt;&lt;strong&gt;About the Role:&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic&#39;s ML Performance and Scaling team trains our production pretrained models, work that directly shapes the company&#39;s future and our mission to build safe, beneficial AI systems. As a Research Engineer on this team, you&#39;ll ensure our frontier models train reliably, efficiently, and at scale. This is demanding, high-impact work that requires both deep technical expertise and a genuine passion for the craft of large-scale ML systems.&lt;/p&gt;\n&lt;p&gt;This role lives at the boundary between research and engineering. You&#39;ll work across our entire production training stack: performance optimization, hardware debugging, experimental design, and launch coordination. During launches, the team works in tight lockstep, responding to production issues that can&#39;t wait for tomorrow.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Responsibilities:&amp;nbsp;&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Own critical aspects of our production pretraining pipeline, including model operations, performance optimization, observability, and reliability&lt;/li&gt;\n&lt;li&gt;Debug and resolve complex issues across the full stack—from hardware errors and networking to training dynamics and evaluation infrastructure&lt;/li&gt;\n&lt;li&gt;Design and run experiments to improve training efficiency, reduce step time, increase uptime, and enhance model performance&lt;/li&gt;\n&lt;li&gt;Respond to on-call incidents during model launches, diagnosing problems quickly and coordinating solutions across teams&lt;/li&gt;\n&lt;li&gt;Build and maintain production logging, monitoring dashboards, and evaluation infrastructure&lt;/li&gt;\n&lt;li&gt;Add new capabilities to the training codebase, such as long context support or novel architectures&lt;/li&gt;\n&lt;li&gt;Collaborate closely with teammates across SF and London, as well as with Tokens, Architectures, and Systems teams&lt;/li&gt;\n&lt;li&gt;Contribute to the team&#39;s institutional knowledge by documenting systems, debugging approaches, and lessons learned&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;&lt;strong&gt;You May Be a Good Fit If You:&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Have hands-on experience training large language models, or deep expertise with JAX, TPU, PyTorch, or large-scale distributed systems&lt;/li&gt;\n&lt;li&gt;Genuinely enjoy both research and engineering work—you&#39;d describe your ideal split as roughly 50/50 rather than heavily weighted toward one or the other&lt;/li&gt;\n&lt;li&gt;Are excited about being on-call for production systems, working long days during launches, and solving hard problems under pressure&lt;/li&gt;\n&lt;li&gt;Thrive when working on whatever is most impactful, even if that changes day-to-day based on what the production model needs&lt;/li&gt;\n&lt;li&gt;Excel at debugging complex, ambiguous problems across multiple layers of the stack&lt;/li&gt;\n&lt;li&gt;Communicate clearly and collaborate effectively, especially when coordinating across time zones or during high-stress incidents&lt;/li&gt;\n&lt;li&gt;Are passionate about the work itself and want to refine your craft as a research engineer&lt;/li&gt;\n&lt;li&gt;Care about the societal impacts of AI and responsible scaling&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;&lt;strong&gt;Strong Candidates May Also Have:&amp;nbsp;&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Previous experience training LLM’s or working extensively with JAX/TPU, PyTorch, or other ML frameworks at scale&lt;/li&gt;\n&lt;li&gt;Contributed to open-source LLM frameworks (e.g., open_lm, llm-foundry, mesh-transformer-jax)&lt;/li&gt;\n&lt;li&gt;Published research on model training, scaling laws, or ML systems&lt;/li&gt;\n&lt;li&gt;Experience with production ML systems, observability tools, or evaluation infrastructure&lt;/li&gt;\n&lt;li&gt;Background as a systems engineer, quant, or in other roles requiring both technical depth and operational excellence&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;&lt;strong&gt;What Makes This Role Unique:&amp;nbsp;&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;This is not a typical research engineering role. The work is highly operational—you&#39;ll be deeply involved in keeping our production models training smoothly, which means being responsive to incidents, flexible about priorities, and comfortable with uncertainty. During launches, the team often works extended hours and may need to respond to issues on evenings and weekends.&lt;/p&gt;\n&lt;p&gt;However, this operational intensity comes with extraordinary learning opportunities. You&#39;ll gain hands-on experience with some of the largest, most sophisticated training runs in the industry. You&#39;ll work alongside world-class researchers and engineers, and the institutional knowledge you build will compound in ways that can&#39;t be easily transferred. For people who thrive on this type of work, it&#39;s uniquely rewarding.&lt;/p&gt;\n&lt;p&gt;We&#39;re building a close-knit team of people who genuinely care about doing excellent work together. If you&#39;re someone who wants to be part of training the models that will define the future of AI—and you&#39;re excited about the full reality of what that entails—we&#39;d love to hear from you.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;Location:&lt;/strong&gt;This role requires working in-office 5 days per week in San Francisco.&amp;nbsp;&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;Deadline to apply:&lt;/strong&gt; None. Applications will be reviewed on a rolling basis.&lt;/p&gt;&lt;div class=&quot;content-pay-transparency&quot;&gt;&lt;div class=&quot;pay-input&quot;&gt;&lt;div class=&quot;description&quot;&gt;&lt;p&gt;The expected&amp;nbsp;base compensation for this position is below. Our total compensation package for full-time employees includes equity, benefits, and may include incentive compensation.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;title&quot;&gt;Annual Salary:&lt;/div&gt;&lt;div class=&quot;pay-range&quot;&gt;&lt;span&gt;$315,000&lt;/span&gt;&lt;span class=&quot;divider&quot;&gt;&amp;mdash;&lt;/span&gt;&lt;span&gt;$560,000 USD&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;content-conclusion&quot;&gt;&lt;h2&gt;&lt;strong&gt;Logistics&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;Education requirements: &lt;/strong&gt;We require at least a Bachelor&#39;s degree in a related field or equivalent experience.&lt;strong&gt;&lt;br&gt;&lt;br&gt;Location-based hybrid policy:&lt;/strong&gt; Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices.&lt;/p&gt;\n&lt;p&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Visa sponsorship:&lt;/strong&gt;&amp;nbsp;We do sponsor visas! However, we aren&#39;t able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;We encourage you to apply even if you do not believe you meet every single qualification.&lt;/strong&gt; Not all strong candidates will meet every single qualification as listed.&amp;nbsp; Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you&#39;re interested in this work. We think AI systems like the ones we&#39;re building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;How we&#39;re different&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We&#39;re an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills.&lt;/p&gt;\n&lt;p&gt;The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI &amp;amp; Compute, Concrete Problems in AI Safety, and Learning from Human Preferences.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Come work with us!&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. &lt;strong data-stringify-type=&quot;bold&quot;&gt;Guidance on Candidates&#39; AI Usage:&lt;/strong&gt;&amp;nbsp;Learn about&amp;nbsp;&lt;a class=&quot;c-link&quot; href=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; target=&quot;_blank&quot; data-stringify-link=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; data-sk=&quot;tooltip_parent&quot;&gt;our policy&lt;/a&gt;&amp;nbsp;for using AI in our application process&lt;/p&gt;&lt;/div&gt;",
    "_raw": {
      "source": "greenhouse",
      "original_id": 4938432008
    },
    "job_posted_at": "1h",
    "description_platform": "greenhouse",
    "description_success": true
  },
  {
    "job_title": "Research Engineer, Production Model Post Training",
    "employer_name": "anthropic",
    "job_city": "San Francisco, CA | New York City, NY | Seattle, WA",
    "job_apply_link": "https://job-boards.greenhouse.io/anthropic/jobs/4613592008",
    "job_posted_at_datetime_utc": "2025-11-22T09:28:05-05:00",
    "job_description": "About Anthropic Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.About the role Anthropic's production models undergo sophisticated post-training processes to enhance their capabilities, alignment, and safety. As a Research Engineer on our Post-Training team, you'll train our base models through the complete post-training stack to deliver the production Claude models that users interact with. You'll work at the intersection of cutting-edge research and production engineering, implementing, scaling, and improving post-training techniques like Constitutional AI, RLHF, and other alignment methodologies. Your work will directly impact the quality, safety, and capabilities of our production models. Note: For this role, we conduct all interviews in Python. This role may require responding to incidents on short-notice, including on weekends. Responsibilities: Implement and optimize post-training techniques at scale on frontier models Conduct research to develop and optimize post-training recipes that directly improve production model quality Design, build, and run robust, efficient pipelines for model fine-tuning and evaluation Develop tools to measure and improve model performance across various dimensions Collaborate with research teams to translate emerging techniques into production-ready implementations Debug complex issues in training pipelines and model behavior Help establish best practices for reliable, reproducible model post-training You may be a good fit if you: Thrive in controlled chaos and are energised, rather than overwhelmed, when juggling multiple urgent priorities Adapt quickly to changing priorities Maintain clarity when debugging complex, time-sensitive issues Have strong software engineering skills with experience building complex ML systems Are comfortable working with large-scale distributed systems and high-performance computing Have experience with training, fine-tuning, or evaluating large language models Can balance research exploration with engineering rigor and operational reliability Are adept at analyzing and debugging model training processes Enjoy collaborating across research and engineering disciplines Can navigate ambiguity and make progress in fast-moving research environments Strong candidates may also: Have experience with LLMs Have a keen interest in AI safety and responsible deployment We welcome candidates at various experience levels, with a preference for senior engineers who have hands-on experience with frontier AI systems. However, proficiency in Python, deep learning frameworks, and distributed computing is required for this role.The expected base compensation for this position is below. Our total compensation package for full-time employees includes equity, benefits, and may include incentive compensation.Annual Salary:$315,000 - $340,000 USDLogistics Education requirements: We require at least a Bachelor's degree in a related field or equivalent experience.Location-based hybrid policy: Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices. Visa sponsorship: We do sponsor visas! However, we aren't able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this. We encourage you to apply even if you do not believe you meet every single qualification. Not all strong candidates will meet every single qualification as listed. Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you're interested in this work. We think AI systems like the ones we're building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team. How we're different We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We're an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills. The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI & Compute, Concrete Problems in AI Safety, and Learning from Human Preferences. Come work with us! Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. Guidance on Candidates' AI Usage: Learn about our policy for using AI in our application process",
    "id": "job-boards-greenhouse-io-anthropic-jobs-4613592008",
    "source": "greenhouse",
    "source_url": "boards-api.greenhouse.io",
    "source_id": "4613592008",
    "title": "Research Engineer, Production Model Post Training",
    "company_name": "anthropic",
    "company_slug": "anthropic",
    "location": "San Francisco, CA | New York City, NY | Seattle, WA",
    "locations": [
      "San Francisco, CA | New York City, NY | Seattle, WA"
    ],
    "url": "https://job-boards.greenhouse.io/anthropic/jobs/4613592008",
    "departments": [
      "AI Research & Engineering"
    ],
    "employment_type": null,
    "posted_at": "2025-11-22T09:28:05-05:00",
    "fetched_at": "2026-01-03T04:44:24.618Z",
    "description": "&lt;div class=&quot;content-intro&quot;&gt;&lt;h2&gt;&lt;strong&gt;About Anthropic&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.&lt;/p&gt;&lt;/div&gt;&lt;h2&gt;About the role&lt;/h2&gt;\n&lt;p&gt;Anthropic&#39;s production models undergo sophisticated post-training processes to enhance their capabilities, alignment, and safety. As a Research Engineer on our Post-Training team, you&#39;ll train our base models through the complete post-training stack to deliver the production Claude models that users interact with.&lt;/p&gt;\n&lt;p&gt;You&#39;ll work at the intersection of cutting-edge research and production engineering, implementing, scaling, and improving post-training techniques like Constitutional AI, RLHF, and other alignment methodologies. Your work will directly impact the quality, safety, and capabilities of our production models.&lt;/p&gt;\n&lt;p&gt;&lt;em&gt;Note: For this role, we conduct all interviews in Python. This role may require responding to incidents on short-notice, including on weekends.&lt;/em&gt;&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Responsibilities:&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Implement and optimize post-training techniques at scale on frontier models&lt;/li&gt;\n&lt;li&gt;Conduct research to develop and optimize post-training recipes that directly improve production model quality&lt;/li&gt;\n&lt;li&gt;Design, build, and run robust, efficient pipelines for model fine-tuning and evaluation&lt;/li&gt;\n&lt;li&gt;Develop tools to measure and improve model performance across various dimensions&lt;/li&gt;\n&lt;li&gt;Collaborate with research teams to translate emerging techniques into production-ready implementations&lt;/li&gt;\n&lt;li&gt;Debug complex issues in training pipelines and model behavior&lt;/li&gt;\n&lt;li&gt;Help establish best practices for reliable, reproducible model post-training&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;&lt;strong&gt;You may be a good fit if you:&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li data-stringify-indent=&quot;0&quot; data-stringify-border=&quot;1&quot;&gt;Thrive in controlled chaos and&amp;nbsp;are energised, rather than overwhelmed, when juggling multiple urgent priorities&lt;/li&gt;\n&lt;li data-stringify-indent=&quot;0&quot; data-stringify-border=&quot;1&quot;&gt;Adapt quickly to changing priorities&lt;/li&gt;\n&lt;li data-stringify-indent=&quot;0&quot; data-stringify-border=&quot;1&quot;&gt;Maintain clarity when debugging complex, time-sensitive issues&lt;/li&gt;\n&lt;li&gt;Have strong software engineering skills with experience building complex ML systems&lt;/li&gt;\n&lt;li&gt;Are comfortable working with large-scale distributed systems and high-performance computing&lt;/li&gt;\n&lt;li&gt;Have experience with training, fine-tuning, or evaluating large language models&lt;/li&gt;\n&lt;li&gt;Can balance research exploration with engineering rigor and operational reliability&lt;/li&gt;\n&lt;li&gt;Are adept at analyzing and debugging model training processes&lt;/li&gt;\n&lt;li&gt;Enjoy collaborating across research and engineering disciplines&lt;/li&gt;\n&lt;li&gt;Can navigate ambiguity and make progress in fast-moving research environments&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;Strong candidates may also:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Have experience with LLMs&lt;/li&gt;\n&lt;li&gt;Have a keen interest in AI safety and responsible deployment&lt;/li&gt;\n&lt;/ul&gt;\n&lt;p&gt;We welcome candidates at various experience levels, with a preference for senior engineers who have hands-on experience with frontier AI systems. However, proficiency in Python, deep learning frameworks, and distributed computing is required for this role.&lt;/p&gt;&lt;div class=&quot;content-pay-transparency&quot;&gt;&lt;div class=&quot;pay-input&quot;&gt;&lt;div class=&quot;description&quot;&gt;&lt;p&gt;The expected&amp;nbsp;base compensation for this position is below. Our total compensation package for full-time employees includes equity, benefits, and may include incentive compensation.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;title&quot;&gt;Annual Salary:&lt;/div&gt;&lt;div class=&quot;pay-range&quot;&gt;&lt;span&gt;$315,000&lt;/span&gt;&lt;span class=&quot;divider&quot;&gt;&amp;mdash;&lt;/span&gt;&lt;span&gt;$340,000 USD&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;content-conclusion&quot;&gt;&lt;h2&gt;&lt;strong&gt;Logistics&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;Education requirements: &lt;/strong&gt;We require at least a Bachelor&#39;s degree in a related field or equivalent experience.&lt;strong&gt;&lt;br&gt;&lt;br&gt;Location-based hybrid policy:&lt;/strong&gt; Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices.&lt;/p&gt;\n&lt;p&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Visa sponsorship:&lt;/strong&gt;&amp;nbsp;We do sponsor visas! However, we aren&#39;t able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;We encourage you to apply even if you do not believe you meet every single qualification.&lt;/strong&gt; Not all strong candidates will meet every single qualification as listed.&amp;nbsp; Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you&#39;re interested in this work. We think AI systems like the ones we&#39;re building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;How we&#39;re different&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We&#39;re an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills.&lt;/p&gt;\n&lt;p&gt;The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI &amp;amp; Compute, Concrete Problems in AI Safety, and Learning from Human Preferences.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Come work with us!&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. &lt;strong data-stringify-type=&quot;bold&quot;&gt;Guidance on Candidates&#39; AI Usage:&lt;/strong&gt;&amp;nbsp;Learn about&amp;nbsp;&lt;a class=&quot;c-link&quot; href=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; target=&quot;_blank&quot; data-stringify-link=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; data-sk=&quot;tooltip_parent&quot;&gt;our policy&lt;/a&gt;&amp;nbsp;for using AI in our application process&lt;/p&gt;&lt;/div&gt;",
    "_raw": {
      "source": "greenhouse",
      "original_id": 4613592008
    },
    "job_posted_at": "1h",
    "description_platform": "greenhouse",
    "description_success": true
  },
  {
    "job_title": "Research Engineer / Research Scientist, Biology & Life Sciences",
    "employer_name": "anthropic",
    "job_city": "San Francisco, CA",
    "job_apply_link": "https://job-boards.greenhouse.io/anthropic/jobs/4924308008",
    "job_posted_at_datetime_utc": "2025-11-22T09:28:05-05:00",
    "job_description": "About Anthropic Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.About the Role We're seeking an exceptional Research Engineer / Research Scientist to join our Life Science team at Anthropic. Our team is organized around the north star goal of accelerating progress in the life sciences, from early discovery through translation, by an order of magnitude. Our team likes to think across the whole model stack. In this role, you'll combine your deep expertise in biology with machine learning engineering to develop novel evaluation frameworks and training strategies that push the frontier of what AI can achieve in biology. As a founding member of our team, you'll work at the intersection of cutting-edge AI and the biological sciences, developing rigorous methods to measure and improve model performance on complex scientific tasks. You'll collaborate closely with world-class researchers and engineers to build AI systems that can engage in all phases of research and development, while maintaining our commitment to safety and beneficial impact. Responsibilities: Design and implement evaluation methodologies for assessing AI model capabilities relevant to biological research and applications Develop and execute strategies to systematically improve model performance on scientific tasks Develop approaches to address long-horizon task completion and complex reasoning challenges essential for scientific discovery Collaborate with domain experts and partners to establish benchmarks and gather high-quality data Translate between biological domain knowledge and machine learning objectives You may be a good fit if you: Have 8+ years of machine learning experience, with demonstrated ability to train and evaluate large language models Have 5+ years of hands-on experience in life sciences R&D, with deep expertise in areas such as molecular biology, drug discovery, or computational biology Have a track record of bridging biological domain knowledge with computational approaches to solve real scientific problems Are proficient in Python and familiar with modern ML development practices Are comfortable navigating ambiguity and developing solutions in rapidly evolving research environments Can work independently while maintaining strong collaboration with cross-functional teams Are results-oriented, with a bias towards flexibility and impact Thrive in a fast-paced research environment where you balance rigorous scientific standards with rapid iteration Are passionate about using AI to accelerate scientific discovery while maintaining high ethical standards Have experience managing data pipelines and working with large-scale biological datasets Strong candidates may have: Ph.D. in a biological science (molecular biology, biochemistry, computational biology), in Machine Learning, or in a related field, or equivalent industry experience Published research or practical experience in scientific AI applications or long-horizon reasoning A history working on Reinforcement Learning and/or Pretraining Knowledge of containerization technologies (Docker, Kubernetes) and cloud deployment at scale Demonstrated ability to work across multiple domains (language modeling, systems engineering, scientific computing) Experience with modern machine learning techniques and model training methodologies Familiarity with biological databases (UniProt, GenBank, PDB) and computational biology tools Experience in drug discovery, including computational chemistry or structure-based design Knowledge of regulatory requirements for therapeutic development or clinical research Contributions to open-source scientific software or databases This role offers a unique opportunity to shape how AI transforms biological research. You'll work with some of the world's best AI researchers while tackling problems that matter deeply for human health and scientific understanding. If you're excited about using your expertise to guide the development of transformative AI systems, we want to hear from you. Deadline to apply: None. Applications will be reviewed on a rolling basis. The expected base compensation for this position is below. Our total compensation package for full-time employees includes equity, benefits, and may include incentive compensation.Annual Salary:$315,000 - $340,000 USDLogistics Education requirements: We require at least a Bachelor's degree in a related field or equivalent experience.Location-based hybrid policy: Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices. Visa sponsorship: We do sponsor visas! However, we aren't able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this. We encourage you to apply even if you do not believe you meet every single qualification. Not all strong candidates will meet every single qualification as listed. Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you're interested in this work. We think AI systems like the ones we're building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team. How we're different We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We're an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills. The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI & Compute, Concrete Problems in AI Safety, and Learning from Human Preferences. Come work with us! Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. Guidance on Candidates' AI Usage: Learn about our policy for using AI in our application process",
    "id": "job-boards-greenhouse-io-anthropic-jobs-4924308008",
    "source": "greenhouse",
    "source_url": "boards-api.greenhouse.io",
    "source_id": "4924308008",
    "title": "Research Engineer / Research Scientist, Biology & Life Sciences",
    "company_name": "anthropic",
    "company_slug": "anthropic",
    "location": "San Francisco, CA",
    "locations": [
      "San Francisco, CA"
    ],
    "url": "https://job-boards.greenhouse.io/anthropic/jobs/4924308008",
    "departments": [
      "AI Research & Engineering"
    ],
    "employment_type": null,
    "posted_at": "2025-11-22T09:28:05-05:00",
    "fetched_at": "2026-01-03T04:44:24.618Z",
    "description": "&lt;div class=&quot;content-intro&quot;&gt;&lt;h2&gt;&lt;strong&gt;About Anthropic&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.&lt;/p&gt;&lt;/div&gt;&lt;h2&gt;&lt;strong&gt;About the Role&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We&#39;re seeking an exceptional Research Engineer / Research Scientist to join our Life Science team at Anthropic. Our team is organized around the north star goal of accelerating progress in the life sciences, from early discovery through translation, by an order of magnitude. Our team likes to think across the whole model stack. In this role, you&#39;ll combine your deep expertise in biology with machine learning engineering to develop novel evaluation frameworks and training strategies that push the frontier of what AI can achieve in biology.&lt;/p&gt;\n&lt;p&gt;As a founding member of our team, you&#39;ll work at the intersection of cutting-edge AI and the biological sciences, developing rigorous methods to measure and improve model performance on complex scientific tasks. You&#39;ll collaborate closely with world-class researchers and engineers to build AI systems that can engage in all phases of research and development, while maintaining our commitment to safety and beneficial impact.&lt;/p&gt;\n&lt;h2&gt;Responsibilities:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Design and implement evaluation methodologies for assessing AI model capabilities relevant to biological research and applications&lt;/li&gt;\n&lt;li&gt;Develop and execute strategies to systematically improve model performance on scientific tasks&lt;/li&gt;\n&lt;li&gt;Develop approaches to address long-horizon task completion and complex reasoning challenges essential for scientific discovery&lt;/li&gt;\n&lt;li&gt;Collaborate with domain experts and partners to establish benchmarks and gather high-quality data&lt;/li&gt;\n&lt;li&gt;Translate between biological domain knowledge and machine learning objectives&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;You may be a good fit if you:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Have 8+ years of machine learning experience, with demonstrated ability to train and evaluate large language models&lt;/li&gt;\n&lt;li&gt;Have 5+ years of hands-on experience in life sciences R&amp;amp;D, with deep expertise in areas such as molecular biology, drug discovery, or computational biology&lt;/li&gt;\n&lt;li&gt;Have a track record of bridging biological domain knowledge with computational approaches to solve real scientific problems&lt;/li&gt;\n&lt;li&gt;Are proficient in Python and familiar with modern ML development practices&lt;/li&gt;\n&lt;li&gt;Are comfortable navigating ambiguity and developing solutions in rapidly evolving research environments&lt;/li&gt;\n&lt;li&gt;Can work independently while maintaining strong collaboration with cross-functional teams&lt;/li&gt;\n&lt;li&gt;Are results-oriented, with a bias towards flexibility and impact&lt;/li&gt;\n&lt;li&gt;Thrive in a fast-paced research environment where you balance rigorous scientific standards with rapid iteration&lt;/li&gt;\n&lt;li&gt;Are passionate about using AI to accelerate scientific discovery while maintaining high ethical standards&lt;/li&gt;\n&lt;li&gt;Have experience managing data pipelines and working with large-scale biological datasets&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;&lt;strong&gt;Strong candidates may have:&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Ph.D. in a biological science (molecular biology, biochemistry, computational biology), in Machine Learning, or in a related field, or equivalent industry experience&lt;/li&gt;\n&lt;li&gt;Published research or practical experience in scientific AI applications or long-horizon reasoning&lt;/li&gt;\n&lt;li&gt;A history working on Reinforcement Learning and/or Pretraining&lt;/li&gt;\n&lt;li&gt;Knowledge of containerization technologies (Docker, Kubernetes) and cloud deployment at scale&lt;/li&gt;\n&lt;li&gt;Demonstrated ability to work across multiple domains (language modeling, systems engineering, scientific computing)&lt;/li&gt;\n&lt;li&gt;Experience with modern machine learning techniques and model training methodologies&lt;/li&gt;\n&lt;li&gt;Familiarity with biological databases (UniProt, GenBank, PDB) and computational biology tools&lt;/li&gt;\n&lt;li&gt;Experience in drug discovery, including computational chemistry or structure-based design&lt;/li&gt;\n&lt;li&gt;Knowledge of regulatory requirements for therapeutic development or clinical research&lt;/li&gt;\n&lt;li&gt;Contributions to open-source scientific software or databases&lt;/li&gt;\n&lt;/ul&gt;\n&lt;p&gt;This role offers a unique opportunity to shape how AI transforms biological research. You&#39;ll work with some of the world&#39;s best AI researchers while tackling problems that matter deeply for human health and scientific understanding. If you&#39;re excited about using your expertise to guide the development of transformative AI systems, we want to hear from you.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;Deadline to apply:&amp;nbsp;&lt;/strong&gt;None. Applications will be reviewed on a rolling basis.&amp;nbsp;&lt;/p&gt;&lt;div class=&quot;content-pay-transparency&quot;&gt;&lt;div class=&quot;pay-input&quot;&gt;&lt;div class=&quot;description&quot;&gt;&lt;p&gt;The expected&amp;nbsp;base compensation for this position is below. Our total compensation package for full-time employees includes equity, benefits, and may include incentive compensation.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;title&quot;&gt;Annual Salary:&lt;/div&gt;&lt;div class=&quot;pay-range&quot;&gt;&lt;span&gt;$315,000&lt;/span&gt;&lt;span class=&quot;divider&quot;&gt;&amp;mdash;&lt;/span&gt;&lt;span&gt;$340,000 USD&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;content-conclusion&quot;&gt;&lt;h2&gt;&lt;strong&gt;Logistics&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;Education requirements: &lt;/strong&gt;We require at least a Bachelor&#39;s degree in a related field or equivalent experience.&lt;strong&gt;&lt;br&gt;&lt;br&gt;Location-based hybrid policy:&lt;/strong&gt; Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices.&lt;/p&gt;\n&lt;p&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Visa sponsorship:&lt;/strong&gt;&amp;nbsp;We do sponsor visas! However, we aren&#39;t able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;We encourage you to apply even if you do not believe you meet every single qualification.&lt;/strong&gt; Not all strong candidates will meet every single qualification as listed.&amp;nbsp; Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you&#39;re interested in this work. We think AI systems like the ones we&#39;re building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;How we&#39;re different&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We&#39;re an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills.&lt;/p&gt;\n&lt;p&gt;The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI &amp;amp; Compute, Concrete Problems in AI Safety, and Learning from Human Preferences.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Come work with us!&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. &lt;strong data-stringify-type=&quot;bold&quot;&gt;Guidance on Candidates&#39; AI Usage:&lt;/strong&gt;&amp;nbsp;Learn about&amp;nbsp;&lt;a class=&quot;c-link&quot; href=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; target=&quot;_blank&quot; data-stringify-link=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; data-sk=&quot;tooltip_parent&quot;&gt;our policy&lt;/a&gt;&amp;nbsp;for using AI in our application process&lt;/p&gt;&lt;/div&gt;",
    "_raw": {
      "source": "greenhouse",
      "original_id": 4924308008
    },
    "job_posted_at": "1h",
    "description_platform": "greenhouse",
    "description_success": true
  },
  {
    "job_title": " Research Engineer / Research Scientist, Tokens",
    "employer_name": "anthropic",
    "job_city": "New York City, NY; New York City, NY | Seattle, WA; San Francisco, CA",
    "job_apply_link": "https://job-boards.greenhouse.io/anthropic/jobs/4951814008",
    "job_posted_at_datetime_utc": "2025-11-22T09:28:05-05:00",
    "job_description": "About Anthropic Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.You want to build large scale ML systems from the ground up. You care about making safe, steerable, trustworthy systems. As a Research Engineer, you'll touch all parts of our code and infrastructure, whether that's making the cluster more reliable for our big jobs, improving throughput and efficiency, running and designing scientific experiments, or improving our dev tooling. You're excited to write code when you understand the research context and more broadly why it's important. Note: This is an \"evergreen\" role that we keep open on an ongoing basis. We receive many applications for this position, and you may not hear back from us directly if we do not currently have an open role on any of our teams that matches your skills and experience. We encourage you to apply despite this, as we are continually evaluating for top talent to join our team. You are also welcome to reapply as you gain more experience, but we suggest only reapplying once per year. We may also put up separate, team-specific job postings. In those cases, the teams will give preference to candidates who apply to the team-specific postings, so if you are interested in a specific team please make sure to check for team-specific job postings! You may be a good fit if you: Have significant software engineering experience Are results-oriented, with a bias towards flexibility and impact Pick up slack, even if it goes outside your job description Enjoy pair programming (we love to pair!) Want to learn more about machine learning research Care about the societal impacts of your work Strong candidates may also have experience with: High performance, large-scale ML systems GPUs, Kubernetes, Pytorch, or OS internals Language modeling with transformers Reinforcement learning Large-scale ETL Representative projects: Optimizing the throughput of a new attention mechanism Comparing the compute efficiency of two Transformer variants Making a Wikipedia dataset in a format models can easily consume Scaling a distributed training job to thousands of GPUs Writing a design doc for fault tolerance strategies Creating an interactive visualization of attention between tokens in a language model The expected base compensation for this position is below. Our total compensation package for full-time employees includes equity, benefits, and may include incentive compensation.Annual Salary:$340,000 - $425,000 USDLogistics Education requirements: We require at least a Bachelor's degree in a related field or equivalent experience.Location-based hybrid policy: Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices. Visa sponsorship: We do sponsor visas! However, we aren't able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this. We encourage you to apply even if you do not believe you meet every single qualification. Not all strong candidates will meet every single qualification as listed. Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you're interested in this work. We think AI systems like the ones we're building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team. How we're different We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We're an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills. The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI & Compute, Concrete Problems in AI Safety, and Learning from Human Preferences. Come work with us! Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. Guidance on Candidates' AI Usage: Learn about our policy for using AI in our application process",
    "id": "job-boards-greenhouse-io-anthropic-jobs-4951814008",
    "source": "greenhouse",
    "source_url": "boards-api.greenhouse.io",
    "source_id": "4951814008",
    "title": " Research Engineer / Research Scientist, Tokens",
    "company_name": "anthropic",
    "company_slug": "anthropic",
    "location": "New York City, NY; New York City, NY | Seattle, WA; San Francisco, CA",
    "locations": [
      "New York City, NY; New York City, NY | Seattle, WA; San Francisco, CA"
    ],
    "url": "https://job-boards.greenhouse.io/anthropic/jobs/4951814008",
    "departments": [
      "AI Research & Engineering"
    ],
    "employment_type": null,
    "posted_at": "2025-11-22T09:28:05-05:00",
    "fetched_at": "2026-01-03T04:44:24.618Z",
    "description": "&lt;div class=&quot;content-intro&quot;&gt;&lt;h2&gt;&lt;strong&gt;About Anthropic&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;You want to build large scale ML systems from the ground up. You care about making safe, steerable, trustworthy systems. As a Research Engineer, you&#39;ll touch all parts of our code and infrastructure, whether that&#39;s making the cluster more reliable for our big jobs, improving throughput and efficiency, running and designing scientific experiments, or improving our dev tooling. You&#39;re excited to write code when you understand the research context and more broadly why it&#39;s important.&lt;/div&gt;\n&lt;div&gt;&amp;nbsp;&lt;/div&gt;\n&lt;div&gt;&lt;em&gt;Note: This is an &quot;evergreen&quot; role that we keep open on an ongoing basis. We receive many applications for this position, and you may not hear back from us directly if we do not currently have an open role on any of our teams that matches your skills and experience. We encourage you to apply despite this, as we are continually evaluating for top talent to join our team. You are also welcome to reapply as you gain more experience, but we suggest only reapplying once per year.&lt;/em&gt;&lt;/div&gt;\n&lt;div&gt;&amp;nbsp;&lt;/div&gt;\n&lt;div&gt;&lt;em&gt;We may also put up separate, team-specific&amp;nbsp;&lt;/em&gt;&lt;a class=&quot;postings-link&quot; href=&quot;https://www.anthropic.com/jobs&quot;&gt;&lt;em&gt;job postings&lt;/em&gt;&lt;/a&gt;&lt;em&gt;. In those cases, the teams will give preference to candidates who apply to the team-specific postings, so if you are interested in a specific team please make sure to check for team-specific job postings!&lt;/em&gt;&lt;/div&gt;\n&lt;div&gt;\n&lt;div class=&quot;section page-centered&quot;&gt;\n&lt;div&gt;\n&lt;h2&gt;You may be a good fit if you:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Have significant software engineering experience&lt;/li&gt;\n&lt;li&gt;Are results-oriented, with a bias towards flexibility and impact&lt;/li&gt;\n&lt;li&gt;Pick up slack, even if it goes outside your job description&lt;/li&gt;\n&lt;li&gt;Enjoy pair programming (we love to pair!)&lt;/li&gt;\n&lt;li&gt;Want to learn more about machine learning research&lt;/li&gt;\n&lt;li&gt;Care about the societal impacts of your work&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div class=&quot;section page-centered&quot;&gt;\n&lt;div&gt;\n&lt;h2&gt;Strong candidates may also have experience with:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;High performance, large-scale ML systems&lt;/li&gt;\n&lt;li&gt;GPUs, Kubernetes, Pytorch, or OS internals&lt;/li&gt;\n&lt;li&gt;Language modeling with transformers&lt;/li&gt;\n&lt;li&gt;Reinforcement learning&lt;/li&gt;\n&lt;li&gt;Large-scale ETL&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div class=&quot;section page-centered&quot;&gt;\n&lt;div&gt;\n&lt;h2&gt;Representative projects:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Optimizing the throughput of a new attention mechanism&lt;/li&gt;\n&lt;li&gt;Comparing the compute efficiency of two Transformer variants&lt;/li&gt;\n&lt;li&gt;Making a Wikipedia dataset in a format models can easily consume&lt;/li&gt;\n&lt;li&gt;Scaling a distributed training job to thousands of GPUs&lt;/li&gt;\n&lt;li&gt;Writing a design doc for fault tolerance strategies&lt;/li&gt;\n&lt;li&gt;Creating an interactive visualization of attention between tokens in a language model&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;&lt;div class=&quot;content-pay-transparency&quot;&gt;&lt;div class=&quot;pay-input&quot;&gt;&lt;div class=&quot;description&quot;&gt;&lt;p&gt;The expected&amp;nbsp;base compensation for this position is below. Our total compensation package for full-time employees includes equity, benefits, and may include incentive compensation.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;title&quot;&gt;Annual Salary:&lt;/div&gt;&lt;div class=&quot;pay-range&quot;&gt;&lt;span&gt;$340,000&lt;/span&gt;&lt;span class=&quot;divider&quot;&gt;&amp;mdash;&lt;/span&gt;&lt;span&gt;$425,000 USD&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;content-conclusion&quot;&gt;&lt;h2&gt;&lt;strong&gt;Logistics&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;Education requirements: &lt;/strong&gt;We require at least a Bachelor&#39;s degree in a related field or equivalent experience.&lt;strong&gt;&lt;br&gt;&lt;br&gt;Location-based hybrid policy:&lt;/strong&gt; Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices.&lt;/p&gt;\n&lt;p&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Visa sponsorship:&lt;/strong&gt;&amp;nbsp;We do sponsor visas! However, we aren&#39;t able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;We encourage you to apply even if you do not believe you meet every single qualification.&lt;/strong&gt; Not all strong candidates will meet every single qualification as listed.&amp;nbsp; Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you&#39;re interested in this work. We think AI systems like the ones we&#39;re building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;How we&#39;re different&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We&#39;re an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills.&lt;/p&gt;\n&lt;p&gt;The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI &amp;amp; Compute, Concrete Problems in AI Safety, and Learning from Human Preferences.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Come work with us!&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. &lt;strong data-stringify-type=&quot;bold&quot;&gt;Guidance on Candidates&#39; AI Usage:&lt;/strong&gt;&amp;nbsp;Learn about&amp;nbsp;&lt;a class=&quot;c-link&quot; href=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; target=&quot;_blank&quot; data-stringify-link=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; data-sk=&quot;tooltip_parent&quot;&gt;our policy&lt;/a&gt;&amp;nbsp;for using AI in our application process&lt;/p&gt;&lt;/div&gt;",
    "_raw": {
      "source": "greenhouse",
      "original_id": 4951814008
    },
    "job_posted_at": "1h",
    "description_platform": "greenhouse",
    "description_success": true
  },
  {
    "job_title": "Research Engineer / Scientist, Alignment Science",
    "employer_name": "anthropic",
    "job_city": "San Francisco, CA",
    "job_apply_link": "https://job-boards.greenhouse.io/anthropic/jobs/4631822008",
    "job_posted_at_datetime_utc": "2025-11-22T09:28:05-05:00",
    "job_description": "About Anthropic Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems. About the role: You want to build and run elegant and thorough machine learning experiments to help us understand and steer the behavior of powerful AI systems. You care about making AI helpful, honest, and harmless, and are interested in the ways that this could be challenging in the context of human-level capabilities. You could describe yourself as both a scientist and an engineer. As a Research Engineer on Alignment Science, you'll contribute to exploratory experimental research on AI safety, with a focus on risks from powerful future systems (like those we would designate as ASL-3 or ASL-4 under our Responsible Scaling Policy), often in collaboration with other teams including Interpretability, Fine-Tuning, and the Frontier Red Team. Our blog provides an overview of topics that the Alignment Science team is either currently exploring or has previously explored. Our current topics of focus include... Scalable Oversight: Developing techniques to keep highly capable models helpful and honest, even as they surpass human-level intelligence in various domains. AI Control: Creating methods to ensure advanced AI systems remain safe and harmless in unfamiliar or adversarial scenarios. Alignment Stress-testing: Creating model organisms of misalignment to improve our empirical understanding of how alignment failures might arise. Automated Alignment Research: Building and aligning a system that can speed up & improve alignment research. Alignment Assessments: Understanding and documenting the highest-stakes and most concerning emerging properties of models through pre-deployment alignment and welfare assessments (see our Claude 4 System Card), misalignment-risk safety cases, and coordination with third-party evaluators. Safeguards Research: Developing robust defenses against adversarial attacks, comprehensive evaluation frameworks for model safety, and automated systems to detect and mitigate potential risks before deployment. Model Welfare: Investigating and addressing potential model welfare, moral status, and related questions. See our program announcement and welfare assessment in the Claude 4 system card for more. Note: For this role, we conduct all interviews in Python and prefer candidates to be based in the Bay Area. Representative projects: Testing the robustness of our safety techniques by training language models to subvert our safety techniques, and seeing how effective they are at subverting our interventions. Run multi-agent reinforcement learning experiments to test out techniques like AI Debate. Build tooling to efficiently evaluate the effectiveness of novel LLM-generated jailbreaks. Write scripts and prompts to efficiently produce evaluation questions to test models’ reasoning abilities in safety-relevant contexts. Contribute ideas, figures, and writing to research papers, blog posts, and talks. Run experiments that feed into key AI safety efforts at Anthropic, like the design and implementation of our Responsible Scaling Policy. You may be a good fit if you: Have significant software, ML, or research engineering experience Have some experience contributing to empirical AI research projects Have some familiarity with technical AI safety research Prefer fast-moving collaborative projects to extensive solo efforts Pick up slack, even if it goes outside your job description Care about the impacts of AI Strong candidates may also: Have experience authoring research papers in machine learning, NLP, or AI safety Have experience with LLMs Have experience with reinforcement learning Have experience with Kubernetes clusters and complex shared codebases Candidates need not have: 100% of the skills needed to perform the job Formal certifications or education credentials The expected base compensation for this position is below. Our total compensation package for full-time employees includes equity, benefits, and may include incentive compensation.Annual Salary:$315,000 - $340,000 USDLogistics Education requirements: We require at least a Bachelor's degree in a related field or equivalent experience.Location-based hybrid policy: Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices. Visa sponsorship: We do sponsor visas! However, we aren't able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this. We encourage you to apply even if you do not believe you meet every single qualification. Not all strong candidates will meet every single qualification as listed. Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you're interested in this work. We think AI systems like the ones we're building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team. How we're different We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We're an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills. The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI & Compute, Concrete Problems in AI Safety, and Learning from Human Preferences. Come work with us! Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. Guidance on Candidates' AI Usage: Learn about our policy for using AI in our application process",
    "id": "job-boards-greenhouse-io-anthropic-jobs-4631822008",
    "source": "greenhouse",
    "source_url": "boards-api.greenhouse.io",
    "source_id": "4631822008",
    "title": "Research Engineer / Scientist, Alignment Science",
    "company_name": "anthropic",
    "company_slug": "anthropic",
    "location": "San Francisco, CA",
    "locations": [
      "San Francisco, CA"
    ],
    "url": "https://job-boards.greenhouse.io/anthropic/jobs/4631822008",
    "departments": [
      "AI Research & Engineering"
    ],
    "employment_type": null,
    "posted_at": "2025-11-22T09:28:05-05:00",
    "fetched_at": "2026-01-03T04:44:24.618Z",
    "description": "&lt;div class=&quot;content-intro&quot;&gt;&lt;h2&gt;&lt;strong&gt;About Anthropic&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;\n&lt;h2&gt;About the role:&lt;/h2&gt;\n&lt;/div&gt;\n&lt;div&gt;You want to build and run elegant and thorough machine learning experiments to help us understand and steer the behavior of powerful AI systems. You care about making AI helpful, honest, and harmless, and are interested in the ways that this could be challenging in the context of human-level capabilities. You could describe yourself as both a scientist and an engineer. As a Research Engineer on Alignment Science, you&#39;ll contribute to exploratory experimental research on AI safety, with a focus on risks from powerful future systems (like those we would designate as ASL-3 or ASL-4 under our &lt;a class=&quot;postings-link&quot; href=&quot;https://www.anthropic.com/news/anthropics-responsible-scaling-policy&quot;&gt;Responsible Scaling Policy&lt;/a&gt;), often in collaboration with other teams including Interpretability, Fine-Tuning, and the Frontier Red Team.&lt;/div&gt;\n&lt;div&gt;&amp;nbsp;&lt;/div&gt;\n&lt;div&gt;&lt;a href=&quot;https://alignment.anthropic.com/&quot;&gt;Our blog&lt;/a&gt; provides an overview of topics that the Alignment Science team is either currently exploring or has previously explored. Our current topics of focus include...\n&lt;ul class=&quot;p-rich_text_list p-rich_text_list__bullet p-rich_text_list--nested&quot; data-stringify-type=&quot;unordered-list&quot; data-list-tree=&quot;true&quot; data-indent=&quot;0&quot; data-border=&quot;0&quot;&gt;\n&lt;li data-stringify-indent=&quot;0&quot; data-stringify-border=&quot;0&quot;&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Scalable Oversight:&amp;nbsp;&lt;/strong&gt;Developing techniques to keep highly capable models helpful and honest, even as they surpass human-level intelligence in various domains.&lt;/li&gt;\n&lt;li data-stringify-indent=&quot;0&quot; data-stringify-border=&quot;0&quot;&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;AI Control:&amp;nbsp;&lt;/strong&gt;Creating methods to ensure advanced AI systems remain safe and harmless in unfamiliar or adversarial scenarios.&lt;/li&gt;\n&lt;li data-stringify-indent=&quot;0&quot; data-stringify-border=&quot;0&quot;&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;&lt;a class=&quot;c-link&quot; href=&quot;https://www.lesswrong.com/posts/EPDSdXr8YbsDkgsDG/introducing-alignment-stress-testing-at-anthropic&quot; target=&quot;_blank&quot; data-stringify-link=&quot;https://www.lesswrong.com/posts/EPDSdXr8YbsDkgsDG/introducing-alignment-stress-testing-at-anthropic&quot; data-sk=&quot;tooltip_parent&quot;&gt;Alignment Stress-testing&lt;/a&gt;&lt;/strong&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;:&lt;/strong&gt;&amp;nbsp;Creating&amp;nbsp;&lt;a class=&quot;c-link&quot; href=&quot;https://www.lesswrong.com/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1&quot; target=&quot;_blank&quot; data-stringify-link=&quot;https://www.lesswrong.com/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1&quot; data-sk=&quot;tooltip_parent&quot;&gt;model organisms of misalignment&lt;/a&gt;&amp;nbsp;to improve our empirical understanding of how alignment failures might arise.&lt;/li&gt;\n&lt;li data-stringify-indent=&quot;0&quot; data-stringify-border=&quot;0&quot;&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Automated Alignment Research:&amp;nbsp;&lt;/strong&gt;Building and aligning a system that can speed up &amp;amp; improve alignment research.&lt;/li&gt;\n&lt;li class=&quot;whitespace-normal break-words&quot;&gt;&lt;strong&gt;Alignment Assessments&lt;/strong&gt;: Understanding and documenting the highest-stakes and most concerning emerging properties of models through pre-deployment alignment and welfare assessments (see our &lt;span class=&quot;s1&quot;&gt;&lt;a href=&quot;https://www-cdn.anthropic.com/6be99a52cb68eb70eb9572b4cafad13df32ed995.pdf&quot;&gt;&lt;span class=&quot;s2&quot;&gt;Claude 4 System Card&lt;/span&gt;&lt;/a&gt;)&lt;/span&gt;, misalignment-risk safety cases, and coordination with third-party evaluators.&lt;/li&gt;\n&lt;li class=&quot;whitespace-normal break-words&quot;&gt;&lt;a href=&quot;https://job-boards.greenhouse.io/anthropic/jobs/4459012008&quot;&gt;&lt;strong&gt;Safeguards Research&lt;/strong&gt;&lt;/a&gt;: Developing robust defenses against adversarial attacks, comprehensive evaluation frameworks for model safety, and automated systems to detect and mitigate potential risks before deployment.&lt;/li&gt;\n&lt;li class=&quot;whitespace-normal break-words&quot;&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Model Welfare:&amp;nbsp;&lt;/strong&gt;Investigating and addressing potential model welfare, moral status, and related questions. See our&amp;nbsp;&lt;a class=&quot;c-link&quot; href=&quot;https://www.anthropic.com/research/exploring-model-welfare&quot; target=&quot;_blank&quot; data-stringify-link=&quot;https://www.anthropic.com/research/exploring-model-welfare&quot; data-sk=&quot;tooltip_parent&quot;&gt;program announcement&lt;/a&gt;&amp;nbsp;and welfare assessment in the&amp;nbsp;&lt;a class=&quot;c-link&quot; href=&quot;https://www-cdn.anthropic.com/07b2a3f9902ee19fe39a36ca638e5ae987bc64dd.pdf&quot; target=&quot;_blank&quot; data-stringify-link=&quot;https://www-cdn.anthropic.com/07b2a3f9902ee19fe39a36ca638e5ae987bc64dd.pdf&quot; data-sk=&quot;tooltip_parent&quot;&gt;Claude 4 system card&lt;/a&gt;&amp;nbsp;for more.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;p&gt;&lt;em&gt;Note: For this role, we conduct all interviews in Python and prefer candidates to be based in the Bay Area.&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;div&gt;\n&lt;div class=&quot;section page-centered&quot;&gt;\n&lt;div&gt;\n&lt;h2&gt;Representative projects:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Testing the robustness of our safety techniques by training language models to subvert our safety techniques, and seeing how effective they are at subverting our interventions.&lt;/li&gt;\n&lt;li&gt;Run multi-agent reinforcement learning experiments to test out techniques like&amp;nbsp;&lt;a class=&quot;postings-link&quot; href=&quot;https://arxiv.org/abs/1805.00899&quot;&gt;AI Debate&lt;/a&gt;.&lt;/li&gt;\n&lt;li&gt;Build tooling to efficiently evaluate the effectiveness of novel LLM-generated jailbreaks.&lt;/li&gt;\n&lt;li&gt;Write scripts and prompts to efficiently produce evaluation questions to test models’ reasoning abilities in safety-relevant contexts.&lt;/li&gt;\n&lt;li&gt;Contribute ideas, figures, and writing to research papers, blog posts, and talks.&lt;/li&gt;\n&lt;li&gt;Run experiments that feed into key AI safety efforts at Anthropic, like the design and implementation of our&amp;nbsp;&lt;a class=&quot;postings-link&quot; href=&quot;https://www.anthropic.com/news/anthropics-responsible-scaling-policy&quot;&gt;Responsible Scaling Policy&lt;/a&gt;.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div class=&quot;section page-centered&quot;&gt;\n&lt;div&gt;\n&lt;h2&gt;You may be a good fit if you:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Have significant software, ML, or research engineering experience&lt;/li&gt;\n&lt;li&gt;Have some experience contributing to empirical AI research projects&lt;/li&gt;\n&lt;li&gt;Have some familiarity with technical AI safety research&lt;/li&gt;\n&lt;li&gt;Prefer fast-moving collaborative projects to extensive solo efforts&lt;/li&gt;\n&lt;li&gt;Pick up slack, even if it goes outside your job description&lt;/li&gt;\n&lt;li&gt;Care about the impacts of AI&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div class=&quot;section page-centered&quot;&gt;\n&lt;div&gt;\n&lt;h2&gt;Strong candidates may also:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Have experience authoring research papers in machine learning, NLP, or AI safety&lt;/li&gt;\n&lt;li&gt;Have experience with LLMs&lt;/li&gt;\n&lt;li&gt;Have experience with reinforcement learning&lt;/li&gt;\n&lt;li&gt;Have experience with Kubernetes clusters and complex shared codebases&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div class=&quot;section page-centered&quot;&gt;\n&lt;div&gt;\n&lt;h2&gt;Candidates need not have:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;100% of the skills needed to perform the job&lt;/li&gt;\n&lt;li&gt;Formal certifications or education credentials&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;&lt;div class=&quot;content-pay-transparency&quot;&gt;&lt;div class=&quot;pay-input&quot;&gt;&lt;div class=&quot;description&quot;&gt;&lt;p&gt;The expected&amp;nbsp;base compensation for this position is below. Our total compensation package for full-time employees includes equity, benefits, and may include incentive compensation.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;title&quot;&gt;Annual Salary:&lt;/div&gt;&lt;div class=&quot;pay-range&quot;&gt;&lt;span&gt;$315,000&lt;/span&gt;&lt;span class=&quot;divider&quot;&gt;&amp;mdash;&lt;/span&gt;&lt;span&gt;$340,000 USD&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;content-conclusion&quot;&gt;&lt;h2&gt;&lt;strong&gt;Logistics&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;Education requirements: &lt;/strong&gt;We require at least a Bachelor&#39;s degree in a related field or equivalent experience.&lt;strong&gt;&lt;br&gt;&lt;br&gt;Location-based hybrid policy:&lt;/strong&gt; Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices.&lt;/p&gt;\n&lt;p&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Visa sponsorship:&lt;/strong&gt;&amp;nbsp;We do sponsor visas! However, we aren&#39;t able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;We encourage you to apply even if you do not believe you meet every single qualification.&lt;/strong&gt; Not all strong candidates will meet every single qualification as listed.&amp;nbsp; Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you&#39;re interested in this work. We think AI systems like the ones we&#39;re building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;How we&#39;re different&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We&#39;re an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills.&lt;/p&gt;\n&lt;p&gt;The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI &amp;amp; Compute, Concrete Problems in AI Safety, and Learning from Human Preferences.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Come work with us!&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. &lt;strong data-stringify-type=&quot;bold&quot;&gt;Guidance on Candidates&#39; AI Usage:&lt;/strong&gt;&amp;nbsp;Learn about&amp;nbsp;&lt;a class=&quot;c-link&quot; href=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; target=&quot;_blank&quot; data-stringify-link=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; data-sk=&quot;tooltip_parent&quot;&gt;our policy&lt;/a&gt;&amp;nbsp;for using AI in our application process&lt;/p&gt;&lt;/div&gt;",
    "_raw": {
      "source": "greenhouse",
      "original_id": 4631822008
    },
    "job_posted_at": "1h",
    "description_platform": "greenhouse",
    "description_success": true
  },
  {
    "job_title": "Research Scientist, Interpretability",
    "employer_name": "anthropic",
    "job_city": "San Francisco, CA",
    "job_apply_link": "https://job-boards.greenhouse.io/anthropic/jobs/4980427008",
    "job_posted_at_datetime_utc": "2025-11-22T09:28:05-05:00",
    "job_description": "About Anthropic Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.About the role: When you see what modern language models are capable of, do you wonder, \"How do these things work? How can we trust them?\" The Interpretability team at Anthropic is working to reverse-engineer how trained models work because we believe that a mechanistic understanding is the most robust way to make advanced systems safe. We’re looking for researchers and engineers to join our efforts. People mean many different things by \"interpretability\". We're focused on mechanistic interpretability, which aims to discover how neural network parameters map to meaningful algorithms. Some useful analogies might be to think of us as trying to do \"biology\" or \"neuroscience\" of neural networks using “microscopes” we build, or as treating neural networks as binary computer programs we're trying to \"reverse engineer\". A few places to learn more about our work and team at a high level are this introduction to Interpretability from our research lead, Chris Olah; a discussion of our work on the Hard Fork podcast produced by the New York Times, and this blog post (and accompanying video) sharing more about some of the engineering challenges we’d had to solve to get these results. Some of our team's notable publications include A Mathematical Framework for Transformer Circuits, In-context Learning and Induction Heads, Toy Models of Superposition, Scaling Monosemanticity, and our Circuits’ Methods and Biology papers. This work builds on ideas from members' work prior to Anthropic such as the original circuits thread, Multimodal Neurons, Activation Atlases, and Building Blocks. We aim to create a solid foundation for mechanistically understanding neural networks and making them safe (see our vision post). In the short term, we have focused on resolving the issue of \"superposition\" (see Toy Models of Superposition, Superposition, Memorization, and Double Descent, and our May 2023 update), which causes the computational units of the models, like neurons and attention heads, to be individually uninterpretable, and on finding ways to decompose models into more interpretable components. Our subsequent work found millions of features in Sonnet, one of our production language models, represents progress in this direction. In our most recent work, we develop methods that allow us to build circuits using features and use this circuits to understand the mechanisms associated with a model's computation and study specific examples of multi-hop reasoning, planning, and chain-of-thought faithfulness on Haiku 3.5, one of our production models.” This is a stepping stone towards our overall goal of mechanistically understanding neural networks. We often collaborate with teams across Anthropic, such as Alignment Science and Societal Impacts to use our work to make Anthropic’s models safer. We also have an Interpretability Architectures project that involves collaborating with Pretraining. Responsibilities: Develop methods for understanding LLMs by reverse engineering algorithms learned in their weights Design and run robust experiments, both quickly in toy scenarios and at scale in large models Create and analyze new interpretability features and circuits to better understand how models work. Build infrastructure for running experiments and visualizing results Work with colleagues to communicate results internally and publicly You may be a good fit if you: Have a strong track record of scientific research (in any field), and have done some work on Interpretability Enjoy team science – working collaboratively to make big discoveries Are comfortable with messy experimental science. We're inventing the field as we work, and the first textbook is years away You view research and engineering as two sides of the same coin. Every team member writes code, designs and runs experiments, and interprets results You can clearly articulate and discuss the motivations behind your work, and teach us about what you've learned. You like writing up and communicating your results, even when they're null To learn more about the skills we look for and how to prepare for this role, see our blog post – So You Want to Work in Mechanistic Interpretability? Familiarity with Python is required for this role. Role Specific Location Policy: This role is based in San Francisco office; however, we are open to considering exceptional candidates for remote work on a case-by-case basis. The expected base compensation for this position is below. Our total compensation package for full-time employees includes equity, benefits, and may include incentive compensation.Annual Salary:$315,000 - $560,000 USDLogistics Education requirements: We require at least a Bachelor's degree in a related field or equivalent experience.Location-based hybrid policy: Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices. Visa sponsorship: We do sponsor visas! However, we aren't able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this. We encourage you to apply even if you do not believe you meet every single qualification. Not all strong candidates will meet every single qualification as listed. Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you're interested in this work. We think AI systems like the ones we're building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team. How we're different We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We're an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills. The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI & Compute, Concrete Problems in AI Safety, and Learning from Human Preferences. Come work with us! Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. Guidance on Candidates' AI Usage: Learn about our policy for using AI in our application process",
    "id": "job-boards-greenhouse-io-anthropic-jobs-4980427008",
    "source": "greenhouse",
    "source_url": "boards-api.greenhouse.io",
    "source_id": "4980427008",
    "title": "Research Scientist, Interpretability",
    "company_name": "anthropic",
    "company_slug": "anthropic",
    "location": "San Francisco, CA",
    "locations": [
      "San Francisco, CA"
    ],
    "url": "https://job-boards.greenhouse.io/anthropic/jobs/4980427008",
    "departments": [
      "AI Research & Engineering"
    ],
    "employment_type": null,
    "posted_at": "2025-11-22T09:28:05-05:00",
    "fetched_at": "2026-01-03T04:44:24.618Z",
    "description": "&lt;div class=&quot;content-intro&quot;&gt;&lt;h2&gt;&lt;strong&gt;About Anthropic&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.&lt;/p&gt;&lt;/div&gt;&lt;h2 class=&quot;heading&quot;&gt;About the role:&lt;/h2&gt;\n&lt;p&gt;When you see what modern language models are capable of, do you wonder, &quot;How do these things work? How can we trust them?&quot;&lt;/p&gt;\n&lt;p&gt;The Interpretability team at Anthropic is working to reverse-engineer how trained models work because we believe that a mechanistic understanding is the most robust way to make advanced systems safe. We’re looking for researchers and engineers to join our efforts.&amp;nbsp;&lt;/p&gt;\n&lt;p&gt;People mean many different things by &quot;interpretability&quot;. We&#39;re focused on mechanistic interpretability, which aims to discover how neural network parameters map to meaningful algorithms. Some useful analogies might be to think of us as trying to do &quot;biology&quot; or &quot;neuroscience&quot; of neural networks using “microscopes” we build, or as treating neural networks as binary computer programs we&#39;re trying to &quot;reverse engineer&quot;.&lt;/p&gt;\n&lt;p&gt;A few places to learn more about our work and team at a high level are &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://www.youtube.com/watch?v=TxhhMTOTMDg&quot; target=&quot;_blank&quot;&gt;this introduction to Interpretability&lt;/a&gt; from our research lead, &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://colah.github.io/about.html&quot; target=&quot;_blank&quot;&gt;Chris Olah&lt;/a&gt;; a &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://open.spotify.com/episode/5UF79Uu94ia0fwC32a89LU&quot; target=&quot;_blank&quot;&gt;discussion of our work&lt;/a&gt; on the &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://www.nytimes.com/column/hard-fork&quot; target=&quot;_blank&quot;&gt;Hard Fork podcast&lt;/a&gt; produced by the New York Times, and this &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://www.anthropic.com/research/engineering-challenges-interpretability&quot; target=&quot;_blank&quot;&gt;blog post&lt;/a&gt; (and accompanying video) sharing more about some of the engineering challenges we’d had to solve to get these results. Some of our team&#39;s notable publications include &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://transformer-circuits.pub/2021/framework/index.html&quot; target=&quot;_blank&quot;&gt;A Mathematical Framework for Transformer Circuits&lt;/a&gt;, &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html&quot; target=&quot;_blank&quot;&gt;In-context Learning and Induction Heads&lt;/a&gt;, &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://transformer-circuits.pub/2022/toy_model/index.html&quot; target=&quot;_blank&quot;&gt;Toy Models of Superposition&lt;/a&gt;, &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://transformer-circuits.pub/2024/scaling-monosemanticity/&quot; target=&quot;_blank&quot;&gt;Scaling Monosemanticity&lt;/a&gt;, and our Circuits’ &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://transformer-circuits.pub/2025/attribution-graphs/methods.html&quot; target=&quot;_blank&quot;&gt;Methods&lt;/a&gt; and &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://transformer-circuits.pub/2025/attribution-graphs/biology.html&quot; target=&quot;_blank&quot;&gt;Biology&lt;/a&gt; papers. This work builds on ideas from members&#39; work prior to Anthropic such as the &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://distill.pub/2020/circuits/&quot; target=&quot;_blank&quot;&gt;original circuits thread&lt;/a&gt;, &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://distill.pub/2021/multimodal-neurons/&quot; target=&quot;_blank&quot;&gt;Multimodal Neurons&lt;/a&gt;, &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://distill.pub/2019/activation-atlas/&quot; target=&quot;_blank&quot;&gt;Activation Atlases&lt;/a&gt;, and &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://distill.pub/2018/building-blocks/&quot; target=&quot;_blank&quot;&gt;Building Blocks&lt;/a&gt;.&lt;/p&gt;\n&lt;p&gt;We aim to create a solid foundation for mechanistically understanding neural networks and making them safe (see our &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://transformer-circuits.pub/2023/interpretability-dreams/index.html&quot; target=&quot;_blank&quot;&gt;vision post&lt;/a&gt;). In the short term, we have focused on resolving the issue of &quot;superposition&quot; (see &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://transformer-circuits.pub/2022/toy_model/index.html&quot; target=&quot;_blank&quot;&gt;Toy Models of Superposition&lt;/a&gt;, &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://transformer-circuits.pub/2023/toy-double-descent/index.html&quot; target=&quot;_blank&quot;&gt;Superposition, Memorization, and Double Descent&lt;/a&gt;, and our &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://transformer-circuits.pub/2023/may-update/index.html&quot; target=&quot;_blank&quot;&gt;May 2023 update&lt;/a&gt;), which causes the computational units of the models, like neurons and attention heads, to be individually uninterpretable, and on finding ways to decompose models into more interpretable components. Our subsequent &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://www.anthropic.com/news/mapping-mind-language-model&quot; target=&quot;_blank&quot;&gt;work&lt;/a&gt; found millions of features in Sonnet, one of our production language models, represents progress in this direction. In our most recent work, we develop methods that allow us to build circuits using features and use this circuits to understand the mechanisms associated with a model&#39;s computation and study specific examples of multi-hop reasoning, planning, and chain-of-thought faithfulness on Haiku 3.5, one of our production models.” This is a stepping stone towards our overall goal of mechanistically understanding neural networks.&lt;/p&gt;\n&lt;p&gt;We often collaborate with teams across Anthropic, such as Alignment Science and Societal Impacts to use our work to make Anthropic’s models safer. We also have an &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://transformer-circuits.pub/2024/april-update/index.html#interpretability-architecture&quot; target=&quot;_blank&quot;&gt;Interpretability Architectures project&lt;/a&gt; that involves collaborating with Pretraining.&lt;/p&gt;\n&lt;h2 class=&quot;heading&quot;&gt;Responsibilities:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;Develop methods for understanding LLMs by reverse engineering algorithms learned in their weights&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Design and run robust experiments, both quickly in toy scenarios and at scale in large models&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Create and analyze new interpretability features and circuits to better understand how models work.&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Build infrastructure for running experiments and visualizing results&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Work with colleagues to communicate results internally and publicly&lt;/p&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2 class=&quot;heading&quot;&gt;&lt;strong&gt;You may be a good fit if you:&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;Have a strong track record of scientific research (in any field), and have done &lt;em&gt;some&lt;/em&gt; work on Interpretability&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Enjoy team science – working collaboratively to make big discoveries&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Are comfortable with messy experimental science. We&#39;re inventing the field as we work, and the first textbook is years away&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;You view research and engineering as two sides of the same coin. Every team member writes code, designs and runs experiments, and interprets results&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;You can clearly articulate and discuss the motivations behind your work, and teach us about what you&#39;ve learned. You like writing up and communicating your results, even when they&#39;re null&lt;/p&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;p&gt;To learn more about the skills we look for and how to prepare for this role, see our blog post – &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://transformer-circuits.pub/2025/april-update/index.html#work&quot; target=&quot;_blank&quot;&gt;So You Want to Work in Mechanistic Interpretability?&lt;/a&gt;&lt;/p&gt;\n&lt;p&gt;Familiarity with Python is required for this role.&lt;/p&gt;\n&lt;h2 class=&quot;heading&quot;&gt;&lt;strong&gt;Role Specific Location Policy:&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;This role is based in San Francisco office; however, we are open to considering exceptional candidates for remote work on a case-by-case basis.&lt;/p&gt;\n&lt;/li&gt;\n&lt;/ul&gt;&lt;div class=&quot;content-pay-transparency&quot;&gt;&lt;div class=&quot;pay-input&quot;&gt;&lt;div class=&quot;description&quot;&gt;&lt;p&gt;The expected&amp;nbsp;base compensation for this position is below. Our total compensation package for full-time employees includes equity, benefits, and may include incentive compensation.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;title&quot;&gt;Annual Salary:&lt;/div&gt;&lt;div class=&quot;pay-range&quot;&gt;&lt;span&gt;$315,000&lt;/span&gt;&lt;span class=&quot;divider&quot;&gt;&amp;mdash;&lt;/span&gt;&lt;span&gt;$560,000 USD&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;content-conclusion&quot;&gt;&lt;h2&gt;&lt;strong&gt;Logistics&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;Education requirements: &lt;/strong&gt;We require at least a Bachelor&#39;s degree in a related field or equivalent experience.&lt;strong&gt;&lt;br&gt;&lt;br&gt;Location-based hybrid policy:&lt;/strong&gt; Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices.&lt;/p&gt;\n&lt;p&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Visa sponsorship:&lt;/strong&gt;&amp;nbsp;We do sponsor visas! However, we aren&#39;t able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;We encourage you to apply even if you do not believe you meet every single qualification.&lt;/strong&gt; Not all strong candidates will meet every single qualification as listed.&amp;nbsp; Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you&#39;re interested in this work. We think AI systems like the ones we&#39;re building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;How we&#39;re different&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We&#39;re an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills.&lt;/p&gt;\n&lt;p&gt;The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI &amp;amp; Compute, Concrete Problems in AI Safety, and Learning from Human Preferences.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Come work with us!&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. &lt;strong data-stringify-type=&quot;bold&quot;&gt;Guidance on Candidates&#39; AI Usage:&lt;/strong&gt;&amp;nbsp;Learn about&amp;nbsp;&lt;a class=&quot;c-link&quot; href=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; target=&quot;_blank&quot; data-stringify-link=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; data-sk=&quot;tooltip_parent&quot;&gt;our policy&lt;/a&gt;&amp;nbsp;for using AI in our application process&lt;/p&gt;&lt;/div&gt;",
    "_raw": {
      "source": "greenhouse",
      "original_id": 4980427008
    },
    "job_posted_at": "1h",
    "description_platform": "greenhouse",
    "description_success": true
  },
  {
    "job_title": "Senior+ Software Engineer, Research Tools",
    "employer_name": "anthropic",
    "job_city": "San Francisco, CA | New York City, NY",
    "job_apply_link": "https://job-boards.greenhouse.io/anthropic/jobs/4981828008",
    "job_posted_at_datetime_utc": "2025-11-22T09:28:05-05:00",
    "job_description": "About Anthropic Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.About the role Anthropic's research teams are pushing the boundaries of AI safety and capability research, and they need exceptional tools to do their best work. As a Software Engineer on the Research Tools team, you'll build the infrastructure and applications that enable our researchers to iterate quickly, run complex experiments, and extract insights from frontier AI systems. This role sits at the intersection of product thinking and full-stack engineering. You'll work directly with researchers and engineers to deeply understand their workflows, identify bottlenecks, and rapidly ship solutions that multiply their productivity. Whether you're building human feedback interfaces for model evaluation, creating platforms for experiment orchestration, or developing novel visualization tools for understanding model behavior, your work will directly accelerate our mission to build safe, reliable AI systems. We're looking for someone who can operate with high agency in an ambiguous environment—someone who can be dropped into a research team, quickly develop domain expertise, and independently drive impactful projects from conception to delivery.No ML or Research experience is required Responsibilities Build and maintain full-stack applications and infrastructure that researchers use daily to conduct experiments, collect feedback, and analyze results Partner closely with research teams to understand their workflows, pain points, and requirements, translating these into technical solutions Design intuitive interfaces and abstractions that make complex research tasks accessible and efficient Create reusable platforms and tools that accelerate the development of new research applications Rapidly prototype and iterate on solutions, gathering feedback from users and refining based on real-world usage Take ownership of complete product areas, from understanding user needs through design, implementation, and ongoing iteration Contribute to technical strategy and architectural decisions for research tooling Mentor other engineers and help establish best practices for research application development You may be a good fit if you Have 5+ years of software engineering experience with a strong focus on full-stack development Excel at rapid iteration and shipping—you can move from concept to working prototype quickly Have experience building tools, platforms, or infrastructure for technical users (engineers, researchers, data scientists, analysts, etc.) Demonstrate high agency and ability to operate independently in ambiguous environments Can quickly develop deep understanding of complex technical domains Have strong product instincts and can identify the right problems to solve Are proficient with modern web technologies (React, TypeScript, Python, etc.) Have a track record of building user-facing applications that are actually used and loved by their target audience Communicate effectively with both technical and non-technical stakeholders Care about the societal impacts of your work and are motivated by Anthropic's mission Strong candidates may also have Experience building research tools, scientific software, or experimentation platforms Background in machine learning, AI research, or working closely with ML researchers Founded or been an early engineer at a startup, particularly one focused on developer or researcher tools Built open-source tools or platforms with active user communities Experience with data visualization, interactive interfaces, or novel interaction paradigms Contributed to engineering platforms or internal tooling at scale (similar to Heroku, Vercel, or other platform-as-a-service products) Experience leveraging AI/LLMs to build more powerful or efficient tools Previous work in creative tools, artist tools, or other domains requiring deep user empathy Domain knowledge in areas like human-computer interaction, systems safety, or AI alignment Representative projects Building interfaces for collecting and managing human feedback on model outputs at scale Creating experiment orchestration platforms that make it easy to launch, monitor, and analyze complex research runs Developing visualization tools that help researchers understand model behavior and identify failure modes Designing reusable components and frameworks that enable rapid development of new research applications Building sandboxed execution environments for safely running AI-generated code The expected base compensation for this position is below. Our total compensation package for full-time employees includes equity, benefits, and may include incentive compensation.Annual Salary:$300,000 - $405,000 USDLogistics Education requirements: We require at least a Bachelor's degree in a related field or equivalent experience.Location-based hybrid policy: Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices. Visa sponsorship: We do sponsor visas! However, we aren't able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this. We encourage you to apply even if you do not believe you meet every single qualification. Not all strong candidates will meet every single qualification as listed. Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you're interested in this work. We think AI systems like the ones we're building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team. How we're different We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We're an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills. The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI & Compute, Concrete Problems in AI Safety, and Learning from Human Preferences. Come work with us! Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. Guidance on Candidates' AI Usage: Learn about our policy for using AI in our application process",
    "id": "job-boards-greenhouse-io-anthropic-jobs-4981828008",
    "source": "greenhouse",
    "source_url": "boards-api.greenhouse.io",
    "source_id": "4981828008",
    "title": "Senior+ Software Engineer, Research Tools",
    "company_name": "anthropic",
    "company_slug": "anthropic",
    "location": "San Francisco, CA | New York City, NY",
    "locations": [
      "San Francisco, CA | New York City, NY"
    ],
    "url": "https://job-boards.greenhouse.io/anthropic/jobs/4981828008",
    "departments": [
      "AI Research & Engineering"
    ],
    "employment_type": null,
    "posted_at": "2025-11-22T09:28:05-05:00",
    "fetched_at": "2026-01-03T04:44:24.619Z",
    "description": "&lt;div class=&quot;content-intro&quot;&gt;&lt;h2&gt;&lt;strong&gt;About Anthropic&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.&lt;/p&gt;&lt;/div&gt;&lt;h2 class=&quot;heading&quot;&gt;&lt;strong&gt;About the role&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic&#39;s research teams are pushing the boundaries of AI safety and capability research, and they need exceptional tools to do their best work. As a Software Engineer on the Research Tools team, you&#39;ll build the infrastructure and applications that enable our researchers to iterate quickly, run complex experiments, and extract insights from frontier AI systems.&lt;/p&gt;\n&lt;p&gt;This role sits at the intersection of product thinking and full-stack engineering. You&#39;ll work directly with researchers and engineers to deeply understand their workflows, identify bottlenecks, and rapidly ship solutions that multiply their productivity. Whether you&#39;re building human feedback interfaces for model evaluation, creating platforms for experiment orchestration, or developing novel visualization tools for understanding model behavior, your work will directly accelerate our mission to build safe, reliable AI systems.&lt;/p&gt;\n&lt;p&gt;We&#39;re looking for someone who can operate with high agency in an ambiguous environment—someone who can be dropped into a research team, quickly develop domain expertise, and independently drive impactful projects from conception to delivery.&lt;br&gt;&lt;br&gt;&lt;strong&gt;&lt;em&gt;No ML or Research experience is required&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n&lt;h2 class=&quot;heading&quot;&gt;&lt;strong&gt;Responsibilities&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;Build and maintain full-stack applications and infrastructure that researchers use daily to conduct experiments, collect feedback, and analyze results&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Partner closely with research teams to understand their workflows, pain points, and requirements, translating these into technical solutions&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Design intuitive interfaces and abstractions that make complex research tasks accessible and efficient&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Create reusable platforms and tools that accelerate the development of new research applications&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Rapidly prototype and iterate on solutions, gathering feedback from users and refining based on real-world usage&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Take ownership of complete product areas, from understanding user needs through design, implementation, and ongoing iteration&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Contribute to technical strategy and architectural decisions for research tooling&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Mentor other engineers and help establish best practices for research application development&lt;/p&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2 class=&quot;heading&quot;&gt;&lt;strong&gt;You may be a good fit if you&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;Have 5+ years of software engineering experience with a strong focus on full-stack development&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Excel at rapid iteration and shipping—you can move from concept to working prototype quickly&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Have experience building tools, platforms, or infrastructure for technical users (engineers, researchers, data scientists, analysts, etc.)&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Demonstrate high agency and ability to operate independently in ambiguous environments&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Can quickly develop deep understanding of complex technical domains&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Have strong product instincts and can identify the right problems to solve&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Are proficient with modern web technologies (React, TypeScript, Python, etc.)&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Have a track record of building user-facing applications that are actually used and loved by their target audience&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Communicate effectively with both technical and non-technical stakeholders&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Care about the societal impacts of your work and are motivated by Anthropic&#39;s mission&lt;/p&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2 class=&quot;heading&quot;&gt;&lt;strong&gt;Strong candidates may also have&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;Experience building research tools, scientific software, or experimentation platforms&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Background in machine learning, AI research, or working closely with ML researchers&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Founded or been an early engineer at a startup, particularly one focused on developer or researcher tools&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Built open-source tools or platforms with active user communities&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Experience with data visualization, interactive interfaces, or novel interaction paradigms&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Contributed to engineering platforms or internal tooling at scale (similar to Heroku, Vercel, or other platform-as-a-service products)&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Experience leveraging AI/LLMs to build more powerful or efficient tools&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Previous work in creative tools, artist tools, or other domains requiring deep user empathy&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Domain knowledge in areas like human-computer interaction, systems safety, or AI alignment&lt;/p&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2 class=&quot;heading&quot;&gt;&lt;strong&gt;Representative projects&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;Building interfaces for collecting and managing human feedback on model outputs at scale&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Creating experiment orchestration platforms that make it easy to launch, monitor, and analyze complex research runs&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Developing visualization tools that help researchers understand model behavior and identify failure modes&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Designing reusable components and frameworks that enable rapid development of new research applications&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Building sandboxed execution environments for safely running AI-generated code&lt;/p&gt;\n&lt;/li&gt;\n&lt;/ul&gt;&lt;div class=&quot;content-pay-transparency&quot;&gt;&lt;div class=&quot;pay-input&quot;&gt;&lt;div class=&quot;description&quot;&gt;&lt;p&gt;The expected&amp;nbsp;base compensation for this position is below. Our total compensation package for full-time employees includes equity, benefits, and may include incentive compensation.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;title&quot;&gt;Annual Salary:&lt;/div&gt;&lt;div class=&quot;pay-range&quot;&gt;&lt;span&gt;$300,000&lt;/span&gt;&lt;span class=&quot;divider&quot;&gt;&amp;mdash;&lt;/span&gt;&lt;span&gt;$405,000 USD&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;content-conclusion&quot;&gt;&lt;h2&gt;&lt;strong&gt;Logistics&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;Education requirements: &lt;/strong&gt;We require at least a Bachelor&#39;s degree in a related field or equivalent experience.&lt;strong&gt;&lt;br&gt;&lt;br&gt;Location-based hybrid policy:&lt;/strong&gt; Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices.&lt;/p&gt;\n&lt;p&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Visa sponsorship:&lt;/strong&gt;&amp;nbsp;We do sponsor visas! However, we aren&#39;t able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;We encourage you to apply even if you do not believe you meet every single qualification.&lt;/strong&gt; Not all strong candidates will meet every single qualification as listed.&amp;nbsp; Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you&#39;re interested in this work. We think AI systems like the ones we&#39;re building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;How we&#39;re different&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We&#39;re an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills.&lt;/p&gt;\n&lt;p&gt;The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI &amp;amp; Compute, Concrete Problems in AI Safety, and Learning from Human Preferences.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Come work with us!&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. &lt;strong data-stringify-type=&quot;bold&quot;&gt;Guidance on Candidates&#39; AI Usage:&lt;/strong&gt;&amp;nbsp;Learn about&amp;nbsp;&lt;a class=&quot;c-link&quot; href=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; target=&quot;_blank&quot; data-stringify-link=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; data-sk=&quot;tooltip_parent&quot;&gt;our policy&lt;/a&gt;&amp;nbsp;for using AI in our application process&lt;/p&gt;&lt;/div&gt;",
    "_raw": {
      "source": "greenhouse",
      "original_id": 4981828008
    },
    "job_posted_at": "1h",
    "description_platform": "greenhouse",
    "description_success": true
  }
]