[
  {
    "job_title": "Product Operations Manager, Research Product ",
    "employer_name": "anthropic",
    "job_city": "San Francisco, CA",
    "job_apply_link": "https://job-boards.greenhouse.io/anthropic/jobs/4992071008",
    "job_posted_at_datetime_utc": "2025-11-22T09:28:05-05:00",
    "job_description": "About Anthropic Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.About the Role: We're hiring a Product Operations Manager to work directly with our Product and Engineering teams on our Growth, Enterprise, and Verticals team. They will build, launch, and improve bleeding edge products that make the most of our frontier models’ capabilities. The Product Operations team connects strategy to execution by creating alignment up, down, and across the company. They will work closely with Product Managers and Engineers to identify bottlenecks, streamline workflows, enhance decision-making processes, and scale our Product’s impact. Working as an extension of the product leadership team, they will balance hands-on tactical execution with strategic initiatives, bringing a pragmatic eye for scale and operationalization in a fit-for-purpose way. The Research Product Team is responsible for launching our new models into the wild. They are the connective tissue between our Research Org, our internal product teams, and our users themselves. The ideal candidate will be hands-on and have experience building and operationalizing end-to-end product delivery. They are passionate about creating scalable systems that help Product teams better understand users. This includes implementing feedback loops, developing planning frameworks, and designing launch playbooks that elevate our Product organization's effectiveness. Responsibilities: Inputs to Product Teams – Ensuring product teams have the information they need to make great decisions Voice of customer synthesis and feedback routing from strategic to tactical Create high-leverage engagement points with partners throughout the product lifecycle Establish rigor in understanding users via reliable metrics, dashboards, and clear hypotheses Establish mechanisms for measuring product success and impact, including analytics dashboards and reporting systems Streamline the most important decision points for teams and impacted partners Ops of the Product Org – Creating the operating systems that enable product teams to thrive Support team rhythms, rituals, and operational models (offsites, Monthly Business Reviews, Team town halls, etc) Create reliable run-of-business systems across product Improve common product development processes and tooling Facilitate effective collaboration between Product, Engineering, Sales, Customer Success, and Marketing teams Outputs from Product Teams – Amplifying product impact by connecting what we build to those who need it Run Early Access Programs (EAPs) and beta programs that validate hypotheses and improve products Maintain launch motions that allow us to ship with confidence and monitor impact Create cross-team roadmap visibility that drives cross-functional alignment Make clear to all of Anthropic what Product is working on and how it's going You may be a good fit if you have: 4+ years of experience in product operations, program management, or related operational roles in hyper-scaling tech companies Mission-aligned with building safe and beneficial AI systems Track record of building processes and programs from 0 to 1 and scaling them thoughtfully Experience working deeply with AI and frontier models Strong cross-functional partnership skills with ability to influence without authority Strong analytical skills with the ability to translate complex qualitative and quantitative data into actionable insights with clear recommendations Success managing complex, multi-stakeholder initiatives in fast-paced, ambiguous environments Experience with launch coordination, early access programs, or customer feedback loops A passion for iterative, user-driven product development Experience as a bridge builder who connects strategy to execution and creates alignment across teams. You are a problem seeker comfortable with ambiguity and skilled at creating structure where none exists. The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (\"OTE\") range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.Annual Salary:$210,000 - $240,000 USDLogistics Education requirements: We require at least a Bachelor's degree in a related field or equivalent experience.Location-based hybrid policy: Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices. Visa sponsorship: We do sponsor visas! However, we aren't able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this. We encourage you to apply even if you do not believe you meet every single qualification. Not all strong candidates will meet every single qualification as listed. Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you're interested in this work. We think AI systems like the ones we're building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.Your safety matters to us. To protect yourself from potential scams, remember that Anthropic recruiters only contact you from @anthropic.com email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you're ever unsure about a communication, don't click any links—visit anthropic.com/careers directly for confirmed position openings. How we're different We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We're an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills. The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI & Compute, Concrete Problems in AI Safety, and Learning from Human Preferences. Come work with us! Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. Guidance on Candidates' AI Usage: Learn about our policy for using AI in our application process",
    "id": "job-boards-greenhouse-io-anthropic-jobs-4992071008",
    "source": "greenhouse",
    "source_url": "boards-api.greenhouse.io",
    "source_id": "4992071008",
    "title": "Product Operations Manager, Research Product ",
    "company_name": "anthropic",
    "company_slug": "anthropic",
    "location": "San Francisco, CA",
    "locations": [
      "San Francisco, CA"
    ],
    "url": "https://job-boards.greenhouse.io/anthropic/jobs/4992071008",
    "departments": [
      "Product Management, Support, & Operations"
    ],
    "employment_type": null,
    "posted_at": "2025-11-22T09:28:05-05:00",
    "fetched_at": "2026-01-12T18:26:14.167Z",
    "description": "&lt;div class=&quot;content-intro&quot;&gt;&lt;h2&gt;&lt;strong&gt;About Anthropic&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.&lt;/p&gt;&lt;/div&gt;&lt;h2&gt;About the Role:&lt;/h2&gt;\n&lt;p&gt;We&#39;re hiring a Product Operations Manager to work directly with our Product and Engineering teams on our Growth, Enterprise, and Verticals team. They will build, launch, and improve bleeding edge products that make the most of our frontier models’ capabilities.&amp;nbsp;&lt;/p&gt;\n&lt;p&gt;The Product Operations team connects strategy to execution by creating alignment up, down, and across the company. They will&amp;nbsp; work closely with Product Managers and Engineers to identify bottlenecks, streamline workflows, enhance decision-making processes, and scale our Product’s impact. Working as an extension of the product leadership team, they will balance hands-on tactical execution with strategic initiatives, bringing a pragmatic eye for scale and operationalization in a fit-for-purpose way.&lt;/p&gt;\n&lt;p&gt;The Research Product Team is responsible for launching our new models into the wild. They are the connective tissue between our Research Org, our internal product teams, and our users themselves.&amp;nbsp;&amp;nbsp;&lt;/p&gt;\n&lt;p&gt;The ideal candidate will be hands-on and have experience building and operationalizing end-to-end product delivery. They are passionate about creating scalable systems that help Product teams better understand users. This includes implementing feedback loops, developing planning frameworks, and designing launch playbooks that elevate our Product organization&#39;s effectiveness.&lt;/p&gt;\n&lt;h2&gt;Responsibilities:&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;Inputs to Product Teams&lt;/strong&gt; – Ensuring product teams have the information they need to make great decisions&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;Voice of customer synthesis and feedback routing from strategic to tactical&lt;/li&gt;\n&lt;li&gt;Create high-leverage engagement points with partners throughout the product lifecycle&lt;/li&gt;\n&lt;li&gt;Establish rigor in understanding users via reliable metrics, dashboards, and clear hypotheses&lt;/li&gt;\n&lt;li&gt;Establish mechanisms for measuring product success and impact, including analytics dashboards and reporting systems&lt;/li&gt;\n&lt;li&gt;Streamline the most important decision points for teams and impacted partners&lt;/li&gt;\n&lt;/ul&gt;\n&lt;p&gt;&lt;strong&gt;Ops of the Product Org&lt;/strong&gt; – Creating the operating systems that enable product teams to thrive&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;Support team rhythms, rituals, and operational models (offsites, Monthly Business Reviews, Team town halls, etc)&lt;/li&gt;\n&lt;li&gt;Create reliable run-of-business systems across product&lt;/li&gt;\n&lt;li&gt;Improve common product development processes and tooling&lt;/li&gt;\n&lt;li&gt;Facilitate effective collaboration between Product, Engineering, Sales, Customer Success, and Marketing teams&lt;/li&gt;\n&lt;/ul&gt;\n&lt;p&gt;&lt;strong&gt;Outputs from Product Teams&lt;/strong&gt; – Amplifying product impact by connecting what we build to those who need it&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;Run Early Access Programs (EAPs) and beta programs that validate hypotheses and improve products&lt;/li&gt;\n&lt;li&gt;Maintain launch motions that allow us to ship with confidence and monitor impact&lt;/li&gt;\n&lt;li&gt;Create cross-team roadmap visibility that drives cross-functional alignment&lt;/li&gt;\n&lt;li&gt;Make clear to all of Anthropic what Product is working on and how it&#39;s going&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;You may be a good fit if you have:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;4+ years of experience in product operations, program management, or related operational roles in hyper-scaling tech companies&lt;/li&gt;\n&lt;li&gt;Mission-aligned with building safe and beneficial AI systems&lt;/li&gt;\n&lt;li&gt;Track record of building processes and programs from 0 to 1 and scaling them thoughtfully&lt;/li&gt;\n&lt;li&gt;Experience working deeply with AI and frontier models&lt;/li&gt;\n&lt;li&gt;Strong cross-functional partnership skills with ability to influence without authority&lt;/li&gt;\n&lt;li&gt;Strong analytical skills with the ability to translate complex qualitative and quantitative data into actionable insights with clear recommendations&lt;/li&gt;\n&lt;li&gt;Success managing complex, multi-stakeholder initiatives in fast-paced, ambiguous environments&lt;/li&gt;\n&lt;li&gt;Experience with launch coordination, early access programs, or customer feedback loops&lt;/li&gt;\n&lt;li&gt;A passion for iterative, user-driven product development&lt;/li&gt;\n&lt;li&gt;Experience as a bridge builder who connects strategy to execution and creates alignment across teams. You are a problem seeker comfortable with ambiguity and skilled at creating structure where none exists.&lt;/li&gt;\n&lt;/ul&gt;&lt;div class=&quot;content-pay-transparency&quot;&gt;&lt;div class=&quot;pay-input&quot;&gt;&lt;div class=&quot;description&quot;&gt;&lt;p&gt;The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (&quot;OTE&quot;) range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;title&quot;&gt;Annual Salary:&lt;/div&gt;&lt;div class=&quot;pay-range&quot;&gt;&lt;span&gt;$210,000&lt;/span&gt;&lt;span class=&quot;divider&quot;&gt;&amp;mdash;&lt;/span&gt;&lt;span&gt;$240,000 USD&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;content-conclusion&quot;&gt;&lt;h2&gt;&lt;strong&gt;Logistics&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;Education requirements: &lt;/strong&gt;We require at least a Bachelor&#39;s degree in a related field or equivalent experience.&lt;strong&gt;&lt;br&gt;&lt;br&gt;Location-based hybrid policy:&lt;/strong&gt; Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices.&lt;/p&gt;\n&lt;p&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Visa sponsorship:&lt;/strong&gt;&amp;nbsp;We do sponsor visas! However, we aren&#39;t able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;We encourage you to apply even if you do not believe you meet every single qualification.&lt;/strong&gt; Not all strong candidates will meet every single qualification as listed.&amp;nbsp; Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you&#39;re interested in this work. We think AI systems like the ones we&#39;re building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.&lt;br&gt;&lt;br&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Your safety matters to us.&lt;/strong&gt;&amp;nbsp;To protect yourself from potential scams, remember that Anthropic recruiters only contact you from&amp;nbsp;@anthropic.com&amp;nbsp;email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you&#39;re ever unsure about a communication, don&#39;t click any links—visit&amp;nbsp;&lt;u data-stringify-type=&quot;underline&quot;&gt;&lt;a class=&quot;c-link c-link--underline&quot; href=&quot;http://anthropic.com/careers&quot; target=&quot;_blank&quot; data-stringify-link=&quot;http://anthropic.com/careers&quot; data-sk=&quot;tooltip_parent&quot; data-remove-tab-index=&quot;true&quot;&gt;anthropic.com/careers&lt;/a&gt;&lt;/u&gt;&amp;nbsp;directly for confirmed position openings.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;How we&#39;re different&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We&#39;re an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills.&lt;/p&gt;\n&lt;p&gt;The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI &amp;amp; Compute, Concrete Problems in AI Safety, and Learning from Human Preferences.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Come work with us!&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. &lt;strong data-stringify-type=&quot;bold&quot;&gt;Guidance on Candidates&#39; AI Usage:&lt;/strong&gt;&amp;nbsp;Learn about&amp;nbsp;&lt;a class=&quot;c-link&quot; href=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; target=&quot;_blank&quot; data-stringify-link=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; data-sk=&quot;tooltip_parent&quot;&gt;our policy&lt;/a&gt;&amp;nbsp;for using AI in our application process&lt;/p&gt;&lt;/div&gt;",
    "_raw": {
      "source": "greenhouse",
      "original_id": 4992071008
    },
    "job_posted_at": "1h",
    "description_platform": "greenhouse",
    "description_success": true
  },
  {
    "job_title": "Product Support Manager",
    "employer_name": "anthropic",
    "job_city": "San Francisco, CA | New York City, NY | Seattle, WA",
    "job_apply_link": "https://job-boards.greenhouse.io/anthropic/jobs/4983655008",
    "job_posted_at_datetime_utc": "2025-11-22T09:28:05-05:00",
    "job_description": "About Anthropic Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.About the role: We are hiring a Product Support Manager in NYC, Seattle, or San Francisco to manage a team of Product Support Specialists and focus on enhancing our Enterprise Support offering. In this role, you’ll be responsible for building and managing a happy and high-performing Specialist team that is at the front lines of safely delivering AI to the world. As part of a global Support organization, you’ll collaborate closely with peers in other regions to ensure users of all types have a great experience with Anthropic’s products. Responsibilities: Hire, lead, and develop a team of happy, high-performing Product Support Specialists Provide thoughtful coaching and feedback to your direct reports, and partner with them on their career development goals and growth Monitor team performance and course correct both in real-time and strategically as needed Manage day to day team operations, including proactive capacity management and ad hoc unblocking of your Specialists as they action their daily work Partner with peer leaders in other regions to ensure consistent global support delivery in routine casework as well as on-call or high-urgency responsibilities Work closely with go-to-market (GTM) stakeholders to scope, execute, and iterate upon our offerings for our most strategic customers; interact with these internal users daily Drive large-scale initiatives that raise the bar for our organization, leveraging data to make decisions and with a keen understanding of broader business goals Continuously strive for exceptional user experiences, with a focus on high-touch Enterprise Support Partner with cross-functional stakeholders across the organization to build efficiencies and improve user experience Communicate clearly and effectively with your team, stakeholders, and external customers You may be a good fit if you: Have 6+ years of product support experience and 3+ years in a people management role Have been part of a B2B Enterprise or Strategic Support team (as a bonus, you also understand the needs of Consumer, scaled support users) Thrive in a fast-paced, ever-changing environment, and have demonstrated success in bringing your team along during periods of rapid growth Successfully operate in ambiguity, practicing good judgment and awareness of broader priorities in order to make decisions and get things done Care deeply about continuous improvement and elevating ambitions in the name of user experience Enjoy building trust and collaborating closely with cross-functional partners Can capably navigate tough conversations, empathetically driving solutions and steps forward Value regularly seeking, providing, and incorporating feedback when it comes to the way you and your team operate Are interested in developing deep product expertise in order to comprehensively support your team and knowledgeably role model user first behaviors Prefer to use data to make decisions or advocate for users, and know your way around basic to intermediate SQL queries Consider yourself at least somewhat knowledgeable with APIs and capable of confidently understanding technical documentation in order to help debug errors Are comfortable working with a globally distributed team and building strong remote and in-office relationships Are excited about Anthropic’s products and already familiar with some of the ways AI can have a positive impact on your work The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (\"OTE\") range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.Annual Salary:$165,000 - $210,000 USDLogistics Education requirements: We require at least a Bachelor's degree in a related field or equivalent experience.Location-based hybrid policy: Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices. Visa sponsorship: We do sponsor visas! However, we aren't able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this. We encourage you to apply even if you do not believe you meet every single qualification. Not all strong candidates will meet every single qualification as listed. Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you're interested in this work. We think AI systems like the ones we're building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.Your safety matters to us. To protect yourself from potential scams, remember that Anthropic recruiters only contact you from @anthropic.com email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you're ever unsure about a communication, don't click any links—visit anthropic.com/careers directly for confirmed position openings. How we're different We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We're an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills. The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI & Compute, Concrete Problems in AI Safety, and Learning from Human Preferences. Come work with us! Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. Guidance on Candidates' AI Usage: Learn about our policy for using AI in our application process",
    "id": "job-boards-greenhouse-io-anthropic-jobs-4983655008",
    "source": "greenhouse",
    "source_url": "boards-api.greenhouse.io",
    "source_id": "4983655008",
    "title": "Product Support Manager",
    "company_name": "anthropic",
    "company_slug": "anthropic",
    "location": "San Francisco, CA | New York City, NY | Seattle, WA",
    "locations": [
      "San Francisco, CA | New York City, NY | Seattle, WA"
    ],
    "url": "https://job-boards.greenhouse.io/anthropic/jobs/4983655008",
    "departments": [
      "Product Management, Support, & Operations"
    ],
    "employment_type": null,
    "posted_at": "2025-11-22T09:28:05-05:00",
    "fetched_at": "2026-01-12T18:26:14.167Z",
    "description": "&lt;div class=&quot;content-intro&quot;&gt;&lt;h2&gt;&lt;strong&gt;About Anthropic&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.&lt;/p&gt;&lt;/div&gt;&lt;h2&gt;About the role:&lt;/h2&gt;\n&lt;p&gt;We are hiring a Product Support Manager in NYC, Seattle, or San Francisco to manage a team of Product Support Specialists and focus on enhancing our Enterprise Support offering. In this role, you’ll be responsible for building and managing a happy and high-performing Specialist team that is at the front lines of safely delivering AI to the world. As part of a global Support organization, you’ll collaborate closely with peers in other regions to ensure users of all types have a great experience with Anthropic’s products.&lt;/p&gt;\n&lt;h2&gt;Responsibilities:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Hire, lead, and develop a team of happy, high-performing Product Support Specialists&lt;/li&gt;\n&lt;li&gt;Provide thoughtful coaching and feedback to your direct reports, and partner with them on their career development goals and growth&lt;/li&gt;\n&lt;li&gt;Monitor team performance and course correct both in real-time and strategically as needed&lt;/li&gt;\n&lt;li&gt;Manage day to day team operations, including proactive capacity management and ad hoc unblocking of your Specialists as they action their daily work&lt;/li&gt;\n&lt;li&gt;Partner with peer leaders in other regions to ensure consistent global support delivery in routine casework as well as on-call or high-urgency responsibilities&lt;/li&gt;\n&lt;li&gt;Work closely with go-to-market (GTM) stakeholders to scope, execute, and iterate upon our offerings for our most strategic customers; interact with these internal users daily&lt;/li&gt;\n&lt;li&gt;Drive large-scale initiatives that raise the bar for our organization, leveraging data to make decisions and with a keen understanding of broader business goals&lt;/li&gt;\n&lt;li&gt;Continuously strive for exceptional user experiences, with a focus on high-touch Enterprise Support&lt;/li&gt;\n&lt;li&gt;Partner with cross-functional stakeholders across the organization to build efficiencies and improve user experience&lt;/li&gt;\n&lt;li&gt;Communicate clearly and effectively with your team, stakeholders, and external customers&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;You may be a good fit if you:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Have 6+ years of product support experience and 3+ years in a people management role&lt;/li&gt;\n&lt;li&gt;Have been part of a B2B Enterprise or Strategic Support team (as a bonus, you also understand the needs of Consumer, scaled support users)&lt;/li&gt;\n&lt;li&gt;Thrive in a fast-paced, ever-changing environment, and have demonstrated success in bringing your team along during periods of rapid growth&lt;/li&gt;\n&lt;li&gt;Successfully operate in ambiguity, practicing good judgment and awareness of broader priorities in order to make decisions and get things done&lt;/li&gt;\n&lt;li&gt;Care deeply about continuous improvement and elevating ambitions in the name of user experience&lt;/li&gt;\n&lt;li&gt;Enjoy building trust and collaborating closely with cross-functional partners&lt;/li&gt;\n&lt;li&gt;Can capably navigate tough conversations, empathetically driving solutions and steps forward&lt;/li&gt;\n&lt;li&gt;Value regularly seeking, providing, and incorporating feedback when it comes to the way you and your team operate&lt;/li&gt;\n&lt;li&gt;Are interested in developing deep product expertise in order to comprehensively support your team and knowledgeably role model user first behaviors&lt;/li&gt;\n&lt;li&gt;Prefer to use data to make decisions or advocate for users, and know your way around basic to intermediate SQL queries&lt;/li&gt;\n&lt;li&gt;Consider yourself at least somewhat knowledgeable with APIs and capable of confidently understanding technical documentation in order to help debug errors&lt;/li&gt;\n&lt;li&gt;Are comfortable working with a globally distributed team and building strong remote and in-office relationships&lt;/li&gt;\n&lt;li&gt;Are excited about Anthropic’s products and already familiar with some of the ways AI can have a positive impact on your work&lt;/li&gt;\n&lt;/ul&gt;&lt;div class=&quot;content-pay-transparency&quot;&gt;&lt;div class=&quot;pay-input&quot;&gt;&lt;div class=&quot;description&quot;&gt;&lt;p&gt;The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (&quot;OTE&quot;) range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;title&quot;&gt;Annual Salary:&lt;/div&gt;&lt;div class=&quot;pay-range&quot;&gt;&lt;span&gt;$165,000&lt;/span&gt;&lt;span class=&quot;divider&quot;&gt;&amp;mdash;&lt;/span&gt;&lt;span&gt;$210,000 USD&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;content-conclusion&quot;&gt;&lt;h2&gt;&lt;strong&gt;Logistics&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;Education requirements: &lt;/strong&gt;We require at least a Bachelor&#39;s degree in a related field or equivalent experience.&lt;strong&gt;&lt;br&gt;&lt;br&gt;Location-based hybrid policy:&lt;/strong&gt; Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices.&lt;/p&gt;\n&lt;p&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Visa sponsorship:&lt;/strong&gt;&amp;nbsp;We do sponsor visas! However, we aren&#39;t able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;We encourage you to apply even if you do not believe you meet every single qualification.&lt;/strong&gt; Not all strong candidates will meet every single qualification as listed.&amp;nbsp; Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you&#39;re interested in this work. We think AI systems like the ones we&#39;re building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.&lt;br&gt;&lt;br&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Your safety matters to us.&lt;/strong&gt;&amp;nbsp;To protect yourself from potential scams, remember that Anthropic recruiters only contact you from&amp;nbsp;@anthropic.com&amp;nbsp;email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you&#39;re ever unsure about a communication, don&#39;t click any links—visit&amp;nbsp;&lt;u data-stringify-type=&quot;underline&quot;&gt;&lt;a class=&quot;c-link c-link--underline&quot; href=&quot;http://anthropic.com/careers&quot; target=&quot;_blank&quot; data-stringify-link=&quot;http://anthropic.com/careers&quot; data-sk=&quot;tooltip_parent&quot; data-remove-tab-index=&quot;true&quot;&gt;anthropic.com/careers&lt;/a&gt;&lt;/u&gt;&amp;nbsp;directly for confirmed position openings.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;How we&#39;re different&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We&#39;re an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills.&lt;/p&gt;\n&lt;p&gt;The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI &amp;amp; Compute, Concrete Problems in AI Safety, and Learning from Human Preferences.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Come work with us!&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. &lt;strong data-stringify-type=&quot;bold&quot;&gt;Guidance on Candidates&#39; AI Usage:&lt;/strong&gt;&amp;nbsp;Learn about&amp;nbsp;&lt;a class=&quot;c-link&quot; href=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; target=&quot;_blank&quot; data-stringify-link=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; data-sk=&quot;tooltip_parent&quot;&gt;our policy&lt;/a&gt;&amp;nbsp;for using AI in our application process&lt;/p&gt;&lt;/div&gt;",
    "_raw": {
      "source": "greenhouse",
      "original_id": 4983655008
    },
    "job_posted_at": "1h",
    "description_platform": "greenhouse",
    "description_success": true
  },
  {
    "job_title": "Product Support Specialist",
    "employer_name": "anthropic",
    "job_city": "San Francisco, CA | New York City, NY | Seattle, WA",
    "job_apply_link": "https://job-boards.greenhouse.io/anthropic/jobs/4979585008",
    "job_posted_at_datetime_utc": "2025-11-22T09:28:05-05:00",
    "job_description": "About Anthropic Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.About the role: As a Product Support Specialist, you’ll be at the front lines of safely delivering AI to the world by responding to, investigating, and tracking user needs in your day to day. Additionally, you’ll help us identify – and close – gaps in our team’s technical knowledge, provide high-touch support to strategic customers, and demonstrate deep care for how we systematically support customers at scale. Note: Specialists will work either Tues - Sat or Sun - Thurs Responsibilities: Become an expert in all Anthropic products Respond to user support cases with a variety of complexity, from questions for individuals to complex API debugging for large businesses Clearly and empathetically communicate with a wide range of user personas, context-switching between guiding executives in a high-touch model to assisting consumer users in a rapid pace Manage on-call tasks for high-urgency user issues with extreme ownership Prioritize critically and comfortably adapt to an ever-evolving product landscape Operate in ambiguity, making informed decisions even in never-before-seen situations Partner with engineers, teammates, and other internal stakeholders to diagnose and resolve user issues, both individually and at scale Suggest and drive improvements to increase user satisfaction through support processes as well as own initiatives that increase efficiency and drive down contact rates Uplevel our team’s technical knowledge by scoping gaps, working with cross-functional partners to deeply understand relevant nuances, and building resources that grow with our products You may be a good fit if you: Have experience in technical product support, including API debugging, preferably in a second tier, escalated, or priority support team Are familiar with APIs and technical SaaS products and can deeply understand technical docs with ease Have demonstrated an ability to thrive in fast-paced, reactive situations while meeting core support metrics targets (e.g. CSAT, SLA, etc.) Possess strong user empathy and are expert in the lifecycle of a support case; you can read between the lines of a user’s question, put yourself in their shoes, and get at the heart of their needs for a speedy, satisfying resolution Have crisp but kind written communication skills and a deep care for the details Enjoy helping others learn about new features and complex concepts Are persistent and curious; you delight in the hunt of tracking down a bug or issue, and are energized by fixing this for all similar users going forward Have experience contributing to the foundations of a support team – this is essential, highly valuable, but often unglamorous work Are proficient at working in a technical environment and are interested in Anthropic’s products Possess a deep sense of ownership, and are excited to help us build our team! The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (\"OTE\") range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.Annual Salary:$116,480 - $165,000 USDLogistics Education requirements: We require at least a Bachelor's degree in a related field or equivalent experience.Location-based hybrid policy: Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices. Visa sponsorship: We do sponsor visas! However, we aren't able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this. We encourage you to apply even if you do not believe you meet every single qualification. Not all strong candidates will meet every single qualification as listed. Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you're interested in this work. We think AI systems like the ones we're building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.Your safety matters to us. To protect yourself from potential scams, remember that Anthropic recruiters only contact you from @anthropic.com email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you're ever unsure about a communication, don't click any links—visit anthropic.com/careers directly for confirmed position openings. How we're different We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We're an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills. The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI & Compute, Concrete Problems in AI Safety, and Learning from Human Preferences. Come work with us! Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. Guidance on Candidates' AI Usage: Learn about our policy for using AI in our application process",
    "id": "job-boards-greenhouse-io-anthropic-jobs-4979585008",
    "source": "greenhouse",
    "source_url": "boards-api.greenhouse.io",
    "source_id": "4979585008",
    "title": "Product Support Specialist",
    "company_name": "anthropic",
    "company_slug": "anthropic",
    "location": "San Francisco, CA | New York City, NY | Seattle, WA",
    "locations": [
      "San Francisco, CA | New York City, NY | Seattle, WA"
    ],
    "url": "https://job-boards.greenhouse.io/anthropic/jobs/4979585008",
    "departments": [
      "Product Management, Support, & Operations"
    ],
    "employment_type": null,
    "posted_at": "2025-11-22T09:28:05-05:00",
    "fetched_at": "2026-01-12T18:26:14.167Z",
    "description": "&lt;div class=&quot;content-intro&quot;&gt;&lt;h2&gt;&lt;strong&gt;About Anthropic&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.&lt;/p&gt;&lt;/div&gt;&lt;h2&gt;About the role:&lt;/h2&gt;\n&lt;p&gt;As a Product Support Specialist, you’ll be at the front lines of safely delivering AI to the world by responding to, investigating, and tracking user needs in your day to day. Additionally, you’ll help us identify – and close – gaps in our team’s technical knowledge, provide high-touch support to strategic customers, and demonstrate deep care for how we systematically support customers at scale.&amp;nbsp;&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Specialists will work either Tues - Sat or Sun - Thurs&amp;nbsp;&amp;nbsp;&lt;/p&gt;\n&lt;h2 class=&quot;p1&quot;&gt;&lt;strong&gt;Responsibilities:&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul class=&quot;ul1&quot;&gt;\n&lt;li class=&quot;li1&quot;&gt;Become an expert in all Anthropic products&lt;/li&gt;\n&lt;li class=&quot;li1&quot;&gt;Respond to user support cases with a variety of complexity, from questions for individuals to complex API debugging for large businesses&amp;nbsp;&lt;/li&gt;\n&lt;li class=&quot;li1&quot;&gt;Clearly and empathetically communicate with a wide range of user personas, context-switching between guiding executives in a high-touch model to assisting consumer users in a rapid pace&lt;/li&gt;\n&lt;li class=&quot;li1&quot;&gt;Manage on-call tasks for high-urgency user issues with extreme ownership&lt;/li&gt;\n&lt;li class=&quot;li1&quot;&gt;Prioritize critically and comfortably adapt to an ever-evolving product landscape&lt;/li&gt;\n&lt;li class=&quot;li1&quot;&gt;Operate in ambiguity, making informed decisions even in never-before-seen situations&lt;/li&gt;\n&lt;li class=&quot;li1&quot;&gt;Partner with engineers, teammates, and other internal stakeholders to diagnose and resolve user issues, both individually and at scale&lt;/li&gt;\n&lt;li class=&quot;li1&quot;&gt;Suggest and drive improvements to increase user satisfaction through support processes as well as own initiatives that increase efficiency and drive down contact rates&lt;/li&gt;\n&lt;li class=&quot;li1&quot;&gt;Uplevel our team’s technical knowledge by scoping gaps, working with cross-functional partners to deeply understand relevant nuances, and building resources that grow with our products&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2 class=&quot;p1&quot;&gt;&lt;strong&gt;You may be a good fit if you:&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul class=&quot;ul1&quot;&gt;\n&lt;li class=&quot;li1&quot;&gt;Have experience in technical product support, including API debugging, preferably in a second tier, escalated, or priority support team&lt;/li&gt;\n&lt;li class=&quot;li1&quot;&gt;Are familiar with APIs and technical SaaS products and can deeply understand technical docs with ease&lt;/li&gt;\n&lt;li class=&quot;li1&quot;&gt;Have demonstrated an ability to thrive in fast-paced, reactive situations while meeting core support metrics targets (e.g. CSAT, SLA, etc.)&lt;/li&gt;\n&lt;li class=&quot;li1&quot;&gt;Possess strong user empathy and are expert in the lifecycle of a support case; you can read between the lines of a user’s question, put yourself in their shoes, and get at the heart of their needs for a speedy, satisfying resolution&lt;/li&gt;\n&lt;li class=&quot;li1&quot;&gt;Have crisp but kind written communication skills and a deep care for the details&lt;/li&gt;\n&lt;li class=&quot;li1&quot;&gt;Enjoy helping others learn about new features and complex concepts&lt;/li&gt;\n&lt;li class=&quot;li1&quot;&gt;Are persistent and curious; you delight in the hunt of tracking down a bug or issue, and are energized by fixing this for all similar users going forward&lt;/li&gt;\n&lt;li class=&quot;li1&quot;&gt;Have experience contributing to the foundations of a support team – this is essential, highly valuable, but often unglamorous work&lt;/li&gt;\n&lt;li class=&quot;li1&quot;&gt;Are proficient at working in a technical environment and are interested in Anthropic’s products&lt;/li&gt;\n&lt;li class=&quot;li1&quot;&gt;Possess a deep sense of ownership, and are excited to help us build our team!&lt;/li&gt;\n&lt;/ul&gt;&lt;div class=&quot;content-pay-transparency&quot;&gt;&lt;div class=&quot;pay-input&quot;&gt;&lt;div class=&quot;description&quot;&gt;&lt;p&gt;The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (&quot;OTE&quot;) range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;title&quot;&gt;Annual Salary:&lt;/div&gt;&lt;div class=&quot;pay-range&quot;&gt;&lt;span&gt;$116,480&lt;/span&gt;&lt;span class=&quot;divider&quot;&gt;&amp;mdash;&lt;/span&gt;&lt;span&gt;$165,000 USD&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;content-conclusion&quot;&gt;&lt;h2&gt;&lt;strong&gt;Logistics&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;Education requirements: &lt;/strong&gt;We require at least a Bachelor&#39;s degree in a related field or equivalent experience.&lt;strong&gt;&lt;br&gt;&lt;br&gt;Location-based hybrid policy:&lt;/strong&gt; Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices.&lt;/p&gt;\n&lt;p&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Visa sponsorship:&lt;/strong&gt;&amp;nbsp;We do sponsor visas! However, we aren&#39;t able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;We encourage you to apply even if you do not believe you meet every single qualification.&lt;/strong&gt; Not all strong candidates will meet every single qualification as listed.&amp;nbsp; Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you&#39;re interested in this work. We think AI systems like the ones we&#39;re building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.&lt;br&gt;&lt;br&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Your safety matters to us.&lt;/strong&gt;&amp;nbsp;To protect yourself from potential scams, remember that Anthropic recruiters only contact you from&amp;nbsp;@anthropic.com&amp;nbsp;email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you&#39;re ever unsure about a communication, don&#39;t click any links—visit&amp;nbsp;&lt;u data-stringify-type=&quot;underline&quot;&gt;&lt;a class=&quot;c-link c-link--underline&quot; href=&quot;http://anthropic.com/careers&quot; target=&quot;_blank&quot; data-stringify-link=&quot;http://anthropic.com/careers&quot; data-sk=&quot;tooltip_parent&quot; data-remove-tab-index=&quot;true&quot;&gt;anthropic.com/careers&lt;/a&gt;&lt;/u&gt;&amp;nbsp;directly for confirmed position openings.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;How we&#39;re different&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We&#39;re an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills.&lt;/p&gt;\n&lt;p&gt;The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI &amp;amp; Compute, Concrete Problems in AI Safety, and Learning from Human Preferences.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Come work with us!&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. &lt;strong data-stringify-type=&quot;bold&quot;&gt;Guidance on Candidates&#39; AI Usage:&lt;/strong&gt;&amp;nbsp;Learn about&amp;nbsp;&lt;a class=&quot;c-link&quot; href=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; target=&quot;_blank&quot; data-stringify-link=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; data-sk=&quot;tooltip_parent&quot;&gt;our policy&lt;/a&gt;&amp;nbsp;for using AI in our application process&lt;/p&gt;&lt;/div&gt;",
    "_raw": {
      "source": "greenhouse",
      "original_id": 4979585008
    },
    "job_posted_at": "1h",
    "description_platform": "greenhouse",
    "description_success": true
  },
  {
    "job_title": "Recruiter, AI Research",
    "employer_name": "anthropic",
    "job_city": "San Francisco, CA",
    "job_apply_link": "https://job-boards.greenhouse.io/anthropic/jobs/4604015008",
    "job_posted_at_datetime_utc": "2025-11-22T09:28:05-05:00",
    "job_description": "About Anthropic Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.About the Role: Anthropic is looking for a talented AI Research Recruiter to partner with our Research teams. In this pivotal role, you will be instrumental in shaping the future of our organization by identifying, engaging, and hiring the best and brightest minds across a range of disciplines. As we continue to push the boundaries of AI research and development, we need a passionate recruiter who can help us build a world-class team dedicated to creating safe and beneficial AI systems. Responsibilities: Develop and execute strategic recruiting plans to identify, source, and hire highly qualified candidates, with a focus on Machine Learning and AI research talent Partner with Research hiring managers and interviewers to understand hiring needs, team matching, required skills and qualifications Enhance and implement recruiting processes and programs while maintaining an inclusive and high talent bar, such as developing targeted outreach campaigns, building connections with industry leaders, and removing any unfair biases from the hiring process Collaborate with leadership and cross-functional partners to understand organizational needs and map out long-term talent acquisition strategies that balance priorities across all technical teams Enhance Anthropic's employer brand within the research and science community to showcase our mission, culture, and values to candidates Stay up-to-date on recruiting best practices, emerging sourcing techniques, interview innovations, and workplace trends You may be a good fit if you: Have 5+ years of experience in full life cycle recruiting supporting technical research teams Have a passion for AI's potential to positively impact the world and realistic assessment of its risks and limitations Are experimental and are open to new, creative recruiting ideas, or have experience working with hiring managers who are open to non-traditional talent strategies Thrive in fast-paced, dynamic environments and enjoy juggling multiple priorities Possess strong technical aptitude with the ability to understand and evaluate technical qualifications Have enthusiasm for deeply understanding the needs of researchers and innovating on recruiting processes to make them more tailored to the world of research Have excellent organizational skills and attention to detail, as well as a proactive mindset and ability to operate with autonomy Have experience partnering with researchers and hiring talent that work on GenAI and LLMs Have a proven track record of scaling and building diverse and high-performing teams in a fast-paced, high-growth startup environment Strong candidates may also: Bring a deep interest in AI safety research Have experience partnering with researchers and hiring talent that work on GenAI and LLMs Have experience with academic recruitment and research communities Deadline to apply: None. Applications will be reviewed on a rolling basis.The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (\"OTE\") range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.Annual Salary:$170,000 - $295,000 USDLogistics Education requirements: We require at least a Bachelor's degree in a related field or equivalent experience.Location-based hybrid policy: Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices. Visa sponsorship: We do sponsor visas! However, we aren't able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this. We encourage you to apply even if you do not believe you meet every single qualification. Not all strong candidates will meet every single qualification as listed. Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you're interested in this work. We think AI systems like the ones we're building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.Your safety matters to us. To protect yourself from potential scams, remember that Anthropic recruiters only contact you from @anthropic.com email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you're ever unsure about a communication, don't click any links—visit anthropic.com/careers directly for confirmed position openings. How we're different We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We're an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills. The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI & Compute, Concrete Problems in AI Safety, and Learning from Human Preferences. Come work with us! Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. Guidance on Candidates' AI Usage: Learn about our policy for using AI in our application process",
    "id": "job-boards-greenhouse-io-anthropic-jobs-4604015008",
    "source": "greenhouse",
    "source_url": "boards-api.greenhouse.io",
    "source_id": "4604015008",
    "title": "Recruiter, AI Research",
    "company_name": "anthropic",
    "company_slug": "anthropic",
    "location": "San Francisco, CA",
    "locations": [
      "San Francisco, CA"
    ],
    "url": "https://job-boards.greenhouse.io/anthropic/jobs/4604015008",
    "departments": [
      "People"
    ],
    "employment_type": null,
    "posted_at": "2025-11-22T09:28:05-05:00",
    "fetched_at": "2026-01-12T18:26:14.167Z",
    "description": "&lt;div class=&quot;content-intro&quot;&gt;&lt;h2&gt;&lt;strong&gt;About Anthropic&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.&lt;/p&gt;&lt;/div&gt;&lt;h2&gt;About the Role:&amp;nbsp;&lt;/h2&gt;\n&lt;p&gt;Anthropic is looking for a talented AI Research Recruiter to partner with our Research teams. In this pivotal role, you will be instrumental in shaping the future of our organization by identifying, engaging, and hiring the best and brightest minds across a range of disciplines. As we continue to push the boundaries of AI research and development, we need a passionate recruiter who can help us build a world-class team dedicated to creating safe and beneficial AI systems.&lt;/p&gt;\n&lt;h2&gt;Responsibilities:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Develop and execute strategic recruiting plans to identify, source, and hire highly qualified candidates, with a focus on Machine Learning and AI research talent&lt;/li&gt;\n&lt;li&gt;Partner with Research hiring managers and interviewers to understand hiring needs, team matching, required skills and qualifications&lt;/li&gt;\n&lt;li&gt;Enhance and implement recruiting processes and programs while maintaining an inclusive and high talent bar, such as developing targeted outreach campaigns, building connections with industry leaders, and removing any unfair biases from the hiring process&lt;/li&gt;\n&lt;li&gt;Collaborate with leadership and cross-functional partners to understand organizational needs and map out long-term talent acquisition strategies that balance priorities across all technical teams&lt;/li&gt;\n&lt;li&gt;Enhance Anthropic&#39;s employer brand within the research and science community to showcase our mission, culture, and values to candidates&lt;/li&gt;\n&lt;li&gt;Stay up-to-date on recruiting best practices, emerging sourcing techniques, interview innovations, and workplace trends&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;You may be a good fit if you:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Have 5+ years of experience in full life cycle recruiting supporting technical research teams&lt;/li&gt;\n&lt;li&gt;Have a passion for AI&#39;s potential to positively impact the world and realistic assessment of its risks and limitations&lt;/li&gt;\n&lt;li&gt;Are experimental and are open to new, creative recruiting ideas, or have experience working with hiring managers who are open to non-traditional talent strategies&lt;/li&gt;\n&lt;li&gt;Thrive in fast-paced, dynamic environments and enjoy juggling multiple priorities&lt;/li&gt;\n&lt;li&gt;Possess strong technical aptitude with the ability to understand and evaluate technical qualifications&lt;/li&gt;\n&lt;li&gt;Have enthusiasm for deeply understanding the needs of researchers and innovating on recruiting processes to make them more tailored to the world of research&lt;/li&gt;\n&lt;li&gt;Have excellent organizational skills and attention to detail, as well as a proactive mindset and ability to operate with autonomy&lt;/li&gt;\n&lt;li&gt;Have experience partnering with researchers and hiring talent that work on GenAI and LLMs&lt;/li&gt;\n&lt;li&gt;Have a proven track record of scaling and building diverse and high-performing teams in a fast-paced, high-growth startup environment&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;&lt;strong&gt;Strong candidates may also:&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Bring a deep interest in AI safety research&lt;/li&gt;\n&lt;li&gt;Have experience partnering with researchers and hiring talent that work on GenAI and LLMs&lt;/li&gt;\n&lt;li&gt;Have experience with academic recruitment and research communities&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;&lt;strong&gt;Deadline to apply:&amp;nbsp;&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;None. Applications will be reviewed on a rolling basis.&lt;/p&gt;&lt;div class=&quot;content-pay-transparency&quot;&gt;&lt;div class=&quot;pay-input&quot;&gt;&lt;div class=&quot;description&quot;&gt;&lt;p&gt;The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (&quot;OTE&quot;) range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;title&quot;&gt;Annual Salary:&lt;/div&gt;&lt;div class=&quot;pay-range&quot;&gt;&lt;span&gt;$170,000&lt;/span&gt;&lt;span class=&quot;divider&quot;&gt;&amp;mdash;&lt;/span&gt;&lt;span&gt;$295,000 USD&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;content-conclusion&quot;&gt;&lt;h2&gt;&lt;strong&gt;Logistics&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;Education requirements: &lt;/strong&gt;We require at least a Bachelor&#39;s degree in a related field or equivalent experience.&lt;strong&gt;&lt;br&gt;&lt;br&gt;Location-based hybrid policy:&lt;/strong&gt; Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices.&lt;/p&gt;\n&lt;p&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Visa sponsorship:&lt;/strong&gt;&amp;nbsp;We do sponsor visas! However, we aren&#39;t able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;We encourage you to apply even if you do not believe you meet every single qualification.&lt;/strong&gt; Not all strong candidates will meet every single qualification as listed.&amp;nbsp; Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you&#39;re interested in this work. We think AI systems like the ones we&#39;re building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.&lt;br&gt;&lt;br&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Your safety matters to us.&lt;/strong&gt;&amp;nbsp;To protect yourself from potential scams, remember that Anthropic recruiters only contact you from&amp;nbsp;@anthropic.com&amp;nbsp;email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you&#39;re ever unsure about a communication, don&#39;t click any links—visit&amp;nbsp;&lt;u data-stringify-type=&quot;underline&quot;&gt;&lt;a class=&quot;c-link c-link--underline&quot; href=&quot;http://anthropic.com/careers&quot; target=&quot;_blank&quot; data-stringify-link=&quot;http://anthropic.com/careers&quot; data-sk=&quot;tooltip_parent&quot; data-remove-tab-index=&quot;true&quot;&gt;anthropic.com/careers&lt;/a&gt;&lt;/u&gt;&amp;nbsp;directly for confirmed position openings.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;How we&#39;re different&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We&#39;re an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills.&lt;/p&gt;\n&lt;p&gt;The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI &amp;amp; Compute, Concrete Problems in AI Safety, and Learning from Human Preferences.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Come work with us!&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. &lt;strong data-stringify-type=&quot;bold&quot;&gt;Guidance on Candidates&#39; AI Usage:&lt;/strong&gt;&amp;nbsp;Learn about&amp;nbsp;&lt;a class=&quot;c-link&quot; href=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; target=&quot;_blank&quot; data-stringify-link=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; data-sk=&quot;tooltip_parent&quot;&gt;our policy&lt;/a&gt;&amp;nbsp;for using AI in our application process&lt;/p&gt;&lt;/div&gt;",
    "_raw": {
      "source": "greenhouse",
      "original_id": 4604015008
    },
    "job_posted_at": "1h",
    "description_platform": "greenhouse",
    "description_success": true
  },
  {
    "job_title": "Research Engineer, Interpretability",
    "employer_name": "anthropic",
    "job_city": "San Francisco, CA",
    "job_apply_link": "https://job-boards.greenhouse.io/anthropic/jobs/4980430008",
    "job_posted_at_datetime_utc": "2025-11-22T09:28:05-05:00",
    "job_description": "About Anthropic Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems. About the role: When you see what modern language models are capable of, do you wonder, \"How do these things work? How can we trust them?\" The Interpretability team at Anthropic is working to reverse-engineer how trained models work because we believe that a mechanistic understanding is the most robust way to make advanced systems safe. We’re looking for researchers and engineers to join our efforts. People mean many different things by \"interpretability\". We're focused on mechanistic interpretability, which aims to discover how neural network parameters map to meaningful algorithms. Some useful analogies might be to think of us as trying to do \"biology\" or \"neuroscience\" of neural networks using “microscopes” we build, or as treating neural networks as binary computer programs we're trying to \"reverse engineer\". A few places to learn more about our work and team at a high level are this introduction to Interpretability from our research lead, Chris Olah; a discussion of our work on the Hard Fork podcast produced by the New York Times, and this blog post (and accompanying video) sharing more about some of the engineering challenges we’d had to solve to get these results. Some of our team's notable publications include A Mathematical Framework for Transformer Circuits, In-context Learning and Induction Heads, Toy Models of Superposition, Scaling Monosemanticity, and our Circuits’ Methods and Biology papers. This work builds on ideas from members' work prior to Anthropic such as the original circuits thread, Multimodal Neurons, Activation Atlases, and Building Blocks. We aim to create a solid foundation for mechanistically understanding neural networks and making them safe (see our vision post). In the short term, we have focused on resolving the issue of \"superposition\" (see Toy Models of Superposition, Superposition, Memorization, and Double Descent, and our May 2023 update), which causes the computational units of the models, like neurons and attention heads, to be individually uninterpretable, and on finding ways to decompose models into more interpretable components. Our subsequent work found millions of features in Sonnet, one of our production language models, represents progress in this direction. In our most recent work, we develop methods that allow us to build circuits using features and use this circuits to understand the mechanisms associated with a model's computation and study specific examples of multi-hop reasoning, planning, and chain-of-thought faithfulness on Haiku 3.5, one of our production models.” This is a stepping stone towards our overall goal of mechanistically understanding neural networks. We often collaborate with teams across Anthropic, such as Alignment Science and Societal Impacts to use our work to make Anthropic’s models safer. We also have an Interpretability Architectures project that involves collaborating with Pretraining. Responsibilities: Implement and analyze research experiments, both quickly in toy scenarios and at scale in large models Set up and optimize research workflows to run efficiently and reliably at large scale Build tools and abstractions to support rapid pace of research experimentation Develop and improve tools and infrastructure to support other teams in using Interpretability’s work to improve model safety You may be a good fit if you: Have 5-10+ years of experience building software Are highly proficient in at least one programming language (e.g., Python, Rust, Go, Java) and productive with python Have some experience contributing to empirical AI research projects Have a strong ability to prioritize and direct effort toward the most impactful work and are comfortable operating with ambiguity and questioning assumptions. Prefer fast-moving collaborative projects to extensive solo efforts Want to learn more about machine learning research and its applications and collaborate closely with researchers Care about the societal impacts and ethics of your work Strong candidates may also have experience with: Designing a code base so that anyone can quickly code experiments, launch them, and analyze their results without hitting bugs Optimizing the performance of large-scale distributed systems Collaborating closely with researchers Language modeling with transformers GPUs or Pytorch Representative Projects: Building Garcon, a tool that allows researchers to easily access LLMs internals from a jupyter notebook Setting up and optimizing a pipeline to efficiently collect petabytes of transformer activations and shuffle them. Profiling and optimizing ML training, including parallelizing to many GPUs Make launching ML experiments and manipulating+analyzing the results fast and easy Creating an interactive visualization of attention between tokens in a language model Role Specific Location Policy: This role is based in San Francisco office; however, we are open to considering exceptional candidates for remote work on a case-by-case basis. The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (\"OTE\") range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.Annual Salary:$315,000 - $560,000 USDLogistics Education requirements: We require at least a Bachelor's degree in a related field or equivalent experience.Location-based hybrid policy: Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices. Visa sponsorship: We do sponsor visas! However, we aren't able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this. We encourage you to apply even if you do not believe you meet every single qualification. Not all strong candidates will meet every single qualification as listed. Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you're interested in this work. We think AI systems like the ones we're building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.Your safety matters to us. To protect yourself from potential scams, remember that Anthropic recruiters only contact you from @anthropic.com email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you're ever unsure about a communication, don't click any links—visit anthropic.com/careers directly for confirmed position openings. How we're different We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We're an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills. The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI & Compute, Concrete Problems in AI Safety, and Learning from Human Preferences. Come work with us! Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. Guidance on Candidates' AI Usage: Learn about our policy for using AI in our application process",
    "id": "job-boards-greenhouse-io-anthropic-jobs-4980430008",
    "source": "greenhouse",
    "source_url": "boards-api.greenhouse.io",
    "source_id": "4980430008",
    "title": "Research Engineer, Interpretability",
    "company_name": "anthropic",
    "company_slug": "anthropic",
    "location": "San Francisco, CA",
    "locations": [
      "San Francisco, CA"
    ],
    "url": "https://job-boards.greenhouse.io/anthropic/jobs/4980430008",
    "departments": [
      "AI Research & Engineering"
    ],
    "employment_type": null,
    "posted_at": "2025-11-22T09:28:05-05:00",
    "fetched_at": "2026-01-12T18:26:14.167Z",
    "description": "&lt;div class=&quot;content-intro&quot;&gt;&lt;h2&gt;&lt;strong&gt;About Anthropic&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;\n&lt;h2&gt;About the role:&lt;/h2&gt;\n&lt;/div&gt;\n&lt;div&gt;\n&lt;p&gt;When you see what modern language models are capable of, do you wonder, &quot;How do these things work? How can we trust them?&quot;&lt;/p&gt;\n&lt;p&gt;The Interpretability team at Anthropic is working to reverse-engineer how trained models work because we believe that a mechanistic understanding is the most robust way to make advanced systems safe. We’re looking for researchers and engineers to join our efforts.&amp;nbsp;&lt;/p&gt;\n&lt;p&gt;&amp;nbsp;People mean many different things by &quot;interpretability&quot;. We&#39;re focused on mechanistic interpretability, which aims to discover how neural network parameters map to meaningful algorithms. Some useful analogies might be to think of us as trying to do &quot;biology&quot; or &quot;neuroscience&quot; of neural networks using “microscopes” we build, or as treating neural networks as binary computer programs we&#39;re trying to &quot;reverse engineer&quot;.&lt;/p&gt;\n&lt;p&gt;&amp;nbsp;A few places to learn more about our work and team at a high level are &lt;a href=&quot;https://www.youtube.com/watch?v=TxhhMTOTMDg&quot;&gt;this introduction to Interpretability&lt;/a&gt; from our research lead, &lt;a href=&quot;https://colah.github.io/about.html&quot;&gt;Chris Olah&lt;/a&gt;; a &lt;a href=&quot;https://open.spotify.com/episode/5UF79Uu94ia0fwC32a89LU&quot;&gt;discussion of our work&lt;/a&gt; on the &lt;a href=&quot;https://www.nytimes.com/column/hard-fork&quot;&gt;Hard Fork podcast&lt;/a&gt; produced by the New York Times, and this &lt;a href=&quot;https://www.anthropic.com/research/engineering-challenges-interpretability&quot;&gt;blog post&lt;/a&gt; (and accompanying video) sharing more about some of the engineering challenges we’d had to solve to get these results. Some of our team&#39;s notable publications include &lt;a href=&quot;https://transformer-circuits.pub/2021/framework/index.html&quot;&gt;A Mathematical Framework for Transformer Circuits&lt;/a&gt;, &lt;a href=&quot;https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html&quot;&gt;In-context Learning and Induction Heads&lt;/a&gt;, &lt;a href=&quot;https://transformer-circuits.pub/2022/toy_model/index.html&quot;&gt;Toy Models of Superposition&lt;/a&gt;, &lt;a href=&quot;https://transformer-circuits.pub/2024/scaling-monosemanticity/&quot;&gt;Scaling Monosemanticity&lt;/a&gt;, and our Circuits’ &lt;a href=&quot;https://transformer-circuits.pub/2025/attribution-graphs/methods.html&quot;&gt;Methods&lt;/a&gt; and &lt;a href=&quot;https://transformer-circuits.pub/2025/attribution-graphs/biology.html&quot;&gt;Biology&lt;/a&gt; papers. This work builds on ideas from members&#39; work prior to Anthropic such as the &lt;a href=&quot;https://distill.pub/2020/circuits/&quot;&gt;original circuits thread&lt;/a&gt;, &lt;a href=&quot;https://distill.pub/2021/multimodal-neurons/&quot;&gt;Multimodal Neurons&lt;/a&gt;, &lt;a href=&quot;https://distill.pub/2019/activation-atlas/&quot;&gt;Activation Atlases&lt;/a&gt;, and &lt;a href=&quot;https://distill.pub/2018/building-blocks/&quot;&gt;Building Blocks&lt;/a&gt;.&lt;/p&gt;\n&lt;p&gt;We aim to create a solid foundation for mechanistically understanding neural networks and making them safe (see our &lt;a href=&quot;https://transformer-circuits.pub/2023/interpretability-dreams/index.html&quot;&gt;vision post&lt;/a&gt;). In the short term, we have focused on resolving the issue of &quot;superposition&quot; (see &lt;a href=&quot;https://transformer-circuits.pub/2022/toy_model/index.html&quot;&gt;Toy Models of Superposition&lt;/a&gt;, &lt;a href=&quot;https://transformer-circuits.pub/2023/toy-double-descent/index.html&quot;&gt;Superposition, Memorization, and Double Descent&lt;/a&gt;, and our &lt;a href=&quot;https://transformer-circuits.pub/2023/may-update/index.html&quot;&gt;May 2023 update&lt;/a&gt;), which causes the computational units of the models, like neurons and attention heads, to be individually uninterpretable, and on finding ways to decompose models into more interpretable components. Our subsequent &lt;a href=&quot;https://www.anthropic.com/news/mapping-mind-language-model&quot;&gt;work&lt;/a&gt; found millions of features in Sonnet, one of our production language models, represents progress in this direction. In our most recent work, we develop methods that allow us to build circuits using features and use this circuits to understand the mechanisms associated with a model&#39;s computation and study specific examples of multi-hop reasoning, planning, and chain-of-thought faithfulness on Haiku 3.5, one of our production models.” This is a stepping stone towards our overall goal of mechanistically understanding neural networks.&lt;/p&gt;\n&lt;p&gt;We often collaborate with teams across Anthropic, such as Alignment Science and Societal Impacts to use our work to make Anthropic’s models safer. We also have an &lt;a href=&quot;https://transformer-circuits.pub/2024/april-update/index.html#interpretability-architecture&quot;&gt;Interpretability Architectures project&lt;/a&gt; that involves collaborating with Pretraining.&lt;/p&gt;\n&lt;/div&gt;\n&lt;div&gt;\n&lt;div class=&quot;section page-centered&quot;&gt;\n&lt;h2&gt;Responsibilities:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Implement and analyze research experiments, both quickly in toy scenarios and at scale in large models&lt;/li&gt;\n&lt;li&gt;Set up and optimize research workflows to run efficiently and reliably at large scale&lt;/li&gt;\n&lt;li&gt;Build tools and abstractions to support rapid pace of research experimentation&lt;/li&gt;\n&lt;li&gt;Develop and improve tools and infrastructure to support other teams in using Interpretability’s work to improve model safety&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;\n&lt;div class=&quot;section page-centered&quot;&gt;\n&lt;h2&gt;You may be a good fit if you:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Have 5-10+ years of experience building software&lt;/li&gt;\n&lt;li&gt;Are highly proficient in at least one programming language (e.g., Python, Rust, Go, Java) and productive with python&lt;/li&gt;\n&lt;li&gt;Have some experience contributing to empirical AI research projects&lt;/li&gt;\n&lt;li&gt;Have a strong ability to prioritize and direct effort toward the most impactful work and are comfortable operating with ambiguity and questioning assumptions.&lt;/li&gt;\n&lt;li&gt;Prefer fast-moving collaborative projects to extensive solo efforts&lt;/li&gt;\n&lt;li&gt;Want to learn more about machine learning research and its applications and collaborate closely with researchers&lt;/li&gt;\n&lt;li&gt;Care about the societal impacts and ethics of your work&lt;/li&gt;\n&lt;/ul&gt;\n&lt;div class=&quot;section page-centered&quot;&gt;\n&lt;h2&gt;Strong candidates may also have experience with:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Designing a code base so that anyone can quickly code experiments, launch them, and analyze their results without hitting bugs&lt;/li&gt;\n&lt;li&gt;Optimizing the performance of large-scale distributed systems&lt;/li&gt;\n&lt;li&gt;Collaborating closely with researchers&lt;/li&gt;\n&lt;li&gt;Language modeling with transformers&lt;/li&gt;\n&lt;li&gt;GPUs or Pytorch&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;\n&lt;div class=&quot;section page-centered&quot;&gt;\n&lt;h2&gt;Representative Projects:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Building &lt;a href=&quot;https://transformer-circuits.pub/2021/garcon/index.html&quot;&gt;Garcon&lt;/a&gt;, a tool that allows researchers to easily access LLMs internals from a jupyter notebook&lt;/li&gt;\n&lt;li&gt;Setting up and optimizing a pipeline to efficiently collect petabytes of transformer activations and shuffle them.&lt;/li&gt;\n&lt;li&gt;Profiling and optimizing ML training, including parallelizing to many GPUs&lt;/li&gt;\n&lt;li&gt;Make launching ML experiments and manipulating+analyzing the results fast and easy&lt;/li&gt;\n&lt;li&gt;Creating an interactive visualization of attention between tokens in a language model&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;&lt;strong&gt;Role Specific Location Policy:&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;This role is based in San Francisco office; however, we are open to considering exceptional candidates for remote work on a case-by-case basis.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;&lt;div class=&quot;content-pay-transparency&quot;&gt;&lt;div class=&quot;pay-input&quot;&gt;&lt;div class=&quot;description&quot;&gt;&lt;p&gt;The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (&quot;OTE&quot;) range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;title&quot;&gt;Annual Salary:&lt;/div&gt;&lt;div class=&quot;pay-range&quot;&gt;&lt;span&gt;$315,000&lt;/span&gt;&lt;span class=&quot;divider&quot;&gt;&amp;mdash;&lt;/span&gt;&lt;span&gt;$560,000 USD&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;content-conclusion&quot;&gt;&lt;h2&gt;&lt;strong&gt;Logistics&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;Education requirements: &lt;/strong&gt;We require at least a Bachelor&#39;s degree in a related field or equivalent experience.&lt;strong&gt;&lt;br&gt;&lt;br&gt;Location-based hybrid policy:&lt;/strong&gt; Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices.&lt;/p&gt;\n&lt;p&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Visa sponsorship:&lt;/strong&gt;&amp;nbsp;We do sponsor visas! However, we aren&#39;t able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;We encourage you to apply even if you do not believe you meet every single qualification.&lt;/strong&gt; Not all strong candidates will meet every single qualification as listed.&amp;nbsp; Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you&#39;re interested in this work. We think AI systems like the ones we&#39;re building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.&lt;br&gt;&lt;br&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Your safety matters to us.&lt;/strong&gt;&amp;nbsp;To protect yourself from potential scams, remember that Anthropic recruiters only contact you from&amp;nbsp;@anthropic.com&amp;nbsp;email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you&#39;re ever unsure about a communication, don&#39;t click any links—visit&amp;nbsp;&lt;u data-stringify-type=&quot;underline&quot;&gt;&lt;a class=&quot;c-link c-link--underline&quot; href=&quot;http://anthropic.com/careers&quot; target=&quot;_blank&quot; data-stringify-link=&quot;http://anthropic.com/careers&quot; data-sk=&quot;tooltip_parent&quot; data-remove-tab-index=&quot;true&quot;&gt;anthropic.com/careers&lt;/a&gt;&lt;/u&gt;&amp;nbsp;directly for confirmed position openings.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;How we&#39;re different&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We&#39;re an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills.&lt;/p&gt;\n&lt;p&gt;The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI &amp;amp; Compute, Concrete Problems in AI Safety, and Learning from Human Preferences.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Come work with us!&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. &lt;strong data-stringify-type=&quot;bold&quot;&gt;Guidance on Candidates&#39; AI Usage:&lt;/strong&gt;&amp;nbsp;Learn about&amp;nbsp;&lt;a class=&quot;c-link&quot; href=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; target=&quot;_blank&quot; data-stringify-link=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; data-sk=&quot;tooltip_parent&quot;&gt;our policy&lt;/a&gt;&amp;nbsp;for using AI in our application process&lt;/p&gt;&lt;/div&gt;",
    "_raw": {
      "source": "greenhouse",
      "original_id": 4980430008
    },
    "job_posted_at": "1h",
    "description_platform": "greenhouse",
    "description_success": true
  },
  {
    "job_title": "Research Engineer, Model Evaluations",
    "employer_name": "anthropic",
    "job_city": "San Francisco, CA | New York City, NY",
    "job_apply_link": "https://job-boards.greenhouse.io/anthropic/jobs/4990535008",
    "job_posted_at_datetime_utc": "2025-11-22T09:28:05-05:00",
    "job_description": "About Anthropic Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.About the role As a Research Engineer on the Model Evaluations team, you'll lead the design and implementation of Anthropic's evaluation platform—a critical system that shapes how we understand, measure, and improve our models' capabilities and safety. You'll work at the intersection of research and engineering to develop and implement model evaluations that give us insight into emerging capabilities and build robust evaluation infrastructure that directly influences our training decisions and model development roadmap. Your work will be essential to Anthropic's mission of building safe, beneficial AI systems. You'll collaborate closely with training teams, alignment researchers, and safety teams to ensure our models meet the highest standards before deployment. This is a technical leadership role where you'll drive both the strategic vision and hands-on implementation of our evaluation systems. Responsibilities Design novel evaluation methodologies to assess model capabilities across diverse domains including reasoning, safety, helpfulness, and harmlessness Lead the design and architecture of Anthropic's evaluation platform, ensuring it scales with our rapidly evolving model capabilities and research needs Implement and maintain high-throughput evaluation pipelines that run during production training, providing real-time insights to guide training decisions Analyze evaluation results to identify patterns, failure modes, and opportunities for model improvement, translating complex findings into actionable insights Partner with research teams to develop domain-specific evaluations that probe for emerging capabilities and potential risks Build infrastructure to enable rapid iteration on evaluation design, supporting both automated and human-in-the-loop assessment approaches Establish best practices and standards for evaluation development across the organization Mentor team members and contribute to the growth of evaluation expertise at Anthropic Coordinate evaluation efforts during critical training runs, ensuring comprehensive coverage and timely results Contribute to research publications and external communications about evaluation methodologies and findings You may be a good fit if you Have experience designing and implementing evaluation systems for machine learning models, particularly large language models Have demonstrated technical leadership experience, either formally or through leading complex technical projects Are skilled at both systems engineering and experimental design, comfortable building infrastructure while maintaining scientific rigor Have strong programming skills in Python and experience with distributed computing frameworks Can translate between research needs and engineering constraints, finding pragmatic solutions to complex problems Are results-oriented and thrive in fast-paced environments where priorities can shift based on research findings Enjoy collaborative work and can effectively communicate technical concepts to diverse stakeholders Care deeply about AI safety and the societal impacts of the systems we build Have experience with statistical analysis and can draw meaningful conclusions from large-scale experimental data Strong candidates may also have Experience with evaluation during model training, particularly in production environments Familiarity with safety evaluation frameworks and red teaming methodologies Background in psychometrics, experimental psychology, or other fields focused on measurement and assessment Experience with reinforcement learning evaluation or multi-agent systems Contributions to open-source evaluation benchmarks or frameworks Knowledge of prompt engineering and its role in evaluation design Experience managing evaluation infrastructure at scale (thousands of experiments) Published research in machine learning evaluation, benchmarking, or related areas Representative projects Designing comprehensive evaluation suites that assess models across hundreds of capability dimensions Building real-time evaluation dashboards that surface critical insights during multi-week training runs Developing novel evaluation approaches for emerging capabilities like multi-step reasoning or tool use Creating automated systems to detect regression in model performance or safety properties Implementing efficient evaluation sampling strategies that balance coverage with computational constraints Collaborating with external partners to develop industry-standard evaluation benchmarks Building infrastructure to support human evaluation at scale, including quality control and aggregation systems The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (\"OTE\") range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.Annual Salary:$300,000 - $405,000 USDLogistics Education requirements: We require at least a Bachelor's degree in a related field or equivalent experience.Location-based hybrid policy: Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices. Visa sponsorship: We do sponsor visas! However, we aren't able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this. We encourage you to apply even if you do not believe you meet every single qualification. Not all strong candidates will meet every single qualification as listed. Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you're interested in this work. We think AI systems like the ones we're building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.Your safety matters to us. To protect yourself from potential scams, remember that Anthropic recruiters only contact you from @anthropic.com email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you're ever unsure about a communication, don't click any links—visit anthropic.com/careers directly for confirmed position openings. How we're different We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We're an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills. The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI & Compute, Concrete Problems in AI Safety, and Learning from Human Preferences. Come work with us! Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. Guidance on Candidates' AI Usage: Learn about our policy for using AI in our application process",
    "id": "job-boards-greenhouse-io-anthropic-jobs-4990535008",
    "source": "greenhouse",
    "source_url": "boards-api.greenhouse.io",
    "source_id": "4990535008",
    "title": "Research Engineer, Model Evaluations",
    "company_name": "anthropic",
    "company_slug": "anthropic",
    "location": "San Francisco, CA | New York City, NY",
    "locations": [
      "San Francisco, CA | New York City, NY"
    ],
    "url": "https://job-boards.greenhouse.io/anthropic/jobs/4990535008",
    "departments": [
      "AI Research & Engineering"
    ],
    "employment_type": null,
    "posted_at": "2025-11-22T09:28:05-05:00",
    "fetched_at": "2026-01-12T18:26:14.167Z",
    "description": "&lt;div class=&quot;content-intro&quot;&gt;&lt;h2&gt;&lt;strong&gt;About Anthropic&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.&lt;/p&gt;&lt;/div&gt;&lt;h2&gt;&lt;strong&gt;About the role&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;As a Research Engineer on the Model Evaluations team, you&#39;ll lead the design and implementation of Anthropic&#39;s evaluation platform—a critical system that shapes how we understand, measure, and improve our models&#39; capabilities and safety. You&#39;ll work at the intersection of research and engineering to develop and implement model evaluations that give us insight into emerging capabilities and build robust evaluation infrastructure that directly influences our training decisions and model development roadmap.&lt;/p&gt;\n&lt;p&gt;Your work will be essential to Anthropic&#39;s mission of building safe, beneficial AI systems. You&#39;ll collaborate closely with training teams, alignment researchers, and safety teams to ensure our models meet the highest standards before deployment. This is a technical leadership role where you&#39;ll drive both the strategic vision and hands-on implementation of our evaluation systems.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Responsibilities&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Design novel evaluation methodologies to assess model capabilities across diverse domains including reasoning, safety, helpfulness, and harmlessness&lt;/li&gt;\n&lt;li&gt;Lead the design and architecture of Anthropic&#39;s evaluation platform, ensuring it scales with our rapidly evolving model capabilities and research needs&lt;/li&gt;\n&lt;li&gt;Implement and maintain high-throughput evaluation pipelines that run during production training, providing real-time insights to guide training decisions&lt;/li&gt;\n&lt;li&gt;Analyze evaluation results to identify patterns, failure modes, and opportunities for model improvement, translating complex findings into actionable insights&lt;/li&gt;\n&lt;li&gt;Partner with research teams to develop domain-specific evaluations that probe for emerging capabilities and potential risks&lt;/li&gt;\n&lt;li&gt;Build infrastructure to enable rapid iteration on evaluation design, supporting both automated and human-in-the-loop assessment approaches&lt;/li&gt;\n&lt;li&gt;Establish best practices and standards for evaluation development across the organization&lt;/li&gt;\n&lt;li&gt;Mentor team members and contribute to the growth of evaluation expertise at Anthropic&lt;/li&gt;\n&lt;li&gt;Coordinate evaluation efforts during critical training runs, ensuring comprehensive coverage and timely results&lt;/li&gt;\n&lt;li&gt;Contribute to research publications and external communications about evaluation methodologies and findings&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;&lt;strong&gt;You may be a good fit if you&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Have experience designing and implementing evaluation systems for machine learning models, particularly large language models&lt;/li&gt;\n&lt;li&gt;Have demonstrated technical leadership experience, either formally or through leading complex technical projects&lt;/li&gt;\n&lt;li&gt;Are skilled at both systems engineering and experimental design, comfortable building infrastructure while maintaining scientific rigor&lt;/li&gt;\n&lt;li&gt;Have strong programming skills in Python and experience with distributed computing frameworks&lt;/li&gt;\n&lt;li&gt;Can translate between research needs and engineering constraints, finding pragmatic solutions to complex problems&lt;/li&gt;\n&lt;li&gt;Are results-oriented and thrive in fast-paced environments where priorities can shift based on research findings&lt;/li&gt;\n&lt;li&gt;Enjoy collaborative work and can effectively communicate technical concepts to diverse stakeholders&lt;/li&gt;\n&lt;li&gt;Care deeply about AI safety and the societal impacts of the systems we build&lt;/li&gt;\n&lt;li&gt;Have experience with statistical analysis and can draw meaningful conclusions from large-scale experimental data&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;&lt;strong&gt;Strong candidates may also have&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Experience with evaluation during model training, particularly in production environments&lt;/li&gt;\n&lt;li&gt;Familiarity with safety evaluation frameworks and red teaming methodologies&lt;/li&gt;\n&lt;li&gt;Background in psychometrics, experimental psychology, or other fields focused on measurement and assessment&lt;/li&gt;\n&lt;li&gt;Experience with reinforcement learning evaluation or multi-agent systems&lt;/li&gt;\n&lt;li&gt;Contributions to open-source evaluation benchmarks or frameworks&lt;/li&gt;\n&lt;li&gt;Knowledge of prompt engineering and its role in evaluation design&lt;/li&gt;\n&lt;li&gt;Experience managing evaluation infrastructure at scale (thousands of experiments)&lt;/li&gt;\n&lt;li&gt;Published research in machine learning evaluation, benchmarking, or related areas&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;&lt;strong&gt;Representative projects&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Designing comprehensive evaluation suites that assess models across hundreds of capability dimensions&lt;/li&gt;\n&lt;li&gt;Building real-time evaluation dashboards that surface critical insights during multi-week training runs&lt;/li&gt;\n&lt;li&gt;Developing novel evaluation approaches for emerging capabilities like multi-step reasoning or tool use&lt;/li&gt;\n&lt;li&gt;Creating automated systems to detect regression in model performance or safety properties&lt;/li&gt;\n&lt;li&gt;Implementing efficient evaluation sampling strategies that balance coverage with computational constraints&lt;/li&gt;\n&lt;li&gt;Collaborating with external partners to develop industry-standard evaluation benchmarks&lt;/li&gt;\n&lt;li&gt;Building infrastructure to support human evaluation at scale, including quality control and aggregation systems&amp;nbsp;&lt;/li&gt;\n&lt;/ul&gt;&lt;div class=&quot;content-pay-transparency&quot;&gt;&lt;div class=&quot;pay-input&quot;&gt;&lt;div class=&quot;description&quot;&gt;&lt;p&gt;The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (&quot;OTE&quot;) range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;title&quot;&gt;Annual Salary:&lt;/div&gt;&lt;div class=&quot;pay-range&quot;&gt;&lt;span&gt;$300,000&lt;/span&gt;&lt;span class=&quot;divider&quot;&gt;&amp;mdash;&lt;/span&gt;&lt;span&gt;$405,000 USD&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;content-conclusion&quot;&gt;&lt;h2&gt;&lt;strong&gt;Logistics&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;Education requirements: &lt;/strong&gt;We require at least a Bachelor&#39;s degree in a related field or equivalent experience.&lt;strong&gt;&lt;br&gt;&lt;br&gt;Location-based hybrid policy:&lt;/strong&gt; Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices.&lt;/p&gt;\n&lt;p&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Visa sponsorship:&lt;/strong&gt;&amp;nbsp;We do sponsor visas! However, we aren&#39;t able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;We encourage you to apply even if you do not believe you meet every single qualification.&lt;/strong&gt; Not all strong candidates will meet every single qualification as listed.&amp;nbsp; Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you&#39;re interested in this work. We think AI systems like the ones we&#39;re building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.&lt;br&gt;&lt;br&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Your safety matters to us.&lt;/strong&gt;&amp;nbsp;To protect yourself from potential scams, remember that Anthropic recruiters only contact you from&amp;nbsp;@anthropic.com&amp;nbsp;email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you&#39;re ever unsure about a communication, don&#39;t click any links—visit&amp;nbsp;&lt;u data-stringify-type=&quot;underline&quot;&gt;&lt;a class=&quot;c-link c-link--underline&quot; href=&quot;http://anthropic.com/careers&quot; target=&quot;_blank&quot; data-stringify-link=&quot;http://anthropic.com/careers&quot; data-sk=&quot;tooltip_parent&quot; data-remove-tab-index=&quot;true&quot;&gt;anthropic.com/careers&lt;/a&gt;&lt;/u&gt;&amp;nbsp;directly for confirmed position openings.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;How we&#39;re different&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We&#39;re an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills.&lt;/p&gt;\n&lt;p&gt;The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI &amp;amp; Compute, Concrete Problems in AI Safety, and Learning from Human Preferences.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Come work with us!&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. &lt;strong data-stringify-type=&quot;bold&quot;&gt;Guidance on Candidates&#39; AI Usage:&lt;/strong&gt;&amp;nbsp;Learn about&amp;nbsp;&lt;a class=&quot;c-link&quot; href=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; target=&quot;_blank&quot; data-stringify-link=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; data-sk=&quot;tooltip_parent&quot;&gt;our policy&lt;/a&gt;&amp;nbsp;for using AI in our application process&lt;/p&gt;&lt;/div&gt;",
    "_raw": {
      "source": "greenhouse",
      "original_id": 4990535008
    },
    "job_posted_at": "1h",
    "description_platform": "greenhouse",
    "description_success": true
  },
  {
    "job_title": "Research Engineer, Pretraining Scaling",
    "employer_name": "anthropic",
    "job_city": "San Francisco, CA",
    "job_apply_link": "https://job-boards.greenhouse.io/anthropic/jobs/4938432008",
    "job_posted_at_datetime_utc": "2025-11-22T09:28:05-05:00",
    "job_description": "About Anthropic Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.About the Role: Anthropic's ML Performance and Scaling team trains our production pretrained models, work that directly shapes the company's future and our mission to build safe, beneficial AI systems. As a Research Engineer on this team, you'll ensure our frontier models train reliably, efficiently, and at scale. This is demanding, high-impact work that requires both deep technical expertise and a genuine passion for the craft of large-scale ML systems. This role lives at the boundary between research and engineering. You'll work across our entire production training stack: performance optimization, hardware debugging, experimental design, and launch coordination. During launches, the team works in tight lockstep, responding to production issues that can't wait for tomorrow. Responsibilities: Own critical aspects of our production pretraining pipeline, including model operations, performance optimization, observability, and reliability Debug and resolve complex issues across the full stack—from hardware errors and networking to training dynamics and evaluation infrastructure Design and run experiments to improve training efficiency, reduce step time, increase uptime, and enhance model performance Respond to on-call incidents during model launches, diagnosing problems quickly and coordinating solutions across teams Build and maintain production logging, monitoring dashboards, and evaluation infrastructure Add new capabilities to the training codebase, such as long context support or novel architectures Collaborate closely with teammates across SF and London, as well as with Tokens, Architectures, and Systems teams Contribute to the team's institutional knowledge by documenting systems, debugging approaches, and lessons learned You May Be a Good Fit If You: Have hands-on experience training large language models, or deep expertise with JAX, TPU, PyTorch, or large-scale distributed systems Genuinely enjoy both research and engineering work—you'd describe your ideal split as roughly 50/50 rather than heavily weighted toward one or the other Are excited about being on-call for production systems, working long days during launches, and solving hard problems under pressure Thrive when working on whatever is most impactful, even if that changes day-to-day based on what the production model needs Excel at debugging complex, ambiguous problems across multiple layers of the stack Communicate clearly and collaborate effectively, especially when coordinating across time zones or during high-stress incidents Are passionate about the work itself and want to refine your craft as a research engineer Care about the societal impacts of AI and responsible scaling Strong Candidates May Also Have: Previous experience training LLM’s or working extensively with JAX/TPU, PyTorch, or other ML frameworks at scale Contributed to open-source LLM frameworks (e.g., open_lm, llm-foundry, mesh-transformer-jax) Published research on model training, scaling laws, or ML systems Experience with production ML systems, observability tools, or evaluation infrastructure Background as a systems engineer, quant, or in other roles requiring both technical depth and operational excellence What Makes This Role Unique: This is not a typical research engineering role. The work is highly operational—you'll be deeply involved in keeping our production models training smoothly, which means being responsive to incidents, flexible about priorities, and comfortable with uncertainty. During launches, the team often works extended hours and may need to respond to issues on evenings and weekends. However, this operational intensity comes with extraordinary learning opportunities. You'll gain hands-on experience with some of the largest, most sophisticated training runs in the industry. You'll work alongside world-class researchers and engineers, and the institutional knowledge you build will compound in ways that can't be easily transferred. For people who thrive on this type of work, it's uniquely rewarding. We're building a close-knit team of people who genuinely care about doing excellent work together. If you're someone who wants to be part of training the models that will define the future of AI—and you're excited about the full reality of what that entails—we'd love to hear from you. Location:This role requires working in-office 5 days per week in San Francisco. Deadline to apply: None. Applications will be reviewed on a rolling basis.The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (\"OTE\") range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.Annual Salary:$315,000 - $560,000 USDLogistics Education requirements: We require at least a Bachelor's degree in a related field or equivalent experience.Location-based hybrid policy: Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices. Visa sponsorship: We do sponsor visas! However, we aren't able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this. We encourage you to apply even if you do not believe you meet every single qualification. Not all strong candidates will meet every single qualification as listed. Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you're interested in this work. We think AI systems like the ones we're building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.Your safety matters to us. To protect yourself from potential scams, remember that Anthropic recruiters only contact you from @anthropic.com email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you're ever unsure about a communication, don't click any links—visit anthropic.com/careers directly for confirmed position openings. How we're different We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We're an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills. The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI & Compute, Concrete Problems in AI Safety, and Learning from Human Preferences. Come work with us! Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. Guidance on Candidates' AI Usage: Learn about our policy for using AI in our application process",
    "id": "job-boards-greenhouse-io-anthropic-jobs-4938432008",
    "source": "greenhouse",
    "source_url": "boards-api.greenhouse.io",
    "source_id": "4938432008",
    "title": "Research Engineer, Pretraining Scaling",
    "company_name": "anthropic",
    "company_slug": "anthropic",
    "location": "San Francisco, CA",
    "locations": [
      "San Francisco, CA"
    ],
    "url": "https://job-boards.greenhouse.io/anthropic/jobs/4938432008",
    "departments": [
      "AI Research & Engineering"
    ],
    "employment_type": null,
    "posted_at": "2025-11-22T09:28:05-05:00",
    "fetched_at": "2026-01-12T18:26:14.167Z",
    "description": "&lt;div class=&quot;content-intro&quot;&gt;&lt;h2&gt;&lt;strong&gt;About Anthropic&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.&lt;/p&gt;&lt;/div&gt;&lt;h2&gt;&lt;strong&gt;About the Role:&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic&#39;s ML Performance and Scaling team trains our production pretrained models, work that directly shapes the company&#39;s future and our mission to build safe, beneficial AI systems. As a Research Engineer on this team, you&#39;ll ensure our frontier models train reliably, efficiently, and at scale. This is demanding, high-impact work that requires both deep technical expertise and a genuine passion for the craft of large-scale ML systems.&lt;/p&gt;\n&lt;p&gt;This role lives at the boundary between research and engineering. You&#39;ll work across our entire production training stack: performance optimization, hardware debugging, experimental design, and launch coordination. During launches, the team works in tight lockstep, responding to production issues that can&#39;t wait for tomorrow.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Responsibilities:&amp;nbsp;&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Own critical aspects of our production pretraining pipeline, including model operations, performance optimization, observability, and reliability&lt;/li&gt;\n&lt;li&gt;Debug and resolve complex issues across the full stack—from hardware errors and networking to training dynamics and evaluation infrastructure&lt;/li&gt;\n&lt;li&gt;Design and run experiments to improve training efficiency, reduce step time, increase uptime, and enhance model performance&lt;/li&gt;\n&lt;li&gt;Respond to on-call incidents during model launches, diagnosing problems quickly and coordinating solutions across teams&lt;/li&gt;\n&lt;li&gt;Build and maintain production logging, monitoring dashboards, and evaluation infrastructure&lt;/li&gt;\n&lt;li&gt;Add new capabilities to the training codebase, such as long context support or novel architectures&lt;/li&gt;\n&lt;li&gt;Collaborate closely with teammates across SF and London, as well as with Tokens, Architectures, and Systems teams&lt;/li&gt;\n&lt;li&gt;Contribute to the team&#39;s institutional knowledge by documenting systems, debugging approaches, and lessons learned&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;&lt;strong&gt;You May Be a Good Fit If You:&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Have hands-on experience training large language models, or deep expertise with JAX, TPU, PyTorch, or large-scale distributed systems&lt;/li&gt;\n&lt;li&gt;Genuinely enjoy both research and engineering work—you&#39;d describe your ideal split as roughly 50/50 rather than heavily weighted toward one or the other&lt;/li&gt;\n&lt;li&gt;Are excited about being on-call for production systems, working long days during launches, and solving hard problems under pressure&lt;/li&gt;\n&lt;li&gt;Thrive when working on whatever is most impactful, even if that changes day-to-day based on what the production model needs&lt;/li&gt;\n&lt;li&gt;Excel at debugging complex, ambiguous problems across multiple layers of the stack&lt;/li&gt;\n&lt;li&gt;Communicate clearly and collaborate effectively, especially when coordinating across time zones or during high-stress incidents&lt;/li&gt;\n&lt;li&gt;Are passionate about the work itself and want to refine your craft as a research engineer&lt;/li&gt;\n&lt;li&gt;Care about the societal impacts of AI and responsible scaling&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;&lt;strong&gt;Strong Candidates May Also Have:&amp;nbsp;&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Previous experience training LLM’s or working extensively with JAX/TPU, PyTorch, or other ML frameworks at scale&lt;/li&gt;\n&lt;li&gt;Contributed to open-source LLM frameworks (e.g., open_lm, llm-foundry, mesh-transformer-jax)&lt;/li&gt;\n&lt;li&gt;Published research on model training, scaling laws, or ML systems&lt;/li&gt;\n&lt;li&gt;Experience with production ML systems, observability tools, or evaluation infrastructure&lt;/li&gt;\n&lt;li&gt;Background as a systems engineer, quant, or in other roles requiring both technical depth and operational excellence&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;&lt;strong&gt;What Makes This Role Unique:&amp;nbsp;&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;This is not a typical research engineering role. The work is highly operational—you&#39;ll be deeply involved in keeping our production models training smoothly, which means being responsive to incidents, flexible about priorities, and comfortable with uncertainty. During launches, the team often works extended hours and may need to respond to issues on evenings and weekends.&lt;/p&gt;\n&lt;p&gt;However, this operational intensity comes with extraordinary learning opportunities. You&#39;ll gain hands-on experience with some of the largest, most sophisticated training runs in the industry. You&#39;ll work alongside world-class researchers and engineers, and the institutional knowledge you build will compound in ways that can&#39;t be easily transferred. For people who thrive on this type of work, it&#39;s uniquely rewarding.&lt;/p&gt;\n&lt;p&gt;We&#39;re building a close-knit team of people who genuinely care about doing excellent work together. If you&#39;re someone who wants to be part of training the models that will define the future of AI—and you&#39;re excited about the full reality of what that entails—we&#39;d love to hear from you.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;Location:&lt;/strong&gt;This role requires working in-office 5 days per week in San Francisco.&amp;nbsp;&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;Deadline to apply:&lt;/strong&gt; None. Applications will be reviewed on a rolling basis.&lt;/p&gt;&lt;div class=&quot;content-pay-transparency&quot;&gt;&lt;div class=&quot;pay-input&quot;&gt;&lt;div class=&quot;description&quot;&gt;&lt;p&gt;The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (&quot;OTE&quot;) range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;title&quot;&gt;Annual Salary:&lt;/div&gt;&lt;div class=&quot;pay-range&quot;&gt;&lt;span&gt;$315,000&lt;/span&gt;&lt;span class=&quot;divider&quot;&gt;&amp;mdash;&lt;/span&gt;&lt;span&gt;$560,000 USD&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;content-conclusion&quot;&gt;&lt;h2&gt;&lt;strong&gt;Logistics&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;Education requirements: &lt;/strong&gt;We require at least a Bachelor&#39;s degree in a related field or equivalent experience.&lt;strong&gt;&lt;br&gt;&lt;br&gt;Location-based hybrid policy:&lt;/strong&gt; Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices.&lt;/p&gt;\n&lt;p&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Visa sponsorship:&lt;/strong&gt;&amp;nbsp;We do sponsor visas! However, we aren&#39;t able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;We encourage you to apply even if you do not believe you meet every single qualification.&lt;/strong&gt; Not all strong candidates will meet every single qualification as listed.&amp;nbsp; Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you&#39;re interested in this work. We think AI systems like the ones we&#39;re building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.&lt;br&gt;&lt;br&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Your safety matters to us.&lt;/strong&gt;&amp;nbsp;To protect yourself from potential scams, remember that Anthropic recruiters only contact you from&amp;nbsp;@anthropic.com&amp;nbsp;email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you&#39;re ever unsure about a communication, don&#39;t click any links—visit&amp;nbsp;&lt;u data-stringify-type=&quot;underline&quot;&gt;&lt;a class=&quot;c-link c-link--underline&quot; href=&quot;http://anthropic.com/careers&quot; target=&quot;_blank&quot; data-stringify-link=&quot;http://anthropic.com/careers&quot; data-sk=&quot;tooltip_parent&quot; data-remove-tab-index=&quot;true&quot;&gt;anthropic.com/careers&lt;/a&gt;&lt;/u&gt;&amp;nbsp;directly for confirmed position openings.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;How we&#39;re different&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We&#39;re an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills.&lt;/p&gt;\n&lt;p&gt;The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI &amp;amp; Compute, Concrete Problems in AI Safety, and Learning from Human Preferences.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Come work with us!&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. &lt;strong data-stringify-type=&quot;bold&quot;&gt;Guidance on Candidates&#39; AI Usage:&lt;/strong&gt;&amp;nbsp;Learn about&amp;nbsp;&lt;a class=&quot;c-link&quot; href=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; target=&quot;_blank&quot; data-stringify-link=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; data-sk=&quot;tooltip_parent&quot;&gt;our policy&lt;/a&gt;&amp;nbsp;for using AI in our application process&lt;/p&gt;&lt;/div&gt;",
    "_raw": {
      "source": "greenhouse",
      "original_id": 4938432008
    },
    "job_posted_at": "1h",
    "description_platform": "greenhouse",
    "description_success": true
  },
  {
    "job_title": "Research Scientist, Interpretability",
    "employer_name": "anthropic",
    "job_city": "San Francisco, CA",
    "job_apply_link": "https://job-boards.greenhouse.io/anthropic/jobs/4980427008",
    "job_posted_at_datetime_utc": "2025-11-22T09:28:05-05:00",
    "job_description": "About Anthropic Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.About the role: When you see what modern language models are capable of, do you wonder, \"How do these things work? How can we trust them?\" The Interpretability team at Anthropic is working to reverse-engineer how trained models work because we believe that a mechanistic understanding is the most robust way to make advanced systems safe. We’re looking for researchers and engineers to join our efforts. People mean many different things by \"interpretability\". We're focused on mechanistic interpretability, which aims to discover how neural network parameters map to meaningful algorithms. Some useful analogies might be to think of us as trying to do \"biology\" or \"neuroscience\" of neural networks using “microscopes” we build, or as treating neural networks as binary computer programs we're trying to \"reverse engineer\". A few places to learn more about our work and team at a high level are this introduction to Interpretability from our research lead, Chris Olah; a discussion of our work on the Hard Fork podcast produced by the New York Times, and this blog post (and accompanying video) sharing more about some of the engineering challenges we’d had to solve to get these results. Some of our team's notable publications include A Mathematical Framework for Transformer Circuits, In-context Learning and Induction Heads, Toy Models of Superposition, Scaling Monosemanticity, and our Circuits’ Methods and Biology papers. This work builds on ideas from members' work prior to Anthropic such as the original circuits thread, Multimodal Neurons, Activation Atlases, and Building Blocks. We aim to create a solid foundation for mechanistically understanding neural networks and making them safe (see our vision post). In the short term, we have focused on resolving the issue of \"superposition\" (see Toy Models of Superposition, Superposition, Memorization, and Double Descent, and our May 2023 update), which causes the computational units of the models, like neurons and attention heads, to be individually uninterpretable, and on finding ways to decompose models into more interpretable components. Our subsequent work found millions of features in Sonnet, one of our production language models, represents progress in this direction. In our most recent work, we develop methods that allow us to build circuits using features and use this circuits to understand the mechanisms associated with a model's computation and study specific examples of multi-hop reasoning, planning, and chain-of-thought faithfulness on Haiku 3.5, one of our production models.” This is a stepping stone towards our overall goal of mechanistically understanding neural networks. We often collaborate with teams across Anthropic, such as Alignment Science and Societal Impacts to use our work to make Anthropic’s models safer. We also have an Interpretability Architectures project that involves collaborating with Pretraining. Responsibilities: Develop methods for understanding LLMs by reverse engineering algorithms learned in their weights Design and run robust experiments, both quickly in toy scenarios and at scale in large models Create and analyze new interpretability features and circuits to better understand how models work. Build infrastructure for running experiments and visualizing results Work with colleagues to communicate results internally and publicly You may be a good fit if you: Have a strong track record of scientific research (in any field), and have done some work on Interpretability Enjoy team science – working collaboratively to make big discoveries Are comfortable with messy experimental science. We're inventing the field as we work, and the first textbook is years away You view research and engineering as two sides of the same coin. Every team member writes code, designs and runs experiments, and interprets results You can clearly articulate and discuss the motivations behind your work, and teach us about what you've learned. You like writing up and communicating your results, even when they're null To learn more about the skills we look for and how to prepare for this role, see our blog post – So You Want to Work in Mechanistic Interpretability? Familiarity with Python is required for this role. Role Specific Location Policy: This role is based in San Francisco office; however, we are open to considering exceptional candidates for remote work on a case-by-case basis. The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (\"OTE\") range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.Annual Salary:$315,000 - $560,000 USDLogistics Education requirements: We require at least a Bachelor's degree in a related field or equivalent experience.Location-based hybrid policy: Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices. Visa sponsorship: We do sponsor visas! However, we aren't able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this. We encourage you to apply even if you do not believe you meet every single qualification. Not all strong candidates will meet every single qualification as listed. Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you're interested in this work. We think AI systems like the ones we're building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.Your safety matters to us. To protect yourself from potential scams, remember that Anthropic recruiters only contact you from @anthropic.com email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you're ever unsure about a communication, don't click any links—visit anthropic.com/careers directly for confirmed position openings. How we're different We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We're an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills. The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI & Compute, Concrete Problems in AI Safety, and Learning from Human Preferences. Come work with us! Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. Guidance on Candidates' AI Usage: Learn about our policy for using AI in our application process",
    "id": "job-boards-greenhouse-io-anthropic-jobs-4980427008",
    "source": "greenhouse",
    "source_url": "boards-api.greenhouse.io",
    "source_id": "4980427008",
    "title": "Research Scientist, Interpretability",
    "company_name": "anthropic",
    "company_slug": "anthropic",
    "location": "San Francisco, CA",
    "locations": [
      "San Francisco, CA"
    ],
    "url": "https://job-boards.greenhouse.io/anthropic/jobs/4980427008",
    "departments": [
      "AI Research & Engineering"
    ],
    "employment_type": null,
    "posted_at": "2025-11-22T09:28:05-05:00",
    "fetched_at": "2026-01-12T18:26:14.167Z",
    "description": "&lt;div class=&quot;content-intro&quot;&gt;&lt;h2&gt;&lt;strong&gt;About Anthropic&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.&lt;/p&gt;&lt;/div&gt;&lt;h2 class=&quot;heading&quot;&gt;About the role:&lt;/h2&gt;\n&lt;p&gt;When you see what modern language models are capable of, do you wonder, &quot;How do these things work? How can we trust them?&quot;&lt;/p&gt;\n&lt;p&gt;The Interpretability team at Anthropic is working to reverse-engineer how trained models work because we believe that a mechanistic understanding is the most robust way to make advanced systems safe. We’re looking for researchers and engineers to join our efforts.&amp;nbsp;&lt;/p&gt;\n&lt;p&gt;People mean many different things by &quot;interpretability&quot;. We&#39;re focused on mechanistic interpretability, which aims to discover how neural network parameters map to meaningful algorithms. Some useful analogies might be to think of us as trying to do &quot;biology&quot; or &quot;neuroscience&quot; of neural networks using “microscopes” we build, or as treating neural networks as binary computer programs we&#39;re trying to &quot;reverse engineer&quot;.&lt;/p&gt;\n&lt;p&gt;A few places to learn more about our work and team at a high level are &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://www.youtube.com/watch?v=TxhhMTOTMDg&quot; target=&quot;_blank&quot;&gt;this introduction to Interpretability&lt;/a&gt; from our research lead, &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://colah.github.io/about.html&quot; target=&quot;_blank&quot;&gt;Chris Olah&lt;/a&gt;; a &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://open.spotify.com/episode/5UF79Uu94ia0fwC32a89LU&quot; target=&quot;_blank&quot;&gt;discussion of our work&lt;/a&gt; on the &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://www.nytimes.com/column/hard-fork&quot; target=&quot;_blank&quot;&gt;Hard Fork podcast&lt;/a&gt; produced by the New York Times, and this &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://www.anthropic.com/research/engineering-challenges-interpretability&quot; target=&quot;_blank&quot;&gt;blog post&lt;/a&gt; (and accompanying video) sharing more about some of the engineering challenges we’d had to solve to get these results. Some of our team&#39;s notable publications include &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://transformer-circuits.pub/2021/framework/index.html&quot; target=&quot;_blank&quot;&gt;A Mathematical Framework for Transformer Circuits&lt;/a&gt;, &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html&quot; target=&quot;_blank&quot;&gt;In-context Learning and Induction Heads&lt;/a&gt;, &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://transformer-circuits.pub/2022/toy_model/index.html&quot; target=&quot;_blank&quot;&gt;Toy Models of Superposition&lt;/a&gt;, &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://transformer-circuits.pub/2024/scaling-monosemanticity/&quot; target=&quot;_blank&quot;&gt;Scaling Monosemanticity&lt;/a&gt;, and our Circuits’ &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://transformer-circuits.pub/2025/attribution-graphs/methods.html&quot; target=&quot;_blank&quot;&gt;Methods&lt;/a&gt; and &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://transformer-circuits.pub/2025/attribution-graphs/biology.html&quot; target=&quot;_blank&quot;&gt;Biology&lt;/a&gt; papers. This work builds on ideas from members&#39; work prior to Anthropic such as the &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://distill.pub/2020/circuits/&quot; target=&quot;_blank&quot;&gt;original circuits thread&lt;/a&gt;, &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://distill.pub/2021/multimodal-neurons/&quot; target=&quot;_blank&quot;&gt;Multimodal Neurons&lt;/a&gt;, &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://distill.pub/2019/activation-atlas/&quot; target=&quot;_blank&quot;&gt;Activation Atlases&lt;/a&gt;, and &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://distill.pub/2018/building-blocks/&quot; target=&quot;_blank&quot;&gt;Building Blocks&lt;/a&gt;.&lt;/p&gt;\n&lt;p&gt;We aim to create a solid foundation for mechanistically understanding neural networks and making them safe (see our &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://transformer-circuits.pub/2023/interpretability-dreams/index.html&quot; target=&quot;_blank&quot;&gt;vision post&lt;/a&gt;). In the short term, we have focused on resolving the issue of &quot;superposition&quot; (see &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://transformer-circuits.pub/2022/toy_model/index.html&quot; target=&quot;_blank&quot;&gt;Toy Models of Superposition&lt;/a&gt;, &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://transformer-circuits.pub/2023/toy-double-descent/index.html&quot; target=&quot;_blank&quot;&gt;Superposition, Memorization, and Double Descent&lt;/a&gt;, and our &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://transformer-circuits.pub/2023/may-update/index.html&quot; target=&quot;_blank&quot;&gt;May 2023 update&lt;/a&gt;), which causes the computational units of the models, like neurons and attention heads, to be individually uninterpretable, and on finding ways to decompose models into more interpretable components. Our subsequent &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://www.anthropic.com/news/mapping-mind-language-model&quot; target=&quot;_blank&quot;&gt;work&lt;/a&gt; found millions of features in Sonnet, one of our production language models, represents progress in this direction. In our most recent work, we develop methods that allow us to build circuits using features and use this circuits to understand the mechanisms associated with a model&#39;s computation and study specific examples of multi-hop reasoning, planning, and chain-of-thought faithfulness on Haiku 3.5, one of our production models.” This is a stepping stone towards our overall goal of mechanistically understanding neural networks.&lt;/p&gt;\n&lt;p&gt;We often collaborate with teams across Anthropic, such as Alignment Science and Societal Impacts to use our work to make Anthropic’s models safer. We also have an &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://transformer-circuits.pub/2024/april-update/index.html#interpretability-architecture&quot; target=&quot;_blank&quot;&gt;Interpretability Architectures project&lt;/a&gt; that involves collaborating with Pretraining.&lt;/p&gt;\n&lt;h2 class=&quot;heading&quot;&gt;Responsibilities:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;Develop methods for understanding LLMs by reverse engineering algorithms learned in their weights&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Design and run robust experiments, both quickly in toy scenarios and at scale in large models&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Create and analyze new interpretability features and circuits to better understand how models work.&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Build infrastructure for running experiments and visualizing results&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Work with colleagues to communicate results internally and publicly&lt;/p&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2 class=&quot;heading&quot;&gt;&lt;strong&gt;You may be a good fit if you:&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;Have a strong track record of scientific research (in any field), and have done &lt;em&gt;some&lt;/em&gt; work on Interpretability&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Enjoy team science – working collaboratively to make big discoveries&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Are comfortable with messy experimental science. We&#39;re inventing the field as we work, and the first textbook is years away&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;You view research and engineering as two sides of the same coin. Every team member writes code, designs and runs experiments, and interprets results&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;You can clearly articulate and discuss the motivations behind your work, and teach us about what you&#39;ve learned. You like writing up and communicating your results, even when they&#39;re null&lt;/p&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;p&gt;To learn more about the skills we look for and how to prepare for this role, see our blog post – &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;https://transformer-circuits.pub/2025/april-update/index.html#work&quot; target=&quot;_blank&quot;&gt;So You Want to Work in Mechanistic Interpretability?&lt;/a&gt;&lt;/p&gt;\n&lt;p&gt;Familiarity with Python is required for this role.&lt;/p&gt;\n&lt;h2 class=&quot;heading&quot;&gt;&lt;strong&gt;Role Specific Location Policy:&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;This role is based in San Francisco office; however, we are open to considering exceptional candidates for remote work on a case-by-case basis.&lt;/p&gt;\n&lt;/li&gt;\n&lt;/ul&gt;&lt;div class=&quot;content-pay-transparency&quot;&gt;&lt;div class=&quot;pay-input&quot;&gt;&lt;div class=&quot;description&quot;&gt;&lt;p&gt;The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (&quot;OTE&quot;) range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;title&quot;&gt;Annual Salary:&lt;/div&gt;&lt;div class=&quot;pay-range&quot;&gt;&lt;span&gt;$315,000&lt;/span&gt;&lt;span class=&quot;divider&quot;&gt;&amp;mdash;&lt;/span&gt;&lt;span&gt;$560,000 USD&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;content-conclusion&quot;&gt;&lt;h2&gt;&lt;strong&gt;Logistics&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;Education requirements: &lt;/strong&gt;We require at least a Bachelor&#39;s degree in a related field or equivalent experience.&lt;strong&gt;&lt;br&gt;&lt;br&gt;Location-based hybrid policy:&lt;/strong&gt; Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices.&lt;/p&gt;\n&lt;p&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Visa sponsorship:&lt;/strong&gt;&amp;nbsp;We do sponsor visas! However, we aren&#39;t able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;We encourage you to apply even if you do not believe you meet every single qualification.&lt;/strong&gt; Not all strong candidates will meet every single qualification as listed.&amp;nbsp; Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you&#39;re interested in this work. We think AI systems like the ones we&#39;re building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.&lt;br&gt;&lt;br&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Your safety matters to us.&lt;/strong&gt;&amp;nbsp;To protect yourself from potential scams, remember that Anthropic recruiters only contact you from&amp;nbsp;@anthropic.com&amp;nbsp;email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you&#39;re ever unsure about a communication, don&#39;t click any links—visit&amp;nbsp;&lt;u data-stringify-type=&quot;underline&quot;&gt;&lt;a class=&quot;c-link c-link--underline&quot; href=&quot;http://anthropic.com/careers&quot; target=&quot;_blank&quot; data-stringify-link=&quot;http://anthropic.com/careers&quot; data-sk=&quot;tooltip_parent&quot; data-remove-tab-index=&quot;true&quot;&gt;anthropic.com/careers&lt;/a&gt;&lt;/u&gt;&amp;nbsp;directly for confirmed position openings.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;How we&#39;re different&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We&#39;re an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills.&lt;/p&gt;\n&lt;p&gt;The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI &amp;amp; Compute, Concrete Problems in AI Safety, and Learning from Human Preferences.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Come work with us!&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. &lt;strong data-stringify-type=&quot;bold&quot;&gt;Guidance on Candidates&#39; AI Usage:&lt;/strong&gt;&amp;nbsp;Learn about&amp;nbsp;&lt;a class=&quot;c-link&quot; href=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; target=&quot;_blank&quot; data-stringify-link=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; data-sk=&quot;tooltip_parent&quot;&gt;our policy&lt;/a&gt;&amp;nbsp;for using AI in our application process&lt;/p&gt;&lt;/div&gt;",
    "_raw": {
      "source": "greenhouse",
      "original_id": 4980427008
    },
    "job_posted_at": "1h",
    "description_platform": "greenhouse",
    "description_success": true
  },
  {
    "job_title": "Senior+ Software Engineer, Research Tools",
    "employer_name": "anthropic",
    "job_city": "San Francisco, CA | New York City, NY",
    "job_apply_link": "https://job-boards.greenhouse.io/anthropic/jobs/4981828008",
    "job_posted_at_datetime_utc": "2025-11-22T09:28:05-05:00",
    "job_description": "About Anthropic Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.About the role Anthropic's research teams are pushing the boundaries of AI safety and capability research, and they need exceptional tools to do their best work. As a Software Engineer on the Research Tools team, you'll build the infrastructure and applications that enable our researchers to iterate quickly, run complex experiments, and extract insights from frontier AI systems. This role sits at the intersection of product thinking and full-stack engineering. You'll work directly with researchers and engineers to deeply understand their workflows, identify bottlenecks, and rapidly ship solutions that multiply their productivity. Whether you're building human feedback interfaces for model evaluation, creating platforms for experiment orchestration, or developing novel visualization tools for understanding model behavior, your work will directly accelerate our mission to build safe, reliable AI systems. We're looking for someone who can operate with high agency in an ambiguous environment—someone who can be dropped into a research team, quickly develop domain expertise, and independently drive impactful projects from conception to delivery.No ML or Research experience is required Responsibilities Build and maintain full-stack applications and infrastructure that researchers use daily to conduct experiments, collect feedback, and analyze results Partner closely with research teams to understand their workflows, pain points, and requirements, translating these into technical solutions Design intuitive interfaces and abstractions that make complex research tasks accessible and efficient Create reusable platforms and tools that accelerate the development of new research applications Rapidly prototype and iterate on solutions, gathering feedback from users and refining based on real-world usage Take ownership of complete product areas, from understanding user needs through design, implementation, and ongoing iteration Contribute to technical strategy and architectural decisions for research tooling Mentor other engineers and help establish best practices for research application development You may be a good fit if you Have 5+ years of software engineering experience with a strong focus on full-stack development Excel at rapid iteration and shipping—you can move from concept to working prototype quickly Have experience building tools, platforms, or infrastructure for technical users (engineers, researchers, data scientists, analysts, etc.) Demonstrate high agency and ability to operate independently in ambiguous environments Can quickly develop deep understanding of complex technical domains Have strong product instincts and can identify the right problems to solve Are proficient with modern web technologies (React, TypeScript, Python, etc.) Have a track record of building user-facing applications that are actually used and loved by their target audience Communicate effectively with both technical and non-technical stakeholders Care about the societal impacts of your work and are motivated by Anthropic's mission Strong candidates may also have Experience building research tools, scientific software, or experimentation platforms Background in machine learning, AI research, or working closely with ML researchers Founded or been an early engineer at a startup, particularly one focused on developer or researcher tools Built open-source tools or platforms with active user communities Experience with data visualization, interactive interfaces, or novel interaction paradigms Contributed to engineering platforms or internal tooling at scale (similar to Heroku, Vercel, or other platform-as-a-service products) Experience leveraging AI/LLMs to build more powerful or efficient tools Previous work in creative tools, artist tools, or other domains requiring deep user empathy Domain knowledge in areas like human-computer interaction, systems safety, or AI alignment Representative projects Building interfaces for collecting and managing human feedback on model outputs at scale Creating experiment orchestration platforms that make it easy to launch, monitor, and analyze complex research runs Developing visualization tools that help researchers understand model behavior and identify failure modes Designing reusable components and frameworks that enable rapid development of new research applications Building sandboxed execution environments for safely running AI-generated code The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (\"OTE\") range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.Annual Salary:$300,000 - $405,000 USDLogistics Education requirements: We require at least a Bachelor's degree in a related field or equivalent experience.Location-based hybrid policy: Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices. Visa sponsorship: We do sponsor visas! However, we aren't able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this. We encourage you to apply even if you do not believe you meet every single qualification. Not all strong candidates will meet every single qualification as listed. Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you're interested in this work. We think AI systems like the ones we're building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.Your safety matters to us. To protect yourself from potential scams, remember that Anthropic recruiters only contact you from @anthropic.com email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you're ever unsure about a communication, don't click any links—visit anthropic.com/careers directly for confirmed position openings. How we're different We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We're an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills. The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI & Compute, Concrete Problems in AI Safety, and Learning from Human Preferences. Come work with us! Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. Guidance on Candidates' AI Usage: Learn about our policy for using AI in our application process",
    "id": "job-boards-greenhouse-io-anthropic-jobs-4981828008",
    "source": "greenhouse",
    "source_url": "boards-api.greenhouse.io",
    "source_id": "4981828008",
    "title": "Senior+ Software Engineer, Research Tools",
    "company_name": "anthropic",
    "company_slug": "anthropic",
    "location": "San Francisco, CA | New York City, NY",
    "locations": [
      "San Francisco, CA | New York City, NY"
    ],
    "url": "https://job-boards.greenhouse.io/anthropic/jobs/4981828008",
    "departments": [
      "AI Research & Engineering"
    ],
    "employment_type": null,
    "posted_at": "2025-11-22T09:28:05-05:00",
    "fetched_at": "2026-01-12T18:26:14.167Z",
    "description": "&lt;div class=&quot;content-intro&quot;&gt;&lt;h2&gt;&lt;strong&gt;About Anthropic&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.&lt;/p&gt;&lt;/div&gt;&lt;h2 class=&quot;heading&quot;&gt;&lt;strong&gt;About the role&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic&#39;s research teams are pushing the boundaries of AI safety and capability research, and they need exceptional tools to do their best work. As a Software Engineer on the Research Tools team, you&#39;ll build the infrastructure and applications that enable our researchers to iterate quickly, run complex experiments, and extract insights from frontier AI systems.&lt;/p&gt;\n&lt;p&gt;This role sits at the intersection of product thinking and full-stack engineering. You&#39;ll work directly with researchers and engineers to deeply understand their workflows, identify bottlenecks, and rapidly ship solutions that multiply their productivity. Whether you&#39;re building human feedback interfaces for model evaluation, creating platforms for experiment orchestration, or developing novel visualization tools for understanding model behavior, your work will directly accelerate our mission to build safe, reliable AI systems.&lt;/p&gt;\n&lt;p&gt;We&#39;re looking for someone who can operate with high agency in an ambiguous environment—someone who can be dropped into a research team, quickly develop domain expertise, and independently drive impactful projects from conception to delivery.&lt;br&gt;&lt;br&gt;&lt;strong&gt;&lt;em&gt;No ML or Research experience is required&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n&lt;h2 class=&quot;heading&quot;&gt;&lt;strong&gt;Responsibilities&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;Build and maintain full-stack applications and infrastructure that researchers use daily to conduct experiments, collect feedback, and analyze results&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Partner closely with research teams to understand their workflows, pain points, and requirements, translating these into technical solutions&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Design intuitive interfaces and abstractions that make complex research tasks accessible and efficient&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Create reusable platforms and tools that accelerate the development of new research applications&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Rapidly prototype and iterate on solutions, gathering feedback from users and refining based on real-world usage&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Take ownership of complete product areas, from understanding user needs through design, implementation, and ongoing iteration&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Contribute to technical strategy and architectural decisions for research tooling&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Mentor other engineers and help establish best practices for research application development&lt;/p&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2 class=&quot;heading&quot;&gt;&lt;strong&gt;You may be a good fit if you&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;Have 5+ years of software engineering experience with a strong focus on full-stack development&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Excel at rapid iteration and shipping—you can move from concept to working prototype quickly&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Have experience building tools, platforms, or infrastructure for technical users (engineers, researchers, data scientists, analysts, etc.)&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Demonstrate high agency and ability to operate independently in ambiguous environments&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Can quickly develop deep understanding of complex technical domains&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Have strong product instincts and can identify the right problems to solve&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Are proficient with modern web technologies (React, TypeScript, Python, etc.)&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Have a track record of building user-facing applications that are actually used and loved by their target audience&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Communicate effectively with both technical and non-technical stakeholders&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Care about the societal impacts of your work and are motivated by Anthropic&#39;s mission&lt;/p&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2 class=&quot;heading&quot;&gt;&lt;strong&gt;Strong candidates may also have&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;Experience building research tools, scientific software, or experimentation platforms&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Background in machine learning, AI research, or working closely with ML researchers&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Founded or been an early engineer at a startup, particularly one focused on developer or researcher tools&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Built open-source tools or platforms with active user communities&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Experience with data visualization, interactive interfaces, or novel interaction paradigms&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Contributed to engineering platforms or internal tooling at scale (similar to Heroku, Vercel, or other platform-as-a-service products)&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Experience leveraging AI/LLMs to build more powerful or efficient tools&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Previous work in creative tools, artist tools, or other domains requiring deep user empathy&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Domain knowledge in areas like human-computer interaction, systems safety, or AI alignment&lt;/p&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2 class=&quot;heading&quot;&gt;&lt;strong&gt;Representative projects&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;Building interfaces for collecting and managing human feedback on model outputs at scale&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Creating experiment orchestration platforms that make it easy to launch, monitor, and analyze complex research runs&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Developing visualization tools that help researchers understand model behavior and identify failure modes&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Designing reusable components and frameworks that enable rapid development of new research applications&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Building sandboxed execution environments for safely running AI-generated code&lt;/p&gt;\n&lt;/li&gt;\n&lt;/ul&gt;&lt;div class=&quot;content-pay-transparency&quot;&gt;&lt;div class=&quot;pay-input&quot;&gt;&lt;div class=&quot;description&quot;&gt;&lt;p&gt;The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (&quot;OTE&quot;) range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;title&quot;&gt;Annual Salary:&lt;/div&gt;&lt;div class=&quot;pay-range&quot;&gt;&lt;span&gt;$300,000&lt;/span&gt;&lt;span class=&quot;divider&quot;&gt;&amp;mdash;&lt;/span&gt;&lt;span&gt;$405,000 USD&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;content-conclusion&quot;&gt;&lt;h2&gt;&lt;strong&gt;Logistics&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;Education requirements: &lt;/strong&gt;We require at least a Bachelor&#39;s degree in a related field or equivalent experience.&lt;strong&gt;&lt;br&gt;&lt;br&gt;Location-based hybrid policy:&lt;/strong&gt; Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices.&lt;/p&gt;\n&lt;p&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Visa sponsorship:&lt;/strong&gt;&amp;nbsp;We do sponsor visas! However, we aren&#39;t able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;We encourage you to apply even if you do not believe you meet every single qualification.&lt;/strong&gt; Not all strong candidates will meet every single qualification as listed.&amp;nbsp; Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you&#39;re interested in this work. We think AI systems like the ones we&#39;re building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.&lt;br&gt;&lt;br&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Your safety matters to us.&lt;/strong&gt;&amp;nbsp;To protect yourself from potential scams, remember that Anthropic recruiters only contact you from&amp;nbsp;@anthropic.com&amp;nbsp;email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you&#39;re ever unsure about a communication, don&#39;t click any links—visit&amp;nbsp;&lt;u data-stringify-type=&quot;underline&quot;&gt;&lt;a class=&quot;c-link c-link--underline&quot; href=&quot;http://anthropic.com/careers&quot; target=&quot;_blank&quot; data-stringify-link=&quot;http://anthropic.com/careers&quot; data-sk=&quot;tooltip_parent&quot; data-remove-tab-index=&quot;true&quot;&gt;anthropic.com/careers&lt;/a&gt;&lt;/u&gt;&amp;nbsp;directly for confirmed position openings.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;How we&#39;re different&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We&#39;re an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills.&lt;/p&gt;\n&lt;p&gt;The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI &amp;amp; Compute, Concrete Problems in AI Safety, and Learning from Human Preferences.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Come work with us!&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. &lt;strong data-stringify-type=&quot;bold&quot;&gt;Guidance on Candidates&#39; AI Usage:&lt;/strong&gt;&amp;nbsp;Learn about&amp;nbsp;&lt;a class=&quot;c-link&quot; href=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; target=&quot;_blank&quot; data-stringify-link=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; data-sk=&quot;tooltip_parent&quot;&gt;our policy&lt;/a&gt;&amp;nbsp;for using AI in our application process&lt;/p&gt;&lt;/div&gt;",
    "_raw": {
      "source": "greenhouse",
      "original_id": 4981828008
    },
    "job_posted_at": "1h",
    "description_platform": "greenhouse",
    "description_success": true
  },
  {
    "job_title": "Senior/Staff Software Engineer, Inference",
    "employer_name": "anthropic",
    "job_city": "New York City, NY; San Francisco, CA | New York City, NY | Seattle, WA; Seattle, WA",
    "job_apply_link": "https://job-boards.greenhouse.io/anthropic/jobs/4951696008",
    "job_posted_at_datetime_utc": "2025-11-22T09:28:05-05:00",
    "job_description": "About Anthropic Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems. About the role Our Inference team is responsible for building and maintaining the critical systems that serve Claude to millions of users worldwide. We bring Claude to life by serving our models via the industry's largest compute-agnostic inference deployments. We are responsible for the entire stack from intelligent request routing to fleet-wide orchestration across diverse AI accelerators. The team has a dual mandate: maximizing compute efficiency to serve our explosive customer growth, while enabling breakthrough research by giving our scientists the high-performance inference infrastructure they need to develop next-generation models. We tackle complex, distributed systems challenges across multiple accelerator families and emerging AI hardware running in multiple cloud platforms. You may be a good fit if you: Have significant software engineering experience, particularly with distributed systems Are results-oriented, with a bias towards flexibility and impact Pick up slack, even if it goes outside your job description Enjoy pair programming (we love to pair!) Want to learn more about machine learning systems and infrastructure Thrive in environments where technical excellence directly drives both business results and research breakthroughs Care about the societal impacts of your work Strong candidates may also have experience with: High-performance, large-scale distributed systems Implementing and deploying machine learning systems at scale Load balancing, request routing, or traffic management systems LLM inference optimization, batching, and caching strategies Kubernetes and cloud infrastructure (AWS, GCP) Python or Rust Representative projects: Designing intelligent routing algorithms that optimize request distribution across thousands of accelerators Autoscaling our compute fleet to dynamically match supply with demand across production, research, and experimental workloads Building production-grade deployment pipelines for releasing new models to millions of users Integrating new AI accelerator platforms to maintain our hardware-agnostic competitive advantage Contributing to new inference features (e.g., structured sampling, prompt caching) Supporting inference for new model architectures Analyzing observability data to tune performance based on real-world production workloads Managing multi-region deployments and geographic routing for global customers Deadline to apply: None. Applications will be reviewed on a rolling basis. The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (\"OTE\") range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.Annual Salary:$300,000 - $485,000 USDLogistics Education requirements: We require at least a Bachelor's degree in a related field or equivalent experience.Location-based hybrid policy: Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices. Visa sponsorship: We do sponsor visas! However, we aren't able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this. We encourage you to apply even if you do not believe you meet every single qualification. Not all strong candidates will meet every single qualification as listed. Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you're interested in this work. We think AI systems like the ones we're building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.Your safety matters to us. To protect yourself from potential scams, remember that Anthropic recruiters only contact you from @anthropic.com email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you're ever unsure about a communication, don't click any links—visit anthropic.com/careers directly for confirmed position openings. How we're different We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We're an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills. The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI & Compute, Concrete Problems in AI Safety, and Learning from Human Preferences. Come work with us! Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. Guidance on Candidates' AI Usage: Learn about our policy for using AI in our application process",
    "id": "job-boards-greenhouse-io-anthropic-jobs-4951696008",
    "source": "greenhouse",
    "source_url": "boards-api.greenhouse.io",
    "source_id": "4951696008",
    "title": "Senior/Staff Software Engineer, Inference",
    "company_name": "anthropic",
    "company_slug": "anthropic",
    "location": "New York City, NY; San Francisco, CA | New York City, NY | Seattle, WA; Seattle, WA",
    "locations": [
      "New York City, NY; San Francisco, CA | New York City, NY | Seattle, WA; Seattle, WA"
    ],
    "url": "https://job-boards.greenhouse.io/anthropic/jobs/4951696008",
    "departments": [
      "Software Engineering - Infrastructure"
    ],
    "employment_type": null,
    "posted_at": "2025-11-22T09:28:05-05:00",
    "fetched_at": "2026-01-12T18:26:14.167Z",
    "description": "&lt;div class=&quot;content-intro&quot;&gt;&lt;h2&gt;&lt;strong&gt;About Anthropic&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;\n&lt;div&gt;\n&lt;h2&gt;About the role&lt;/h2&gt;\n&lt;/div&gt;\n&lt;div&gt;\n&lt;p&gt;Our Inference team is responsible for building and maintaining the critical systems that serve Claude to millions of users worldwide. We bring Claude to life by serving our models via the industry&#39;s largest compute-agnostic inference deployments.&amp;nbsp; We are responsible for the entire stack from intelligent request routing to fleet-wide orchestration across diverse AI accelerators.&lt;/p&gt;\nThe team has a dual mandate: &lt;strong&gt;maximizing compute efficiency&lt;/strong&gt; to serve our explosive customer growth, while &lt;strong&gt;enabling breakthrough research&lt;/strong&gt; by giving our scientists the high-performance inference infrastructure they need to develop next-generation models. We tackle complex, distributed systems challenges across multiple accelerator families and emerging AI hardware running in multiple cloud platforms.&lt;/div&gt;\n&lt;/div&gt;\n&lt;h2&gt;You may be a good fit if you:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Have significant software engineering experience, particularly with distributed systems&lt;/li&gt;\n&lt;li&gt;Are results-oriented, with a bias towards flexibility and impact&lt;/li&gt;\n&lt;li&gt;Pick up slack, even if it goes outside your job description&lt;/li&gt;\n&lt;li&gt;Enjoy pair programming (we love to pair!)&lt;/li&gt;\n&lt;li&gt;Want to learn more about machine learning systems and infrastructure&lt;/li&gt;\n&lt;li&gt;Thrive in environments where technical excellence directly drives both business results and research breakthroughs&lt;/li&gt;\n&lt;li&gt;Care about the societal impacts of your work&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;Strong candidates may also have experience with:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;High-performance, large-scale distributed systems&lt;/li&gt;\n&lt;li&gt;Implementing and deploying machine learning systems at scale&lt;/li&gt;\n&lt;li&gt;Load balancing, request routing, or traffic management systems&lt;/li&gt;\n&lt;li&gt;LLM inference optimization, batching, and caching strategies&lt;/li&gt;\n&lt;li&gt;Kubernetes and cloud infrastructure (AWS, GCP)&lt;/li&gt;\n&lt;li&gt;Python or Rust&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;&lt;strong&gt;Representative projects:&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul class=&quot;pt1 text-gray-700&quot;&gt;\n&lt;li&gt;Designing intelligent routing algorithms that optimize request distribution across thousands of accelerators&lt;/li&gt;\n&lt;li&gt;Autoscaling our compute fleet to dynamically match supply with demand across production, research, and experimental workloads&lt;/li&gt;\n&lt;li&gt;Building production-grade deployment pipelines for releasing new models to millions of users&lt;/li&gt;\n&lt;li&gt;Integrating new AI accelerator platforms to maintain our hardware-agnostic competitive advantage&lt;/li&gt;\n&lt;li&gt;Contributing to new inference features (e.g., structured sampling, prompt caching)&lt;/li&gt;\n&lt;li&gt;Supporting inference for new model architectures&lt;/li&gt;\n&lt;li&gt;Analyzing observability data to tune performance based on real-world production workloads&lt;/li&gt;\n&lt;li&gt;Managing multi-region deployments and geographic routing for global customers&lt;/li&gt;\n&lt;/ul&gt;\n&lt;p&gt;&lt;strong&gt;Deadline to apply:&amp;nbsp;&lt;/strong&gt;None. Applications will be reviewed on a rolling basis.&amp;nbsp;&lt;/p&gt;&lt;div class=&quot;content-pay-transparency&quot;&gt;&lt;div class=&quot;pay-input&quot;&gt;&lt;div class=&quot;description&quot;&gt;&lt;p&gt;The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (&quot;OTE&quot;) range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;title&quot;&gt;Annual Salary:&lt;/div&gt;&lt;div class=&quot;pay-range&quot;&gt;&lt;span&gt;$300,000&lt;/span&gt;&lt;span class=&quot;divider&quot;&gt;&amp;mdash;&lt;/span&gt;&lt;span&gt;$485,000 USD&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;content-conclusion&quot;&gt;&lt;h2&gt;&lt;strong&gt;Logistics&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;Education requirements: &lt;/strong&gt;We require at least a Bachelor&#39;s degree in a related field or equivalent experience.&lt;strong&gt;&lt;br&gt;&lt;br&gt;Location-based hybrid policy:&lt;/strong&gt; Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices.&lt;/p&gt;\n&lt;p&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Visa sponsorship:&lt;/strong&gt;&amp;nbsp;We do sponsor visas! However, we aren&#39;t able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;We encourage you to apply even if you do not believe you meet every single qualification.&lt;/strong&gt; Not all strong candidates will meet every single qualification as listed.&amp;nbsp; Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you&#39;re interested in this work. We think AI systems like the ones we&#39;re building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.&lt;br&gt;&lt;br&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Your safety matters to us.&lt;/strong&gt;&amp;nbsp;To protect yourself from potential scams, remember that Anthropic recruiters only contact you from&amp;nbsp;@anthropic.com&amp;nbsp;email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you&#39;re ever unsure about a communication, don&#39;t click any links—visit&amp;nbsp;&lt;u data-stringify-type=&quot;underline&quot;&gt;&lt;a class=&quot;c-link c-link--underline&quot; href=&quot;http://anthropic.com/careers&quot; target=&quot;_blank&quot; data-stringify-link=&quot;http://anthropic.com/careers&quot; data-sk=&quot;tooltip_parent&quot; data-remove-tab-index=&quot;true&quot;&gt;anthropic.com/careers&lt;/a&gt;&lt;/u&gt;&amp;nbsp;directly for confirmed position openings.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;How we&#39;re different&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We&#39;re an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills.&lt;/p&gt;\n&lt;p&gt;The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI &amp;amp; Compute, Concrete Problems in AI Safety, and Learning from Human Preferences.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Come work with us!&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. &lt;strong data-stringify-type=&quot;bold&quot;&gt;Guidance on Candidates&#39; AI Usage:&lt;/strong&gt;&amp;nbsp;Learn about&amp;nbsp;&lt;a class=&quot;c-link&quot; href=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; target=&quot;_blank&quot; data-stringify-link=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; data-sk=&quot;tooltip_parent&quot;&gt;our policy&lt;/a&gt;&amp;nbsp;for using AI in our application process&lt;/p&gt;&lt;/div&gt;",
    "_raw": {
      "source": "greenhouse",
      "original_id": 4951696008
    },
    "job_posted_at": "1h",
    "description_platform": "greenhouse",
    "description_success": true
  }
]